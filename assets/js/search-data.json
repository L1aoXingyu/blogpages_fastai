{
  
    
        "post0": {
            "title": "FastReID: 一个面向学术界和工业界的 ReID Toolbox",
            "content": "引言 . FastReID 平台已经成为京东人工智能研究（JD AI Research）的开源项目之一，它是面向学术界和工业界的研究/开源项目，主要用于研究和应用落地。 先放上 Github 链接： . fast-reid . 我们的 FastReID 也有一篇 paper 进行更加详细地介绍，如果想要了解更多关于 FastReID 的信息，可以直接去看原始 paper。 . FastReID: A Pytorch Toolbox for Real-world Person Re-identification . 接下来会分享开发 FastReID 初衷以及 FastReID 的特点。 . 动机 . 最早的时候和罗博(@罗浩)搞了一个 reid strong baseline，不过那个项目在 pytorch 的基础上，又用 ignite 包了一下，开源之后很多人都说 ignite 比较影响使用体验，所以后面在我自己维护的 baseline 版本里面就去掉了 ignite。 . 我们自己做项目，以及实习生做研究都是基于 strong baseline 去魔改的，后面发现各自搞的 project 和原始的 baseline 差别越来越大，导致我们想要在实际场景中运用研究工作时效果不好，遇到了很多代码不对齐的现象。出现这个问题原因在于其中一个同学修改了某一个训练逻辑或者预处理的地方，他自己忘记了，最终发现把模型合并在一起的效果总是不好，需要花很多时间去解决这些琐碎的问题。 . 正是由于这个原因，我们决定把 baseline 这套框架封成一个库，大家基于这套库去做工作就更利于找到各自定制化的地方。开源社区中也有几个比较流行的 reid 库，比如 torchreid，Person_reID_baseline_pytorch 等等，都是很好的库，值得我们去学习。最近 Facebook AI Researck 开源了 Detectron2 项目，它里面的整体概念和设计哲学都非常棒，所以我们决定参考 detectron2 的设计来整个 FastReID 架构。 基于 FastReID，我们的产品模型和 research 的模型有了比较好的兼容性，同时也比较容易去 customize 一些功能，模块化的设计允许研究人员能自定义地插入自己想要的模块。 . 我们构建 FastReID 的目的在于满足 JD AI Research 的研究需求，能够快速地准备地实现一些 ideas，并且能够将研究员的研究成果快速地部署到实践中。 无论在学术界还是工业界，开源项目都有助于整个社区的快速发展，使我们的想法快速付诸于实际落地项目中。我们也希望 FastReID 的发布能够继续加速行人重识别领域的发展。 . 一些新特性 . FastReID 采用高度模块化设计，它具有更高的灵活性和可扩展性，能够在支持多 GPU 训练，它的扩展性设计使其在重构代码的情况快速实现很多研究项目。 . 下面我们介绍一下其中的一些新特性。 . 1.基于 FastReID，我们在多个 ReID 任务都获得非常不错的性能，并且用于业务线中，包括行人 ReID、Occluded/Partial 行人 ReID、跨域行人 ReID 和车辆 ReID。 . 虽然在 ReID 发展的这几年里面，有了很多 ReID 的 paper，大家的刷的点也越来越高了，但是性能好且稳定的方法其实还是基于最简单的 global feature 和分块的 local feature，其他使用额外信息如 pose，mask，parsing 之类的方法在实际使用中都不够稳定，同时也比较笨重。 . 所以我们在 toolbox 中内置了这两种方法，一种是基于 global feature 的 strong baseline，一种是基于分块的 MGN。 然后在 BagofTricks 的基础上，将其他可能有用的 tricks 都实现了一下，包括有效的，比如 circle loss，gem pooling 之类的，也有没有效果的，比如 SWA, AugMix 等等。 最终基于 ResNet50-ibn backbone，在三个数据库上实现了下面的性能 . Method Market1501 DukeMTMC MSMT17 .   | Rank@1 (mAP) | Rank@1 (mAP) | Rank@1 (mAP) | . BagTricks | 94.4% (86.1%) | 87.1% (76.9%) | 72.3% (48.3%) | . FastReID-baseline | 95.7% (89.3%) | 91.3% (81.6%) | 84.0% (61.2%) | . FastReID-MGN | 95.8% (89.7%) | 91.6% (82.1%) | 85.1% (65.4%) | . 在 Marekt1501 上面提升空间已经不大了，因为后面有一些错误标签，但是在 DukeMTMC 和 MSMT17 上还是有比较显著的提升，详情可以去 model zoo 里面查看完整的配置文件。 . 在 partial re-id 上，我们也基于之前 DSR 的工作，在三个 partial 库上有了持续的提升 . Method PartialReID OccludedReID PartialiLIDS .   | Rank@1 (mAP) | Rank@1 (mAP) | Rank@1 (mAP) | . FPR | 81.0% (76.6%) | 78.3% (68.0%) | 68.1% (61.8%) | . FastReID-DSR | 82.7% (76.8%) | 81.6% (70.9%) | 73.1% (79.8%) | . 具体可以去 projects/PartialReID 中查看代码和训练配置。 . 在 cross-domain reid 上面，我们也做了一些工作，正在投稿中，之后会在开源在projects/Cross-domain-reid 中，从效果上看，在跨域上已经大大缩小了和有监督 reid 的差距。 . Method Market1501 to DukeMTMC DukeMTMC to Market1501 .   | Rank@1 (mAP) | Rank@1 (mAP) | . DirectTransfer | 54.4% (34.0%) | 62.6% (32.1%) | . Our method | 82.7% (69.2%) | 92.7% (80.5%) | . 在实际场景中我们发现穿黑衣服的人是一个比较难的问题，所以我们也基于 FastReID 构建了头肩模块去解决黑衣人的问题，也实现了比较不错的性能提升，paper 正在投稿，后面会开源在 projects/HAA 中。 . Method Black-ReID .   | Rank@1 (mAP) | . Baseline(R50) | 80.9% (70.8%) | . HAA(R50) | 86.7% (79.0%) | . 在 vehicle re-id 上，我们也在 VeRI 数据集上跑了一下 baseline，得到了一个比较不错的结果，另外两个数据集 VehicleID 和 VERI-Wild 上也跑了一下，具体可以去 model zoo 里面查看。 . Method VeRi .   | Rank@1 (mAP) | . FastReID-baseline | 97.0% (81.9%) | . 另外还有一些基于 FastReID 做的工作都在投稿中，就不详细介绍了，后续都会开源在 fast-reid/projects 里面。 . 2.在模型评估上我们实现了更多的功能，比如我们支持比较灵活的测试方式，通过下面的命令可以实现在 Market1501 和 MSMT17 上联合训练，然后在 Market1501 和 DukeMTMC 上进行测试。 . DATASETS: NAMES: (&quot;Market1501&quot;, &quot;MSMT17&quot;,) TESTS: (&quot;Market1501&quot;, &quot;DukeMTMC&quot;,) . 另外也提供了更加丰富的指标评估，除了 reid 中最为常见的 CMC 和 mAP，以及在 reid-survey 中提出的 mINP之外，我们还提供了 ROC 曲线和分布图 . 因为我们发现在实际业务场景中往往是开集测试，甚至 gallery 都是在动态变化的，在这种情况下通过单一的 rank1 或者是 mAP 来评估模型就不那么准确了，在实际应用时往往需要卡阈值再出 topK，所以通过分布和 ROC 曲线可以更好地帮我们找到阈值。 . 除了评估指标，可视化其实非常重要，通过可视化 rank list 可以快速定位模型的问题，同时也会发现一些错误标注，比如通过可视化我们发现 Market1501 里面有一些错误标注，最高的 rank@1 就只能做到 96 左右，而一些公司的 PR 文可以做到 99，我也不知道他们是怎么做到把标注错误都搞定的 😂。 . 我们发现很多库都只是实现了最基本的可视化功能，比如可视化 rank list，但是这种单一的可视化其实并不能帮助我们从多个维度了解问题，所以我们实现了更好的可视化功能。首先可以根据每个 query 的 AP 进行排序展示，比如 AP 从小到大进行展示，那么可视化出来的第一张图片就是 AP 最低的 query，通过这个方式我们可以了解到模型处理能力最差的 bad case。 . 另外我们在看预测结果的时候，其实也会想知道到底这个 query 的标注是怎么样的图片，比如我们再看 duke 数据集中下面的 rank list 时，发现他的 AP 是0，下面的蓝色框表示都是错误的匹配。 . 这时我们就会疑惑，到底这张 query 的标注长什么样，这时如果我们像下面这样将 label 同时可视化出来，我们就可以快速地知道，原来 query 其实是黄衣服后面那个黑衣服的人，因为是用 tracking 算法标注的，他大部分都被前面穿黄衣服的人挡住了，所以模型无法找对，而且这种情况下搞模型结构很难解决的，在实际业务中直接从源头上选择质量好的 query 是一个更好的解决方案。 . 3.大多数的库都只关注学术界做 research，我们更希望能够产学研结合，research 中 work 的东西能够快速到实际场景中去验证效果，发现实际中真正需要解决的问题。 当然在实际研究中可以天马行空去写代码，但是这份代码无法快速地在实际场景中去验证，如果基于 FastReID 去重构和开发，那么我们就能够找到新方法所需要的最小代码实现，就能够很轻易地移植到实际业务中，也不用把大量的时间花在对齐训练逻辑以及预处理上了。 . 另外就是如何将 pytorch 训练的模型更容易地部署到生产环境上，这也是工业界比较关心的事情，python 写的模型如果没有优化和加速的话，在实际中是很慢的。 为了更好地在工业界中应用，我们会在 FastReID 中加上一些脚本能够容易地将 pytorch 训练的模型转到 caffe 和 TensorRT 上，最后做一下模型的量化。目前 pytorch 升级到 1.3 之后慢慢开始支持量化了，我们也会尝试在 pytorch 端直接做量化，和蒸馏小模型。不过这些部分的内容还在整理和开发中，目前还没有 ready。 . 未来的一些改进方向 . 上面说了 FastReID 中的一些新特性，同时还有一些地方需要继续改进。 . 目前的多卡训练还是基于 DataParallel 来实现的，会存在负载不均衡，速度损失以及无法实现多机的缺点，我们正在用 DistributedDataParallel 来替换 DataParallel。 | 模型转换，量化和蒸馏小模型等部分的代码还没有搞定，后续会慢慢开源一部分。 | 可能会考虑将 FastReID 推广到通用的 image retrieval 上。 | 结语 . 科技的进步是整个社区的努力，包括学术界和工业界。 个人的努力永远赶不上整个社区的努力，这也是开源 FastReID 的初衷。 我们一直主张共享代码，快速试验新的想法，通过 FastReID 的发布加速整个 ReID 的产业化落地。 我们也会继续发展和完善FastReID。希望大家能够 star/fork/watch/pr，大家互相学习，共同推动计算机视觉的发展。 . 在此感谢 JD AI 组的同事和老师的支持，正是因为大家的努力让 FastReID 变得更好，并且科研项目也都在 FastReID 上取得了很好的性能。 . . [1] Luo, Hao and Gu, Youzhi and Liao, Xingyu and Lai, Shenqi and Jiang, Wei. Bag of Tricks and a Strong Baseline for Deep Person Re-Identification. . [2] Wang, G. and Yuan, Y. and Chen, X. and Li, J. and Zhou, X. Learning Discriminative Features with Multiple Granularities for Person Re-Identification. . [3] Ye, Mang and Shen, Jianbing and Lin, Gaojie and Xiang, Tao and Shao, Ling and Hoi, Steven C. H.Deep Learning for Person Re-identification: A Survey and Outlook. . [4] Y. Sun, C. Cheng, Y. Zhang, C. Zhang, L. Zheng, Z. Wang, Y. Wei. Circle Loss: A Unified Perspective of Pair Similarity Optimization. .",
            "url": "https://l1aoxingyu.github.io/blogpages/reid/2020/05/29/fastreid.html",
            "relUrl": "/reid/2020/05/29/fastreid.html",
            "date": " • May 29, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://l1aoxingyu.github.io/blogpages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Self-Supervised Learning 入门介绍",
            "content": "引子 . 最近 self-supervised learning 变得非常火，首先是 kaiming 的 MoCo 引发一波热议，然后最近 Yann 在 AAAI 上讲 self-supervised learning 是未来。 所以觉得有必要了解一下 SSL，也看了一些 paper 和 blog，最后决定写这篇文章作为一个总结。 . . 什么是 Self-Supervised Learning . 首先介绍一下到底什么是 SSL，我们知道一般机器学习分为监督学习，非监督学习和强化学习。 而 self-supervised learning 是无监督学习里面的一种，主要是希望能够学习到一种通用的特征表达用于下游任务。 其主要的方式就是通过自己监督自己，比如把一段话里面的几个单词去掉，用他的上下文去预测缺失的单词，或者将图片的一些部分去掉，依赖其周围的信息去预测缺失的 patch。 . 根据我看的文章，现在 self-supervised learning 主要分为两大类：1. Generative Methods；2. Contrastive Methods。 下面我们分别简要介绍一下这这两种方法。 . Generative Methods . 首先我们介绍一下 generative methods。 这类方法主要关注 pixel space 的重建误差，大多以 pixel label 的 loss 为主。 主要是以 AutoEncoder 为代表，以及后面的变形，比如 VAE 等等。 对编码器的基本要求就是尽可能保留原始数据的重要信息，所以如果能通过 decoder 解码回原始图片，则说明 latent code 重建的足够好了。 . . 这种直接在 pixel level 上计算 loss 是一种很直观的做法，除了这种直接的做法外，还有生成对抗网络的方法，通过判别网络来算 loss。 . 对于 generative methods，有一些问题，比如： . 基于 pixel 进行重建计算开销非常大； | 要求模型逐像素重建过于苛刻，而用 GAN 的方式构建一个判别器又会让任务复杂和难以优化。 | 从这个 blog 中我看到一个很好的例子来形容这种 generative methods。 对于一张人民币，我们能够很轻易地分辨其真假，说明我们对其已经提取了一个很好的特征表达，这个特征表达足够去刻画人民币的信息， 但是如果你要我画一张一模一样的人民币的图片，我肯定没法画出来。 通过这个例子可以明显看出，要提取一个好的特征表达的充分条件是能够重建，但是并不是必要条件，所以有了下面这一类方法。 . . Contrasive self-supervised learning . 除了上面这类方法外，还有一类方法是基于 contrastive 的方法。 这类方法并不要求模型能够重建原始输入，而是希望模型能够在特征空间上对不同的输入进行分辨，就像上面美元的例子。 . 这类方法有如下的特点：1. 在 feature space 上构建距离度量；2. 通过特征不变性，可以得到多种预测结果；3. 使用 Siamese Network；4. 不需要 pixel-level 重建。 正因为这类方法不用在 pixel-level 上进行重建，所以优化变得更加容易。当然这类方法也不是没有缺点，因为数据中并没有标签，所以主要的问题就是怎么取构造正样本和负样本。 . 目前基于 contrastive 的方法已经取得了很好的紧张，在分类任上已经接近监督学习的效果，同时在一些检测、分割的下游任务上甚至超越了监督学习作为 pre-train的方法。 . 下面是这两类方法的总结图片。 . . 为什么需要 self-supervised learning . 上面我们讲了什么是 self-supervised learning，那么为什么我们需要自监督学习呢，以及它能够给我们带来哪些帮助？ . 在目前深度学习发展的情况下，对于监督学习，我们希望使用更少的标注样本就能够训练一个泛化能力很好的模型，因为数据很容易获取，但是标注成本却是非常昂贵的。 而在强化学习中，需要大量的经验对 agent 进行训练，如果能搞减少 agent 的尝试次数，也能够加速训练。 除此之外，如果拿到一个好的特征表达，那么也有利于做下游任务的 finetune 和 multi-task 的训练。 . 最后我们总结一下监督学习和自监督学习的特点，其中 supervised learning 的特点如下： . 对于每一张图片，机器预测一个 category 或者是 bounding box | 训练数据都是人所标注的 | 每个样本只能提供非常少的信息(比如 1024 个 categories 只有 10 bits 的信息) | 于此对比的是，self-supervised learning 的特点如下： . 对于一张图片，机器可以预任何的部分 | 对于视频，可以预测未来的帧 | 每个样本可以提供很多的信息 | 所以通过自监督学习，我们可以做的事情可以远超过监督学习，也难怪 Yann 未来看好 self-supervised learning。 目前出现的性能很好的文章主要是基于 contrastive 的方法，所以下面我们介绍几篇基于 contrastive 方法的文章。 . Contrastive Predictive Coding . 第一篇文章是 Representation Learning with Contrastive Predictive Coding。 这篇文章主要是通过 contrastive 的方式在 speech, images, text 和 reinforcement learning 中都取得了很好的效果。 . 从前面我们知道，由一个原始的 input 去建模一个 high-level representation 是很难的，这也是自监督学习想做的事情。 其中常用的策略是: future，missing 和 contextual，即预测未来的信息，比如 video 中当前帧预测后面的帧；丢失的信息或者是上下文的信息，比如 NLP 里面的 word2vec 和 BERT。 . 对于一个目标 x 和他的上下文 c 来说，直接去建模输出 $p(x|c)$ 会损失很多信息，将 target x 和 context c 更合适的建模方式是最大化他们之间的 mutual information，即下面的公式 . I(x;c)=∑x,cp(x,c)log⁡p(x∣c)p(x)I(x; c)= sum_{x, c} p(x, c) log frac{p(x | c)}{p(x)}I(x;c)=x,c∑​p(x,c)logp(x)p(x∣c)​ . 优化了他们之间的互信息，即最大化 $ frac{p(x | c)}{p(x)}$，说明 $p(x|c)$ 要远大于 $p(x)$，即在给定 context c 的情况下， 要找到专属于 c 的那个 x，而不是随机采样的 x。 . 基于这个观察，论文对 density ratio 进行建模，这样可以保留他们之间的互信息 . fk(xt+k,ct)∝p(xt+k∣ct)p(xt+k)f_{k} left(x_{t+k}, c_{t} right) propto frac{p left(x_{t+k} | c_{t} right)}{p left(x_{t+k} right)}fk​(xt+k​,ct​)∝p(xt+k​)p(xt+k​∣ct​)​ . 对于这个 density ratio，可以构建左边的函数 f 去表示它，只要基于函数 f 构造下面的损失函数，优化这个损失函数就等价于优化这个 density ratio，下面论文会证明这一点。 . LN=−EX[log⁡fk(xt+k,ct)∑xj∈Xfk(xj,ct)] mathcal{L}_{ mathrm{N}}=- underset{X}{ mathbb{E}} left[ log frac{f_{k} left(x_{t+k}, c_{t} right)}{ sum_{x_{j} in X} f_{k} left(x_{j}, c_{t} right)} right]LN​=−XE​[log∑xj​∈X​fk​(xj​,ct​)fk​(xt+k​,ct​)​] . 而这个损失函数，其实就是一个类似交叉熵的函数，分子是正样本的概率，分母是正负样本的概率求和。 . 下面我们证明如果能够最优化这个损失函数，则等价于优化了 density ratio，也就优化了互信息。 . 首先将这个 loss 函数变成概率的形式，最大化这个正样本的概率分布，然后通过 bayesian 公式进行推导，其中 X 是负样本，和 $x_i$ 以及 c 都无关。 . p(xi∣X,ct)=p(X∣xi,ct)p(xi∣ct)∑j=1Np(X∣xj,ct)p(xj∣ct)=p(xi∣ct)∏l≠ip(xl)∑j=1Np(xj∣ct)∏l≠jp(xl)=p(xi∣ct)p(xi)∑j=1Np(xj∣ct)p(xj) begin{aligned} p left(x_i | X, c_{t} right) &amp;= frac{p(X | x_i, c_t) p(x_i | c_t)}{ sum_{j=1}^N p(X | x_j, c_t) p(x_j | c_t)} . &amp;= frac{p left(x_{i} | c_{t} right) prod_{l neq i} p left(x_{l} right)}{ sum_{j=1}^{N} p left(x_{j} | c_{t} right) prod_{l neq j} p left(x_{l} right)} &amp;= frac{ frac{p left(x_{i} | c_{t} right)}{p left(x_{i} right)}}{ sum_{j=1}^{N} frac{p left(x_{j} | c_{t} right)}{p left(x_{j} right)}} end{aligned}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;p(xi​∣X,ct​)​=∑j=1N​p(X∣xj​,ct​)p(xj​∣ct​)p(X∣xi​,ct​)p(xi​∣ct​)​=∑j=1N​p(xj​∣ct​)∏l​=j​p(xl​)p(xi​∣ct​)∏l​=i​p(xl​)​=∑j=1N​p(xj​)p(xj​∣ct​)​p(xi​)p(xi​∣ct​)​​​&lt;/span&gt;&lt;/span&gt; . 通过上面的推导，可以看出优化这个损失函数其实就是在优化 density ratio。论文中把 f 定义成一个 log 双线性函数，后面的论文更加简单，直接定义为了 cosine similarity。 . fk(xt+k,ct)=exp⁡(zt+kTWkct)f_{k} left(x_{t+k}, c_{t} right)= exp left(z_{t+k}^{T} W_{k} c_{t} right)fk​(xt+k​,ct​)=exp(zt+kT​Wk​ct​) . 有了这个 loss，我们只需要采集正负样本就可以了。 对于语音和文本，可以充分利用了不同的 k 时间步长，来采集正样本，而负样本可以从序列随机取样来得到。 对于图像任务，可以使用 pixelCNN 的方式将其转化成一个序列类型，用前几个 patch 作为输入，预测下一个 patch。 . . . Deep InfoMax . 通过上面的分析和推导，我们有了这样一个通用的框架，那么 deep infomax 这篇文章就非常好理解了，其中正样本就是第 i 张图片的 global feature 和中间 feature map 上个的 local feature，而负样本就是另外一张图片作为输入，非常好理解。 . . Contrastive MultiView Coding . 除了像上面这样去构建正负样本，还可以通过多模态的信息去构造，比如同一张图片的 RGB图 和 深度图。 CMC 这篇 paper 就是从这一点出发去选择正样本，而且通过这个方式，每个 anchor 不仅仅只有一个正样本，可以通过多模态得到多个正样本，如下图右边所示。 . . 现在我们能够拿到很多正样本，问题是怎么获得大量的负样本，对于 contrastive loss 而言，如何 sample 到很多负样本是关键，mini-batch 里面的负样本太少了，而每次对图片重新提取特征又非常的慢。虽然可以通过 memory bank 将负样本都存下来，但是效果并不好，所以如何节省内存和空间获得大量的负样本仍然没有很好地解决。 . MoCo . 有了上面这么多工作的铺垫，其实 contrastive SSL 的大框架已经形成了，MoCo 这篇文章也变得很好理解，可以把 target x 看成第 i 张图片的随机 crop，他的正样本通过一个 model ema 来得到，可以理解为过去 epochs 对这张图片的 smooth aggregation。 而负样本则从 memory bank 里面拿，同时 memory bank 的 feature 也是通过 model ema 得到，并且通过队列的形式丢掉老的 feature。 . . MoCo 通过工程的方式，和一些 trick，比如 model ema 和 shuffleBN 来解决之前没法很好 sample 负样本的问题。 . SimCLR . 最近，hinton 组也放了一篇做 ssl 的 paper，其实都是用的同一套框架，也没有太多的 novelty。 虽然摘要里面说可以抛弃 memory bank，不过细看论文，训练的 batchsize 需要到几千，要用32-128 cores 的 TPU，普通人根本用不起。 . 不过这篇文章系统地做了很多实验，比如探究了一下数据增强的影响，以及的 projection head 的影响等，不过也没有从理论上去解释这些问题，只是做了实验之后获得了一些结论。 . Results . . 最后展示了不同方法的结果，可以看到在性能其实已经逼近监督学习的效果，但是需要 train 4x 的时间，同时网络参数也比较大。 . 虽然性能没有超过监督学习，不过我认为这仍然给了我们很好的启发，比如训练一个通用的 encoder 来接下游任务，或者是在 cross domain 的时候只需要少量样本去 finetune，这都会给实际落地带来收益。 . Reference . contrastive self-supervised learning . deep infomax 和 深度学习中的互信息 .",
            "url": "https://l1aoxingyu.github.io/blogpages/summary/self-supervised%20learning/2020/02/20/ssl-survey.html",
            "relUrl": "/summary/self-supervised%20learning/2020/02/20/ssl-survey.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "A Simple Framework for Contrastive Learning of Visual Representations" 阅读笔记",
            "content": "介绍 . 这篇文章是 Hinton 团队出品的，主要做的是目前炙手可热的领域，self supervised learning， 提出了一个简单的框架来解决 visual representation 中的 contrastive learning。 其实前两个月 kaiming 团队也提出了一个叫 MoCo 的方法来解决这个问题，这篇文章总体思路和 MoCo 几乎一样，最大的 contribution 我认为是去探索了框架中的每个部分分别对最终结果的影响。 最后根据论文的发现，作者调出了目前最强的结果如下，点数非常高。 . . 主要贡献 . SimCLR 整体框架如下，和目前其他的方法是一致的 . . 主要由四个部分组成： . 随机数据增强 | 神经网络 encoder | project head $g( centerdot)$ 进行非线性映射和降维 | contrastive loss 函数 | li,j=−log⁡exp⁡(sim(zi,zj)/τ)∑k≠iexp⁡(sim(zi,zk)/τ)l_{i,j} = - log frac{ exp(sim(z_i, z_j)/ tau)}{ sum_{k neq i} exp(sim(z_i, z_k)/ tau)}li,j​=−log∑k​=i​exp(sim(zi​,zk​)/τ)exp(sim(zi​,zj​)/τ)​ . Memory bank . 这篇文章提出了可以去掉 memory bank 进行训练，实际上并不可行。 因为作者使用了 8192 的 batch size，这样每个 batch 可以产生 16382 个负样本。 当然当前 batch 提取的 feature 对比 memory bank 更好，但是这需要 128 cores 的 TPU 进行训练，对于财大气粗的 google 当然用得起，对于普通的研究人员来讲，还是老老实实用 memory bank 吧。 . Global BN . 使用 contrastive loss 进行训练的时候，正样本是一张相同的图片通过不同的数据增强方式得到的，这两张图片都在相同的 batch 中，这样非常因为 bn 统计量的问题出现信息泄露。 这篇文章使用了 global bn 的方式来就解决，即大 batch 下面，使用所有图片统计 bn 的均值和方差。 当然使用 MoCo 中的 suffle bn 也是可以的。 . 数据增强 . 本文系统的探索了数据增强对于表示学习的影响，其中 random cropping 和 random color distortion 是非常有用的。 random cropping 可以产生很多小 patch，但是这些小 patch 有着非常相似的颜色分布，所以可以用 color distortion 去弥补这个问题。 . Projection Head . 不同的 head 也有着不同的影响 . . 可以看出，直接使用 global average feature 效果是最差的，而一个 non-linear head 有着最好的效果。 . 其他的因素 . 除了上面这些因素之外，还用 contrastive loss 中的 temperatual factor $ tau$ 的影响，以及是否对 feature 做归一化。 当然这些在别的 paper 中都有了结论，这里就不再赘述。 . 另外还有 batch size 的影响，因为其没有用 memory bank，当然 batch size 越大，包含越多的负样本，效果越好。 . 总结 . 总体来说，这篇文章通过了很多实验来验证到底是哪些因素影响了 SSL 的效果。 很多结论也非常 solid，效果也非常好，可以指导很多调参的工作， 但是 novelty 上并没有给人太大的启发。 .",
            "url": "https://l1aoxingyu.github.io/blogpages/self-supervised%20learning/2020/02/15/simclr.html",
            "relUrl": "/self-supervised%20learning/2020/02/15/simclr.html",
            "date": " • Feb 15, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://l1aoxingyu.github.io/blogpages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://l1aoxingyu.github.io/blogpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://l1aoxingyu.github.io/blogpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}