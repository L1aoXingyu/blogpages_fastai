{
  
    
        "post0": {
            "title": "CSAPP 之 Bomb Lab",
            "content": "0x0 Introduction . 该实验提供了一个 “binary bomb”，这是一个编译好的二进制程序，在运行过程中需要用户提供6次输入，如果有任何一次不正确，炸弹就会爆炸。 . 在这个实验中，我们需要做逆向工程对炸弹进行拆解，对这个二进制程序进行反汇编后，通过阅读他的汇编代码，获取需要输入的6个字符串是什么。 . 在这次实验中，主要使用的工具如下： . objdump: 将二进制文件进行反汇编； | vscode：阅读和注解汇编代码的编辑器； | cgdb(gdb)：单步调试汇编代码的 debugger； | . 通过下面的命令即可对二进制程序进行反汇编，最终获得 bomb.asm 代码，还可以在 vscode 中安装 asm 插件实现代码的高亮。 . objdump bomb -d &gt; bomb.asm . . 0x1.0 phase_1 . 下面的c代码包含了第一阶段的所有内容 . /* Hmm... Six phases must be more secure than one phase! */ input = read_line(); /* Get input */ phase_1(input); /* Run the phase */ phase_defused(); /* Drat! They figured it out! * Let me know how they did it. */ /* Border relations with Canada have never been better. */ . 代码非常简单，程序读取输入之后，送到 phase_1 这个函数中进行处理，如果函数顺利退出则表示炸弹被拆掉，输入指令正确。 . 通过在汇编代码中搜索 phase_1 即可找到其在汇编代码中的片段 . 0000000000400ee0 &lt;phase_1&gt;: 400ee0: 48 83 ec 08 sub $0x8,%rsp 400ee4: be 00 24 40 00 mov $0x402400,%esi 400ee9: e8 4a 04 00 00 callq 401338 &lt;strings_not_equal&gt; 400eee: 85 c0 test %eax,%eax 400ef0: 74 05 je 400ef7 &lt;phase_1+0x17&gt; 400ef2: e8 43 05 00 00 callq 40143a &lt;explode_bomb&gt; 400ef7: 48 83 c4 08 add $0x8,%rsp 400efb: c3 retq . 简单阅读这一段汇编代码，主要是调用了 strings_not_equal 这个函数，而通过名字可以大概了解这就是一个判断字符串是否相等的函数。 . 而上面有一段代码将地址 0x402400 移动到寄存器 %esi 中，通过查表可以了解到 %esi 是函数的第二个参数，而调用 phase_1 时，汇编代码如下，发现是通过 read_line 这个函数，将用户输入的结果移动到寄存器 %rdi 中，而这恰好是函数的第一个参数，所以可以得到第一阶段即要对比用户输入的字符串和 0x402400 这个地址存放的字符串是否相等。 . 400e32: e8 67 06 00 00 callq 40149e &lt;read_line&gt; 400e37: 48 89 c7 mov %rax,%rdi 400e3a: e8 a1 00 00 00 callq 400ee0 &lt;phase_1&gt; . 通过 cgdb 启动程序，在 phase_1 处打断点，注意这里断点需要打到汇编代码中，可以直接通过 b phase_1 打断点，也可以通过地址进行打断点，比如 b *0x400ee0，接着直接运行。 . 在 gdb 中通过 ni 可以在汇编代码中进行单步运行，我们可以运行到 strings_not_equal 这个函数调用前，接着通过 p (char*)0x402400 即可获得这个地址对应的字符串如下 . (gdb) p (char*)(0x402400) $6 = 0x402400 &quot;Border relations with Canada have never been better.&quot; . 接着可以尝试在 phase_1 输入这段字符串，可以获得下面的结果，说明 pahse_1 的输入指令正确，炸弹已经成功拆除了。 . Phase 1 defused. How about the next one? .",
            "url": "https://l1aoxingyu.github.io/blogpages/operation%20system/c/csapp/assembly/2022/02/03/csapp-bomb.html",
            "relUrl": "/operation%20system/c/csapp/assembly/2022/02/03/csapp-bomb.html",
            "date": " • Feb 3, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "如何在 OneFlow 中开发一个新的 UserOp",
            "content": "Introduction . 这篇文章主要记录了笔者学习使用 oneflow 开发 userOp 中的整个过程，将这个流程写成文章是为了进一步加深自己的学习和理解，毕竟输出才是最好的学习过程。 . 因为这篇文章是自己学习过程中的梳理，所以文章中可能会缺少一些背景知识的介绍，笔者会尽量弱化这些背景知识跟本文核心内容之间的联系，尽量用深度学习框架中都有的概念来讲解这些内容。 . 通过阅读这篇文章，你可以了解到 OneFlow 开发 UserOp 的整体流程以及流程中一些关键步骤的作用。 . 整体开发流程 . 要实现一个 Op 分为两个大的部分，分别是 Op 的注册和 kernel 的实现，Op 是用来描述逻辑上的概念，kernel 实现了具体在物理设备上的运算。所以 Op 更多会关注输入输出的参数名和 shape，计算中需要的属性等等，而 kernel 更关注在不同的设备上的具体计算流程，比如在 cpu 和 gpu 上进行计算就需要不同的实现方式。 . 所以整体上需要实现一个 Op 只需要注册号 Op，同时完成这个 Op 在不同设备下的 kernel 实现即可。不过因为在模型搭建中还需要考虑更多的问题，比如 Op 需要计算梯度，因为在深度学习中需要通过反向传播算法计算整个计算图中参数的梯度，用户只会显示构建前向图，反向图如何根据前向图进行自动构建等等，所以完成一个 Op 还需要一些额外的步骤，下面我们具体来讲一下每个流程以及其目的。 . Op 注册 . 第一步需要完成 Op 的注册，即为每个 Op 选择一个唯一的名字，这样当你使用这个名字的时候系统就知道你要调用这个 Op。同时在注册 Op 的时候还需要指定 Op 的输入、输出的参数名，属性的类型和参数名，数据 shape 的推断，参数类型的推断等，最后一个非常重要的作用是设置当前 Op 的 SBP 签名。 . SBP 是 oneflow 区别于其他框架的一个重要特性，在 oneflow 中会使用一致性视角来看待所有的张量，用于简化分布式训练，在这个视角下，整个集群会被抽象成一台机器，用户不用关系具体集群的通信细节，只需要关注逻辑上的计算即可，而在逻辑上整个集群和单卡并没有什么区别。在一致性视角下就有了 sbp 等重要概念，要了解这些内容可以查看 集群的一致性视角 - OneFlow。 . 因为一个 Op 不仅需要前向计算，还需要有梯度计算以进行反向传播，所以需要注册 Op 和 Op_grad 分别用于前向和反向。除此之外，还需要额外注册一个 backward Op conf，这个作用是将 Op 和 Op_grad 进行绑定，在静态图中构图时能够自动基于前向的 Op 生成反向的计算图。 . 注册 Op 的具体实现 . 前面大致介绍了 Op 的注册流程以及作用，下面以 leaky_relu 为例进行简要说明。首先通过宏 REGISTER_USER_OP 对 Op 进行注册，在注册过程中会返回一个 OpRegistry 对象，可以通过对个对象的方法进行调用来设置 Op 的属性。 . 设定输入，输出和属性 . REGISTER_USER_OP(&quot;leaky_relu&quot;) .Input(&quot;x&quot;) .Output(&quot;y&quot;) .Attr&lt;float&gt;(&quot;alpha&quot;) . 通过 Input(&quot;x&quot;) 和 Output(&quot;y&quot;) 设置了输入和输出的参数名，Attr&lt;float&gt;(&quot;alpha&quot;) 则设置了数据类型是 float 的参数 alpha。 . 检查 TensorDesc 的合法性 . .SetTensorDescInferFn([](user_op::InferContext* ctx) -&gt; Maybe&lt;void&gt; { const Shape&amp; x_shape = ctx-&gt;InputShape(&quot;x&quot;, 0); Shape* y_shape = ctx-&gt;OutputShape(&quot;y&quot;, 0); *y_shape = x_shape; return Maybe&lt;void&gt;::Ok(); }) . SetTensorDescInferFn 通过注册回调函数对数据描述进行检查，这里通过输入的 shape 给输出指定 shape 以分配对应的内存。常规的 Op 只需要写一个回调函数，内部会调用 logical 和 physical 的推导设置为同一套，而有一些复杂的 Op 则需要分别写 logical 和 physical 的相同推导。 . 设置和推理输出数据类型 . .SetDataTypeInferFn([](user_op::InferContext* ctx) -&gt; Maybe&lt;void&gt; { *ctx-&gt;OutputDType(&quot;y&quot;, 0) = ctx-&gt;InputDType(&quot;x&quot;, 0); return Maybe&lt;void&gt;::Ok(); }); . 因为是激活函数，所以设置输出的数据类型和输入一致即可。 . 设置 SBP Signature . .SetGetSbpFn([](user_op::SbpContext* ctx) -&gt; Maybe&lt;void&gt; { const user_op::TensorDesc&amp; x_tensor = ctx-&gt;LogicalTensorDesc4InputArgNameAndIndex(&quot;x&quot;, 0); FOR_RANGE(int64_t, i, 0, x_tensor.shape().NumAxes()) { ctx-&gt;NewBuilder().Split(user_op::OpArg(&quot;x&quot;, 0), i).Split(user_op::OpArg(&quot;y&quot;, 0), i).Build(); } return Maybe&lt;void&gt;::Ok(); }) . 在 sbp signature 的设置中，默认支持 broadcast，如果一下支持其他类型的输入和输出，则需要手工进行设置，比如对于上面的 leaky_relu 激活函数，支持在输入和输出的任意维度进行 split，所以可以通过一个在 Axes 上的循环建立不同的 sbp signature。 . 通过上面的方式可以对前向 Op 和反向 Op 进行注册，最后还需要通过宏 REGISTER_USER_OP_GRAD 注册 Op_grad，通过回调函数 SetGenBackwardOpConfFn 将前向 Op 和反向 Op 绑定起来，这样在静态图构建前向图时能够自动构建反向图。 . 实现 Kernel 计算 . Kernel 是实际计算的控制单元，决定了用什么物理设备进行计算，针对什么样的数据类型以及用哪一种计算方式进行计算，所以我们需要实现在 cpu 和 gpu 下的计算流程，最终实现同一个 Op 可以根据情况使用不同的设备进行计算。 . Kernel 计算的具体实现 . Kernel 的注册和 Op 类似，通过 REGISTER_USER_KERNEL 进行注册，在注册之前，需要完成实际的计算过程。 . Kernel 都需要继承 user_op::OpKernel 这个类，通过 override Compute 方法实现具体的计算过程，以 leaky_relu 为例。 . void Compute(user_op::KernelComputeContext* ctx) const override { const user_op::Tensor* x = ctx-&gt;Tensor4ArgNameAndIndex(&quot;x&quot;, 0); user_op::Tensor* y = ctx-&gt;Tensor4ArgNameAndIndex(&quot;y&quot;, 0); const int32_t elem_cnt = x-&gt;shape().elem_cnt(); const float alpha = ctx-&gt;Attr&lt;float&gt;(&quot;alpha&quot;); const T* x_ptr = x-&gt;dptr&lt;T&gt;(); T* y_ptr = y-&gt;mut_dptr&lt;T&gt;(); FOR_RANGE(int32_t, i, 0, elem_cnt) { y_ptr[i] = x_ptr[i] &gt; 0 ? x_ptr[i] : x_ptr[i] * alpha; } } . 整个计算过程如下： . 获得输入的 tensor “x” 和 输出 tensor “y”，这里的参数名在注册 Op 的时候已经确定； . | 计算 x 的元素个数； . | 获得属性 alpha 的值，为一个 float 类型； . | 获得 tensor x 和 tensor y 的指针用于后续的计算； . | 遍历所有的元素，根据 leaky_relu 的公式，如果 x[i] &gt; 0 则直接返回 x[i] 的结果，否则返回 alpha * x[i]。 . | 完成 Kernel 的具体计算流程之后，可以通过下面的方式完成 Kernel 的注册，SetCreateFn 可以将这个 Kernel 具体的计算进行绑定，SetIsMatchedHob 则接受一些表达式用于对设备和数据类型的判断，比如通过 REGISTER_CPU_LEAKY_RELU_KERNEL(float) 则表示在 cpu 设备上，输出 y 的类型是 float 时，使用 CpuLeakyReluKernel&lt;float&gt; 进行计算。 . #define REGISTER_CPU_LEAKY_RELU_KERNEL(dtype) REGISTER_USER_KERNEL(&quot;leaky_relu&quot;) .SetCreateFn&lt;CpuLeakyReluKernel&lt;dtype&gt;&gt;() .SetIsMatchedHob((user_op::HobDeviceType() == DeviceType::kCPU) &amp;&amp; (user_op::HobDataType(&quot;y&quot;, 0) == GetDataType&lt;dtype&gt;::value)); REGISTER_CPU_LEAKY_RELU_KERNEL(float) REGISTER_CPU_LEAKY_RELU_KERNEL(double) . 除了需要完成 cpu 的 Kernel 之外，还需要完成 gpu 的 Kernel，整体的注册流程是类似的，不过在 gpu 实现的时候可以充分利用 cuda 编程，这里就不再展开，cuda 相关的内容后续再写成一篇或者几篇文章进行介绍。 . 完成 functional 接口 . 在 c++ 端完成了 Op 的注册和 Kernel 的实现之后，需要导出到 python 端以及 eager 模式下 autograd engine 进行使用，这是需要利用 functional 进行接口的导出，这样可以通过 oneflow._C.xxx 在 python 中对注册接口进行调用。 . 注册 functional 接口的具体操作 . functional 接口的注册主要分为三个步骤： . 为接口增加前向和反向的 Functor 实现； . | 通过 m.add_functor&lt;impl::MyOpFunctor&gt;(&quot;MyOp&quot;) 将 Functor 注册到 Functional Library 中； . | 在 functional_api.yaml 中增加接口的配置文件自动生成接口。 . | 所有的 functor 函数都在 oneflow/core/functional/impl 中，被设计成 class 或者是 struct，可以持有一个或是多个 Op。 . 在 constructor 中将 Op 都构造好，通常只需要声明好 Op 的输入和输出，属性则可以省略。 . class LeakyReluFunctor { public: LeakyReluFunctor() { op_ = CHECK_JUST(one::OpBuilder(&quot;leaky_relu&quot;).Input(&quot;x&quot;).Output(&quot;y&quot;).Build()); } private: std::shared_ptr&lt;OpExpr&gt; op_; }; . 然后实现 operator() 接口，在这个接口中完成 Op 的所有计算流程，可以是多个 Op 的组合，oneflow 通过 dispatch op 的机制来调用 Op 下面具体执行计算的 kernel 完成计算流程。 . Maybe&lt;Tensor&gt; operator()(const std::shared_ptr&lt;one::Tensor&gt;&amp; x, const float&amp; alpha) const { MutableAttrMap attrs; JUST(attrs.SetAttr&lt;float&gt;(&quot;alpha&quot;, alpha)); return OpInterpUtil::Dispatch&lt;one::Tensor&gt;(*op_, {x}, attrs); } . 在 functional_api.yaml 中增加接口配置文件时，每个接口信息由三个字段组成，示例如下 . - name: &quot;xxx&quot; signature: &quot;R(Args...) =&gt; Func&quot; bind_python: True or False . 其中 name 表示导出到 python 接口的名字，signature 指定了接口函数的签名，签名需要和之前定义的 functor 一致，同时 Func 作为 signature 的函数名，需要和签名注册到 function library 中的函数名一致，因为需要在 c++ 中使用的是这个函数名。 . bind_python 表示是否需要为当前的接口生成 python 接口，所有的前向接口都会在 python 搭建模型中用到，所以需要导出到 python，而有的函数不会在 python 中被显示调用，比如求梯度的函数，只会在 c++ 中 autograd 用到，这时就可以不为这种函数增加 python 的接口。 . 注册 eager 求导逻辑 . 是为了实现 eager 下的 backward op conf，将 eager 过程中的前向 Op 自动绑定反向 Op，所以需要再次注册 eager 下的求导逻辑，主要的代码在 oneflow/core/autograd/gradient_funcs 中。 . eager 求导逻辑的具体实现 . 首先初始化一个结构体来保存一些属性，这个结构体继承 AutoGradCaptureState，比如requires_grad, alpha 等参数。 . struct LeakyReluCaptureState : public AutoGradCaptureState { bool requires_grad; float alpha; }; . 接着需要实现一个 class 继承 OpExprGradFunction，同时需要特例化他的状态结构体 class LeakyRelu : public OpExprGradFunction&lt;LeakyReluCaptureState&gt; . 接着需要实现下面是个成员函数： . 在 Init 中完成一些初始化工作，可以根据前向 Op 的 proto 来初始化一个 attrs . Maybe&lt;void&gt; Init(const OpExpr&amp; op) override { const auto* fw_op_expr = dynamic_cast&lt;const UserOpExpr*&gt;(&amp;op); CHECK_NOTNULL_OR_RETURN(fw_op_expr); base_attrs_ = MakeAttrMapFromUserOpConf(fw_op_expr-&gt;proto()); return Maybe&lt;void&gt;::Ok(); } . 接着在 Capture 中进行输入的检查，查看是否需要对它求梯度，如果需要的话，就保存一些需要在求梯度的时候使用的内容 . Maybe&lt;void&gt; Capture(LeakyReluCaptureState* ctx, const TensorTuple&amp; inputs, const TensorTuple&amp; outputs, const AttrMap&amp; attrs) const override { CHECK_EQ_OR_RETURN(inputs.size(), 1); ctx-&gt;requires_grad = inputs.at(0)-&gt;requires_grad(); if (!ctx-&gt;requires_grad) { return Maybe&lt;void&gt;::Ok(); } ComposedAttrMap composed_attrs(attrs, base_attrs_); ctx-&gt;alpha = JUST(composed_attrs.GetAttr&lt;float&gt;(&quot;alpha&quot;)); ctx-&gt;SaveTensorForBackward(inputs.at(0)); return Maybe&lt;void&gt;::Ok(); } . Apply 是具体求导的过程，首先可以对后面传回来的梯度 out_grads 做一些检查，最后调用在 functional 中注册的接口进行梯度的具体计算，然后将结果写回 in_grads . Maybe&lt;void&gt; Apply(const LeakyReluCaptureState* ctx, const TensorTuple&amp; out_grads, TensorTuple* in_grads) const override { CHECK_EQ_OR_RETURN(out_grads.size(), 1); in_grads-&gt;resize(1); if (ctx-&gt;requires_grad) { const auto&amp; x = ctx-&gt;SavedTensors().at(0); in_grads-&gt;at(0) = JUST(functional::LeakyReluGrad(x, out_grads.at(0), ctx-&gt;alpha)); } return Maybe&lt;void&gt;::Ok(); } . 最后通过 REGISTER_OP_EXPR_GRAD_FUNCTION 注册这个 op 的梯度计算逻辑，第一个参数是之前在 user_op 注册中的名字， 第二个参数是刚才定义的类名，比如 REGISTER_OP_EXPR_GRAD_FUNCTION(&quot;leaky_relu&quot;, LeakyRelu)。 . Ending . 非常感谢你能看到最后，这篇文章主要是从自己学习的角度出发进行编写，所以整篇文章的目的也是为了梳理自己学习过程中的一些总结，如果这篇文章有任何地方能够帮到你，那就更好了。 . 最后如果你发现文章有任何纰漏欢迎斧正，如果你有任何建议和意见，也欢迎去评论区留言。 . Reference . 集群的一致性视角 - OneFlow . | https://github.com/Oneflow-Inc/oneflow/pull/5854/files . | https://github.com/Oneflow-Inc/oneflow/pull/4130 . | https://github.com/Oneflow-Inc/oneflow/pull/5797 . | https://github.com/Oneflow-Inc/oneflow/wiki/Functional-Interface . | https://github.com/Oneflow-Inc/oneflow/pull/5329 . | https://github.com/Oneflow-Inc/oneflow/pull/6544 . | https://cloud.tencent.com/developer/article/1891442 . | .",
            "url": "https://l1aoxingyu.github.io/blogpages/deep%20learning/userop/dl%20framework/2021/11/18/oneflow-userOp.html",
            "relUrl": "/deep%20learning/userop/dl%20framework/2021/11/18/oneflow-userOp.html",
            "date": " • Nov 18, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "AutoDiff 介绍以及简单的代码实现",
            "content": "Introduction . 梯度的计算对于各类基于梯度下降的优化方法来说非常重要，其中应用最为广泛的便是目前非常流行的神经网络中的训练，而对于深度神经网络来说，手工求解等各类求解梯度的方法非常繁琐同时也难以计算，如何有效求解梯度对于高效地训练网络来说也变得异常重要。 . 在这篇文章中会介绍一下目前主流深度学习框架中都在使用的自动微分技术，通过这个技术可以实现高效的梯度求解，使得大规模的深度网络训练成为可能。 . 什么是自动微分(AutoDiff)？ . 一般来说在计算机中使用程序求解梯度有三种方式，分别是自动微分，数值微分和符号微分，下面先简单介绍一下数值微分和符号微分，以此来引出自动微分。 . 数值微分 主要使用的方法是有限差分，因为导数可以通过下面的极限来定义 . f′(x)=lim⁡h→0f(x+h)−f(h)hf&amp;#x27;(x) = lim_{h rightarrow 0} frac{f(x + h) - f(h)}{h}f′(x)=h→0lim​hf(x+h)−f(h)​ . 所以在计算机中可以通过取一个非常小的 h 来模拟这个过程，但是通过公式也可以看出，数值微分在使用的时候计算代价非常高，因为每次计算微分的时候都需要进行前向计算，同时也存在数值稳定性的问题。 虽然不能直接在大规模计算中用来求导，不过它也有一定的作用，就是可以用来测试单个 Op 的导数是不是正确的，通常误差范围取值 1e-6. . 符号微分 主要通过符号微分的计算来推导数学公式，典型的代表就是 mathematica。这种计算方法虽然在数学上是严格正确的，但是会导致结果计算非常复杂且冗余，比如下面对两层的 soft relu 求导公式就已经非常复杂了 . . 同时有一些计算过程会被反复的重新计算，并不适合用在复杂的神经网络中，我们的目标并不是求每一个参数梯度的公式，而是系统通过计算过程获得最终的数值。 . 自动微分 是通过程序来自动求解梯度的一种通用方法，他不像数值微分在每次计算梯度的时候需要重复进行前向计算，也不像符号微分一样在计算梯度的时候需要完整推导数学公式，其会将一个复杂的计算转换成一系列 primitive 算子，然后依次对这个中间的单个算子进行梯度求解的过程，下面我们开始讲解今天的主角 AutoDiff。 . AutoDiff 的数学原理 . AutoDiff 在数学上有两种计算方式，分别是 forward mode 和 reverse mode，下面我们分别通过例子来讲一下这两种 mode 是如何进去梯度求解的。 . AutoDiff 中 forward mode 的实现原理 . forward mode 在计算梯度的时候会从最开始往后进行计算，比如为了获得 y 对 x1x_1x1​ 的导数，会不断计算中间变量 viv_ivi​ 对 x1x_1x1​ 的导数，然后使用链式法则最后就可以获得 y 对 x1x_1x1​ 的导数，下面的例子 f(x1,x2)=ln(x1)+x1x2+sin(x2)f(x_1, x_2) = ln(x_1) + x_1 x_2 + sin(x_2)f(x1​,x2​)=ln(x1​)+x1​x2​+sin(x2​) 可以清楚的描述这个过程，左边是通过 (x1,x2)(x_1, x_2)(x1​,x2​) 计算 y 的过程，右边是通过 forward mode 计算 y 对 x1x_1x1​ 导数的过程 . . 首先设定 x1=1,x2=0x_1=1, x_2=0x1​=1,x2​=0，接着不断计算 viv_ivi​ 对 x1x_1x1​ 的导数即可，中间可以通过链式法则进行计算，比如 v3˙=∂v3/∂v2∗v˙2 dot{v_3} = partial v_3 / partial v_2 * dot{v}_2v3​˙​=∂v3​/∂v2​∗v˙2​，而之前已经求了 v˙2 dot{v}_2v˙2​ 的结果，所以只需要计算当前一步的导数即可。 . 如果用数学语言来描述这个过程，就是需要计算 f 的 Jacobian 矩阵，其中 f:Rn→Rmf: mathcal{R}^n rightarrow mathcal{R}^mf:Rn→Rm 表示由 n 个独立的输入变量 xix_ixi​ 映射到 m 个相关的输出变量 yjy_jyj​。对于上面这种特殊的情况，可以把每一次 AutoDiff 的 foward pass 看成是将变量 x 的其中一个分量 x˙i=1 dot{x}_i = 1x˙i​=1 其他的分量设为 0 的一次推导。所以当 f:R→Rmf: mathcal{R} rightarrow mathcal{R}^mf:R→Rm 时，forward pass 非常高效，因为所有需要计算的偏导只需要进行一次 forward pass 即可。 . AutoDiff 中 reverse mode 的实现原理 . reverse mode 和他的名字一样，会从后往前计算导数，同样以刚才的例子 f(x1,x2)=ln(x1)+x1x2+sin(x2)f(x_1, x_2) = ln(x_1) + x_1 x_2 + sin(x_2)f(x1​,x2​)=ln(x1​)+x1​x2​+sin(x2​) 来描述这个过程，左边是通过 (x1,x2)(x_1, x_2)(x1​,x2​) 计算 f 的过程，右边则是通过 reverse mode 计算 f 对 x1x_1x1​ 导数的过程 . . 左边的计算流程是一样的，但是在求导数的过程却是相反的，设定 vˉi=∂f/∂vi bar{v}_i = partial f / partial v_ivˉi​=∂f/∂vi​，那么 vˉ5=yˉ=1 bar{v}_5 = bar{y} = 1vˉ5​=yˉ​=1，继续求 vˉ4=vˉ5∗∂v5/∂v4 bar{v}_4 = bar{v}_5 * partial v_5 / partial v_4vˉ4​=vˉ5​∗∂v5​/∂v4​，同样通过链式法则只需要计算当前一步的导数即可 . 如果用数学语言来描述这个过程，就是需要计算 f 的 Jacobian 矩阵，其中 f:Rn→Rmf: mathcal{R}^n rightarrow mathcal{R}^mf:Rn→Rm。同样对于上面这种情况，每一次 autodiff 的 backward pass 可以看成是将因变量 y 的其中一个分量 yˉi=1 bar{y}_i = 1yˉ​i​=1 其他分量设为 0 的一次推导。所以当 f:Rn→Rf: mathcal{R}^n rightarrow mathcal{R}f:Rn→R 时，reverse mode 非常高效，因为所有需要计算的偏导只需要进行一次 reverse pass 即可。 . 而我们知道在深度学习中 loss 一般都是一个标量，而参数一般都是一个高维张量，所以 f:Rn→Rf: mathcal{R}^n rightarrow mathcal{R}f:Rn→R 可以表示绝大多数深度学习模型的情况，通过上面的分析可以看出 reverse mode 效率更高，这也是为什么深度学习都是选择 reverse mode 进行梯度计算的原因，这也是反向传播算法的由来。 . AutoDiff 的代码实现 . 上面讲解了自动微分的数学原理，最终仍然需要使用程序进行实现才能真正应用到深度学习中，从上面可以看到 reverse AutoDiff 更加适用于机器学习中，下面我们来讲讲 reverse AutoDiff 的代码实现。 . 伪代码讲解 . . 上面的伪代码描述了 AutoDiff 的整体计算逻辑，在代码实现中使用图这种数据结构来表示计算过程，不管是前向计算还是反向计算都可以通过图中的边和节点来描述，一个节点可以表示一种运算，而节点的入边表示这种运算需要的所有输入，出边表示该节点被其他节点所消费，建立好计算图之后，最终可以通过输入数据来计算最终需要的结果，下面我们以上面计算图为例详细描述一下整个流程。 . 上面左边的黑色子图表示 forward pass，右边的红色子图是 backward pass。 . 如果设定 xˉi=∂y/∂xi bar{x}_i = partial y / partial x_ixˉi​=∂y/∂xi​，那么 xˉ4=1 bar{x}_4 = 1xˉ4​=1； | 接着计算 x4x_4x4​ 在 forward pass 中输入节点 x2,x3x_2, x_3x2​,x3​ 对应的梯度，其中 xˉ3=xˉ4∗∂x4/∂x3=xˉ4∗x2 bar{x}_3 = bar{x}_4 * partial x_4 / partial x_3 = bar{x}_4 * x2xˉ3​=xˉ4​∗∂x4​/∂x3​=xˉ4​∗x2，所以可以看到上面 xˉ3 bar{x}_3xˉ3​ 的输入节点是 x2x_2x2​ 和 xˉ4 bar{x}_4xˉ4​； | 同理接着计算 xˉ2=xˉ4∗x3 bar{x}_2 = bar{x}_4 * x_3xˉ2​=xˉ4​∗x3​，因为还有一部分 x2x_2x2​ 的梯度由 x3x_3x3​ 提供，所以可以将当前 x2x_2x2​ 梯度写为 xˉ21 bar{x}_2^1xˉ21​ 加以区分； | 然后计算 x3x_3x3​ 在 forward pass 中输入节点 x2x_2x2​ 的梯度，xˉ22=xˉ3∗∂x3/∂x2=xˉ3∗1 bar{x}_2^2 = bar{x}_3 * partial x_3 / partial x_2 = bar{x}_3 * 1xˉ22​=xˉ3​∗∂x3​/∂x2​=xˉ3​∗1，最终可以得到 xˉ2=xˉ21+xˉ22 bar{x}_2 = bar{x}_2^1 + bar{x}_2^2xˉ2​=xˉ21​+xˉ22​； | 最后可以计算 x2x_2x2​ 的输入节点 x1x_1x1​ 的梯度得到 xˉ1=xˉ2∗x2 bar{x}_1 = bar{x}_2 * x_2xˉ1​=xˉ2​∗x2​，这样也就完成了 backward pass 的构图。 | . 通过图将流程描述清楚了，左边给出了伪代码，可以简单解释一下。 . node_to_grad 即为要求的所有节点的梯度，首先设定 out 的梯度为 1，即 xˉ4=1 bar{x}_4 = 1xˉ4​=1； | get_node_list(out) 可以取得 out 在 forward pass 中的所有节点序列，在上图中计算的所有节点序列为 x1,x2,x3,x4x_1, x_2, x_3, x_4x1​,x2​,x3​,x4​； | reverse_topo_order(nodes) 求这些节点的反向拓扑排序，即对于 forward pass 这样的有向图，按照节点出现的先后顺序进行反向排序，上面的例子在排序完成之后是 x4,x3,x2,x1x_4, x_3, x_2, x_1x4​,x3​,x2​,x1​ 这样的顺序，排序后便可进行 reverse mode AutoDiff； | 接着对所有的节点进行遍历，首先遍历到 x4x_4x4​，然后计算 xˉ4=1 bar{x}_4=1xˉ4​=1，接着计算 x4x_4x4​ 的所有输入节点的梯度，即 xˉ21,xˉ3 bar{x}_2^1, bar{x}_3xˉ21​,xˉ3​，然后把他们加入到 node_to_grad 当中等待后续的使用； | 然后遍历到 x3x_3x3​，计算其梯度 xˉ3 bar{x}_3xˉ3​，接着计算输入节点梯度 xˉ22 bar{x}_2^2xˉ22​，然后将其加入 node_to_grad 当中； | 接着遍历到 x2x_2x2​，sum partial adjoints 即为 xˉ21+xˉ22=xˉ2 bar{x}_2^1 + bar{x}_2^2 = bar{x}_2xˉ21​+xˉ22​=xˉ2​ 这样获得了 x2x_2x2​ 的梯度，然后计算器输入节点 x1x_1x1​ 的梯度就完成了整个的计算过程； | . Python 代码实现 . 首先需要对图中的节点进行定义，这个节点中需要包含输入节点，以及他们的具体运算，也就是 Op。 . class Node(object): &quot;&quot;&quot;Node in a computation graph.&quot;&quot;&quot; def __init__(self): &quot;&quot;&quot;Constructor, new node is indirectly created by Op object __call__ method. Instance variables self.inputs: the list of input nodes. self.op: the associated op object, e.g. add_op object if this node is created by adding two other nodes. self.const_attr: the add or multiply constant, e.g. self.const_attr=5 if this node is created by x+5. self.name: node name for debugging purposes. &quot;&quot;&quot; self.inputs = [] self.op = None self.const_attr = None self.name = &quot;&quot; def __add__(self, other): &quot;&quot;&quot;Adding two nodes return a new node.&quot;&quot;&quot; if isinstance(other, Node): new_node = add_op(self, other) else: # Add by a constant stores the constant in the new node&#39;s const_attr field. # &#39;other&#39; argument is a constant new_node = add_byconst_op(self, other) return new_node def __mul__(self, other): &quot;&quot;&quot;TODO: Your code here&quot;&quot;&quot; if isinstance(other, Node): new_node = mul_op(self, other) else: # Multiply by a constant stores the constant in the new node&#39;s const_attr field. new_node = mul_byconst_op(self, other) return new_node # Allow left-hand-side add and multiply. __radd__ = __add__ __rmul__ = __mul__ def __str__(self): &quot;&quot;&quot;Allow print to display node name.&quot;&quot;&quot; return self.name __repr__ = __str__ . 对于每一个 node，建立上面的类，其中 self.inputs 记录了这个节点计算需要的输入节点，self.op 表示需要对这些输入进行的运算，__add__ 和 __mul__ 这两种 magic method 可以使得两个节点 node1 和 node2 可以支持 node1 + node2 和 node1 * node2 的操作。 . 接着需要定义具体的 Op，首先定义一个基类 Op，所有实际的 Op 都需要继承这个基类，在 Op 中需要包含 forward 和 backward 两种运算。 . class Op(object): &quot;&quot;&quot;Op represents operations performed on nodes.&quot;&quot;&quot; def __call__(self): &quot;&quot;&quot;Create a new node and associate the op object with the node. Returns - The new node object. &quot;&quot;&quot; new_node = Node() new_node.op = self return new_node def compute(self, node, input_vals): &quot;&quot;&quot;Given values of input nodes, compute the output value. Parameters - node: node that performs the compute. input_vals: values of input nodes. Returns - An output value of the node. &quot;&quot;&quot; raise NotImplementedError def gradient(self, node, output_grad): &quot;&quot;&quot;Given value of output gradient, compute gradient contributions to each input node. Parameters - node: node that performs the gradient. output_grad: value of output gradient summed from children nodes&#39; contributions Returns - A list of gradient contributions to each input node respectively. &quot;&quot;&quot; raise NotImplementedError class AddOp(Op): &quot;&quot;&quot;Op to element-wise add two nodes.&quot;&quot;&quot; def __call__(self, node_A, node_B): new_node = Op.__call__(self) new_node.inputs = [node_A, node_B] new_node.name = &quot;(%s+%s)&quot; % (node_A.name, node_B.name) return new_node def compute(self, node, input_vals): &quot;&quot;&quot;Given values of two input nodes, return result of element-wise addition.&quot;&quot;&quot; assert len(input_vals) == 2 return input_vals[0] + input_vals[1] def gradient(self, node, output_grad): &quot;&quot;&quot;Given gradient of add node, return gradient contributions to each input.&quot;&quot;&quot; return [output_grad, output_grad] . 上面在基类 Op 中定义了 __call__ 这个 magic method，通过 op() 会调用 __call__()，会创建一个新的 node，同时将 node 的 op 设为为当前 op。 . 另外以一个实际的 AddOp 为例，__call__ 方法需要输入为 node_A, node_B，首先调用 Op.__call__(self) 即先调用基类 Op 的 __call__ 方法，接着再将 node_A, node_B 设定为这个节点的输入节点。 . compute 是给定输入节点的值，计算输出结果，gradient 为给定当前节点的梯度，返回输入节点的梯度，这里因为是简单的 add 操作，所以梯度就为当前节点，下面可以看看一个更复杂的矩阵乘法 Op，逻辑是类似的，只是 compute 和 gradient 计算会更加复杂。 . class MatMulOp(Op): &quot;&quot;&quot;Op to matrix multiply two nodes.&quot;&quot;&quot; def __call__(self, node_A, node_B, trans_A=False, trans_B=False): &quot;&quot;&quot;Create a new node that is the result a matrix multiple of two input nodes. Parameters - node_A: lhs of matrix multiply node_B: rhs of matrix multiply trans_A: whether to transpose node_A trans_B: whether to transpose node_B Returns - Returns a node that is the result a matrix multiple of two input nodes. &quot;&quot;&quot; new_node = Op.__call__(self) new_node.matmul_attr_trans_A = trans_A new_node.matmul_attr_trans_B = trans_B new_node.inputs = [node_A, node_B] new_node.name = &quot;MatMul(%s,%s,%s,%s)&quot; % (node_A.name, node_B.name, str(trans_A), str(trans_B)) return new_node def compute(self, node, input_vals): &quot;&quot;&quot;Given values of input nodes, return result of matrix multiplication.&quot;&quot;&quot; &quot;&quot;&quot;TODO: Your code here&quot;&quot;&quot; assert len(input_vals) == 2 if node.matmul_attr_trans_A: input_vals[0] = np.transpose(input_vals[0]) if node.matmul_attr_trans_B: input_vals[1] = np.transpose(input_vals[1]) return np.dot(input_vals[0], input_vals[1]) def gradient(self, node, output_grad): &quot;&quot;&quot;Given gradient of multiply node, return gradient contributions to each input. Useful formula: if Y=AB, then dA=dY B^T, dB=A^T dY &quot;&quot;&quot; &quot;&quot;&quot;TODO: Your code here&quot;&quot;&quot; if node.matmul_attr_trans_A: dA = matmul_op(node.inputs[1], output_grad, trans_B=True) else: dA = matmul_op(output_grad, node.inputs[1], trans_B=True) if node.matmul_attr_trans_B: dB = matmul_op(output_grad, node.inputs[0], trans_A=True) else: dB = matmul_op(node.inputs[0], output_grad, trans_A=True) return [dA, dB] . 如果完成了所有 primitive Op 的构建，那么复杂的计算也可以由多个 primitive Op 构成，有了 Op 的计算，接着可以开始构建计算图了。 . class Executor: &quot;&quot;&quot;Executor computes values for a given subset of nodes in a computation graph.&quot;&quot;&quot; def __init__(self, eval_node_list): &quot;&quot;&quot; Parameters - eval_node_list: list of nodes whose values need to be computed. &quot;&quot;&quot; self.eval_node_list = eval_node_list def run(self, feed_dict): &quot;&quot;&quot;Computes values of nodes in eval_node_list given computation graph. Parameters - feed_dict: list of variable nodes whose values are supplied by user. Returns - A list of values for nodes in eval_node_list. &quot;&quot;&quot; node_to_val_map = dict(feed_dict) # Traverse graph in topological sort order and compute values for all nodes. topo_order = find_topo_sort(self.eval_node_list) &quot;&quot;&quot;TODO: Your code here&quot;&quot;&quot; for node in topo_order: if isinstance(node.op, PlaceholderOp): continue input_vals = [node_to_val_map[input_node] for input_node in node.inputs] node_val = node.op.compute(node, input_vals) node_to_val_map[node] = node_val # Collect node values. node_val_results = [node_to_val_map[node] for node in self.eval_node_list] return node_val_results . Executor 首先将所有需要求值的 node 收集到 eval_node_list 当中，不需要求值的 node 最终的结果并不需要保留，可以起到节约内存的目的，接着在 run 中计算 forward pass，通过对有向图进行拓扑排序可以获得节点的先后顺序，接着遍历每一个节点，对于 placeholder 直接跳过，因为其直接通过用户从 feed_dict 中提供，比如最上面例子中的 x1x_1x1​。 . 对于其他节点，遍历他的输入节点，通过 node_to_val_map[input_node] 获得其值，然后将值传入到 node.op.compute 中进行计算得到节点的值 node_val，同时将其存入 node_to_val_map 中，通过不断计算最终可以获得所有节点的值，将其存在 node_val_results 中。 . 有了 forward pass 的构图后便可以开始进行 reverse AutoDiff 的构图，代码如下 . def gradients(output_node, node_list): &quot;&quot;&quot;Take gradient of output node with respect to each node in node_list. Parameters - output_node: output node that we are taking derivative of. node_list: list of nodes that we are taking derivative wrt. Returns - A list of gradient values, one for each node in node_list respectively. &quot;&quot;&quot; # a map from node to a list of gradient contributions from each output node node_to_output_grads_list = {} # Special note on initializing gradient of output_node as oneslike_op(output_node): # We are really taking a derivative of the scalar reduce_sum(output_node) # instead of the vector output_node. But this is the common case for loss function. node_to_output_grads_list[output_node] = [oneslike_op(output_node)] # a map from node to the gradient of that node node_to_output_grad = {} # Traverse graph in reverse topological order given the output_node that we are taking gradient wrt. reverse_topo_order = reversed(find_topo_sort([output_node])) for node in reverse_topo_order: # Sum partial adjoints from output edges to get output_grad output_grad = zeroslike_op(node_to_output_grads_list[node][0]) for partial_grad in node_to_output_grads_list[node]: output_grad = output_grad + partial_grad # Get grad for input in node.inputs input_grads = node.op.gradient(node, output_grad) for i, input_node in enumerate(node.inputs): if input_node not in node_to_output_grads_list: node_to_output_grads_list[input_node] = [input_grads[i]] else: node_to_output_grads_list[input_node].append(input_grads[i]) node_to_output_grad[node] = output_grad # Collect results for gradients requested. grad_node_list = [node_to_output_grad[node] for node in node_list] return grad_node_list . 整体思路和之前的伪代码一致: . 首先通过 node_to_output_grads_list[output_node] = [oneslike_op(output_node)] 将输出节点的梯度设为 1，因为我们约定计算的梯度是 reduce_sum(output_node) 对于 output_node 是向量的情况，这也是通常机器学习中 loss 的情况； | 接着找到反向的拓扑排序之后，对所有的节点进行遍历，然后先通过 zeroslike_op 初始化一个全 0 node 用于后续求和梯度，最终 output_grad 即为 loss 对当前 node 的梯度； | 然后需要求在 forward pass 下当前 node 的 input_node 的梯度，因为这一步就是一个 primitive 的计算，所以直接调用 node.op.gradient(node, output_grad) 即可获得 input_node 的梯度，每个 op 的 gradient 都由之前我们定义每一个 op 时候手工已经写好了； | 接着将 input_node 加入到 node_to_output_grads_list 中而不是 node_to_output_grad，因为这个节点可能也是其他节点的 input_node，所以可能存在多个梯度，需要完成了所有梯度的计算后再求和才是最终的梯度； | 最后将这个当前 node 的梯度放入到 node_to_output_grad 用于后续的计算，因为当前 node 已经不可能是其他节点的 input_node (拓扑排序)，所以这里计算得到的梯度是完整的； | . 通过上面的方式只需要反向遍历一次所有的 node 便可构建好反向图，同时输入实际的数据之后就可以得到前向的计算结果和反向的梯度结果，只是这里要注意在构建反向图时，所有的计算也是 node 之间的计算，两个 node 的求和 x+y 其实调用的是 __add__ 即内部调用的实际是 add_op, add_byconst_op。 . Ending . 这篇文章介绍了自动微分的基本概念，然后分别从数学和代码实现的角度出发解释了自动微分的原理，也给了一个自动微分的代码示例，不过自动微分是一个非常复杂的系统，这里展示的只是冰山一角，还有很多内容可以探索，比如如何进行高效的高阶导计算等等，感兴趣的同学可以从下面的参考链接中继续学习自动微分相关的知识。 . 最后希望这篇文章能够帮助到你们，如果你有任何建议和问题欢迎在评论去留言。 . Reference link . Automatic Differentiation in Machine Learning: a Survey . | HIPS/autograd: Efficiently computes derivatives of numpy code. (github.com) . | CSC321 Lecture 10: Automatic Differentiation (toronto.edu) . | dlsys-course/assignment1-2018: Assignment 1: automatic differentiation (github.com) . | autodiff (washington.edu) . | CSE599W lecture4 自动微分_skyday123的博客-CSDN博客 . | .",
            "url": "https://l1aoxingyu.github.io/blogpages/deep%20learning/autodiff/dl%20framework/2021/11/10/autodiff.html",
            "relUrl": "/deep%20learning/autodiff/dl%20framework/2021/11/10/autodiff.html",
            "date": " • Nov 10, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "L2 regularization 和 weight decay",
            "content": "Introduction . 通常我们在说 weight decay 的时候，都认为也是在说 L2 regularization，那么到底他们的实现是什么以及他们是否等价呢？这篇文章是一个简单的总结。 . weight decay 和 L2 regularization 的原理 . weight decay 的原理是在每次进行梯度更新的时候，额外再减去一个梯度，如果以普通的梯度下降为例，公式如下 . θt+1=(θt−η∇(θt))−λθt=(1−λ)θt−η∇(θt) theta_{t+1} = ( theta_{t} - eta nabla( theta_t)) - lambda theta_t = (1 - lambda) theta_{t} - eta nabla( theta_t)θt+1​=(θt​−η∇(θt​))−λθt​=(1−λ)θt​−η∇(θt​) . 其中 λ lambdaλ 就是 weight decay 中设定的超参数，通常设定比较小。 . L2 regularization 的原理是在计算 loss 的时候增加一个惩罚项，L2 即为增加一个二范数的惩罚项，即 . freg(θt)=f(θt)+λ2∣∣θ∣∣22f^{reg}( theta_t) = f( theta_t) + frac{ lambda}{2} || theta||_2^2freg(θt​)=f(θt​)+2λ​∣∣θ∣∣22​ . 那么这时对参数求导并进行反向传播就可以有下面的公式 . ∂freg/∂θt=∂f/∂θt+λθt partial f^{reg} / partial theta_t = partial f / partial theta_t + lambda theta_t∂freg/∂θt​=∂f/∂θt​+λθt​ . 那么再进行梯度更新的时候 L2 reg 就相当于额外减去 η∗λ∗θt eta * lambda * theta_tη∗λ∗θt​，其中 η etaη 是学习率，λ lambdaλ 是 L2 reg 中设定的超参数 . 通过上面的公式可以看出 L2 reg 和 weight decay 虽然原理上不一致，不过通过推导在数学形式上最后只差一个常数倍，所以是否可以认为 L2 reg 和 weight decay 是等价的呢？ . L2 reg 和 weight decay 等价吗？ . 通过上面的公式可以推导出 L2 reg 和 weight decay 是等价的，不过有一个大前提即上面的公式表达的是最普通的 SGD 更新方式，除了 vanilla SGD 之外，还有很多 variant optimizer 比如 SGDM，RMSprop，Adam 等等，下面我们以 Adam 为例再次进行 L2 reg 和 weight decay 的公式推导。 . Adam 原理 . 首先回顾一下 Adam 的工作原理，给定超参 β1,β2 beta_1, beta_2β1​,β2​ 以及学习率 η etaη，不考虑 L2 reg 和 weight decay 时，Adam 的更新公式如下 . gt←∇ft(θt−1)mt←β1mt−1+(1−β1)gtvt←β2vt−1+(1−β2)gt2m^t←mt/(1−β1t)v^t←vt/(1−β2t)θt←θt−1−ηm^t/(v^t+ϵ)g_t leftarrow nabla f_t( theta_{t-1}) m_t leftarrow beta_1 m_{t-1} + (1 - beta_1) g_t v_t leftarrow beta_2 v_{t-1} + (1 - beta_2) g_t^2 hat{m}_t leftarrow m_t / (1 - beta_1^t) hat{v}_t leftarrow v_t / (1 - beta_2^t) theta_t leftarrow theta_{t-1} - eta hat{m}_t / ( sqrt{ hat{v}_t} + epsilon)gt​←∇ft​(θt−1​)mt​←β1​mt−1​+(1−β1​)gt​vt​←β2​vt−1​+(1−β2​)gt2​m^t​←mt​/(1−β1t​)v^t​←vt​/(1−β2t​)θt​←θt−1​−ηm^t​/(v^t​ . ​+ϵ) . Adam with weight decay and L2 reg . 接下来可以在上面的公式中增加 L2 reg 和 weight decay，其中红色表示 L2 reg，绿色表示 weight decay . . 其中 m^t hat{m}_tm^t​ 是 bias correction，在 t 比较小的时候可以防止 mtm_tmt​ 因为初始值为 0 导致更新较少的问题，同时当 t→∞t rightarrow inftyt→∞ 时 m^t→mt hat{m}_t rightarrow m_tm^t​→mt​，所以在后续的公式中直接用 mtm_tmt​ 来代替 m^t hat{m}_tm^t​ . 通过对公式的进一步展开和对比，可以发现 L2 reg 和 weight decay 之间除了有个常数倍 η(1−β1) eta(1- beta_1)η(1−β1​) 的区别外，一个更大的区别是 v^t sqrt{ hat{v}_t}v^t​​ 作为分母，这里就引入了不一致性，因为当 grad 中某一个分量的值过大时，v^t sqrt{ hat{v}_t}v^t​​ 就会变大，所以导致 l2 reg 作用在对应分量上的结果变小，这其实和 weight decay 的行为是不一致的，因为 weight decay 对所有的参数都应该是相同的惩罚项，所以正是因为自适应学习率等变种 optimizer 的出现导致 l2 reg 也进行了自适应，所以会导致和 weight decay 的结果最终不同。 . 当然因为 l2 reg 是作用了初始梯度上的，而一阶矩 mtm_tmt​ 和二阶矩 vtv_tvt​ 都需要依赖 gtg_tgt​ 进行更新，所以在不断的更新中也会导致他们在 l2 reg 和 weight decay 下的结果不一致，因为这里是一个积累差异，所以在公式中就没有详细展开了。 . 在这篇文章 ICLR19 的论文 DECOUPLED WEIGHT DECAY REGULARIZATION 中详细的进行了理论的推导和分析 l2 reg 和 weight decay 的差异以及最终的结果，最终为这种真正使用 weight decay 的 Adam 取名为 AdamW 作为区分，感兴趣的同学可以直接去阅读原文。 . 到底使用 Adam 还是 AdamW . 之前大多数深度学习框架包括 TensorFlow 和 PyTorch 等都是按照 l2 reg 去实现的 Adam，不过要改为 AdamW 也比较简单，通过上面的分析是否说明我们应该把所有项目中使用的 Adam 都换成 AdamW 呢？ . 我的观点是之前项目中使用的 Adam 可以换成 AdamW 一试效果，如果精度较低就还是使用回 Adam，如果精度提升就换成 AdamW。那么为什么从上面的理论中得到 AdamW 才是正确的实现，但是实际使用 AdamW 也不一定好呢？原因有很多，因为深度神经网络是一个黑盒，没有人能详细的计算出内部的运算原理，同时之前调参和各种 tricks 都是基于 Adam 去做的，已经针对 Adam+l2 reg 的进行了比较细致的优化，这时直接换成 AdamW 可能重新做一些调参工作可以超过之前的结果，不过就不建议去做重复劳动了。 . 在新项目中可以直接使用 AdamW，同时基于 AdamW 进行调参，这样可以尽可能保证得到更好的结果，比如 Bert 就直接使用的 AdamW。 . Ending . 最后希望这篇文章能够让你了解 l2 reg 和 weight decay 之间的区别和联系，如果你有任何建议和问题欢迎在评论去留言，希望这篇文章能够帮助到你们。 . Reference . 都9102年了，别再用Adam + L2 regularization了 - 知乎 (zhihu.com) . | DECOUPLED WEIGHT DECAY REGULARIZATION . | .",
            "url": "https://l1aoxingyu.github.io/blogpages/deep%20learning/tricks/2021/11/05/l2-reg-weight-decay.html",
            "relUrl": "/deep%20learning/tricks/2021/11/05/l2-reg-weight-decay.html",
            "date": " • Nov 5, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "《黑客与画家》读后感",
            "content": "最近读完了一本书，叫《黑客与画家》，其作者 Paul Graham 被誉为硅谷创业之父，是 Lisp 的狂热爱好者，同时使用 Lisp 创立了 Viaweb，帮助个人用户在网上开店，这也是第一个互联网应用程序，在 1998 年被 Yahoo！公司收购。 . Paul 的经历非常传奇，在 Viaweb 被收购之后，也可以在个人网站上撰写血多关于软件和创业的文章，随后身体力行，创立了风险投资公司 Y Combinator，目前已经资助了超过 80 多家创业公司。 . 在《黑客和画家》中，Paul 首先为大众眼中 的“黑客”正名，黑客是 Hacker 的音译，因为名字里面有“黑”，且在报道中总是出现负面新闻，所以大众认为黑客就是对计算机系统和网络进行可以破坏的人。其实真正的黑客主要是指技术高超的程序员，而 Cracker 才是指恶意破坏的人。 . 在书中 Paul 也表达了一些比较前瞻的观点，比如用户像订报纸那样按照使用时间长短订购软件的使用权，而现在订阅模式正越来越流行。也表达了一些比较偏激的观点，比如某一种编程语言就是要优于另外一种编程语言；在高科技行业中，只有失败者采用“业界最佳实践”等等。 . 虽然有一些观念存在争议，不过仍然有一些观点我认为值得推崇，同时可以引人深思，下面列举其中的一些例子。 . 不要盲目从众 . 在第一章中 Paul 指出，在美国一般都是体育出众的小孩儿更受欢迎，而学习成绩比较好的小孩儿往往被称为 “Nerd”，也就是书呆子，作者在小时候也被称为书呆子，收到排挤。 . 长大后，Paul 思考了一下背后的原因，一方面归结于小孩儿在产生良知之前，会认为折磨就是一种娱乐；另一方面的原因就是大家找一个共同的敌人有利于“受欢迎”，就好比一个政客，他想让选民忘记糟糕的国内局势，方法就是为国家找出一个敌人，哪怕敌人并不真的存在，他也可以创造一个出来，所以书呆子就被挑选出来成为欺负的对象。 而书呆子的注意力都放在读书或者观察世界上，他们琢磨如何更聪明而不是如何更受欢迎，这也是书呆子难以融入校园环境的原因。 . 而到了成年人的世界，一切变得不太一样了，人们都变得更加成熟，你所做的每一件事儿都能产生真正意义上的效果，这是发现正确的答案就变得重要了，这正是书呆子的优势所在，也是像比尔·盖茨、扎克伯克这样的人能成功的真正原因。 所以当你发现你和周围的人格格不入，无法融入其中时，比如在学校寝室室友都在翘课打游戏，问你要不要加入他们。这时先不要着急对自我产生怀疑，先思考和确认自己目前追寻的是否是正确的事情，如果是的话，就勇敢去做，不需要迎合他们，完成自我价值的实现才能找到人生真正的意义。而在面对一些不公平的待遇时，也要学会自我调节，不要钻牛角尖，人生有多种可能，无需在一棵树上吊死。 . 编程与思考的关系 . 另外一个比较有意思的点就是编程和思考的关系，我们一般都认为写代码前要先想清楚结构，这样才有助于写出高效的代码。 . 而 Paul 认为 把整个程序想清楚的时间点，应该是在编写代码的同时，而不是在编写代码之前。他认为编程和画画一样，都是一种艺术创作，而画画的过程就是通过不断地涂改最终完成作品，所以编程也不应该在一开始就定好整体的结构，而是应该在写的过程中不断修正。 . 除了快点动手开始写之外，及时反馈也很重要，先做出原型，再逐步加工做出成品，这种方式有利于鼓舞士气，因为它使得你随时都可以看到工作的成效。开发软件的时候，我有一条规则：任何时候，代码都必须能够运行。如果你正在写的代码一个小时之后就可以看到运行结果，这好比让你看到不远处就是唾手可得的奖励，你因此会受到激励和鼓舞。 . 所以先完成一个最小的可行版本，再根据用户的反馈去不断优化，最终才能呈现出质量良好的产品，这也是现在比较流行的 Scrum 理念一致。 . 对工作的思考 . 工作会占据人生三分之一的时间，我们没有理由不好好考虑一下工作对自身的意义。随着现在 996 越来越常态化，work life balance 也离我们越来越遥远，大家似乎每天都很忙碌，但是却越来越缺少时间思考工作对我们来说到底意味着什么。 . Paul 在书中回答了 “工作到底是什么？”这个问题，他指出工作 真正重要的是做出人们需要的东西，而不是加入某个公司。 不过似乎大部分的人在选择工作的时候更多的会看重给的薪水，而不会去考虑自己所做的事对社会的价值。 . 除此之外，Paul 还提出了两个原则，分别是可测量性和放大性，而且他认为，任何一个通过自身努力而致富的个人，在他们身上应该都能同时发现可测量性和可放大性。 工作职位产生的业绩，应该是可测量的，否则没有办法评判谁应该升职加薪；同时如果你做的事情影响的人很少，那么你的工作非常杰出，产生的效应也非常小。 . 另外团队越小，可测量性就越强，因为每个人所做的贡献能够更准确地估计，所以乔布斯曾经说过，创业的成败取决于最早加入公司的那十个人。而大公司就像一艘巨大的船，一千个划船手共同朝着一个方向划船，但是每个人其实并不能看到自己的努力对船航行的影响，同时因为团队太大，所以每一个人的努力都被平均化了。 . 不过小团队的优势并不在于“小”，而在于“精”，可以自由选择你的队友组成“全明星第一阵容”，从而发挥小团队带来的额外激励。这也解释了为什么能力非常强而且在乎回报的人，通常更愿意出去创业。 . 高科技会带来放大性，大多数人因为创造财富而发财的人都是通过开发新技术而实现的，这在科技类和软件类公司非常显然，甚至在一些看上去和科技无关的公司也是同样的道理，比如沃尔玛并不是通过经营零售业而致富，而是因为设计出了一种新型的商店和销售模式。 . 所以我们在选择方向和团队时，也可以以这两个方向为指导，首先所做的工作具有可测量性，同时所做的事情还具有放大效应，这样最终才有机会做出有意义深远的事情，也有机会影响更多的人。 . 结语 . 以上只是我对 Paul 这本书一些个人肤浅的见解，如果大家感兴趣，推荐大家去读一下这本书，也许能够获得更多的收获。 .",
            "url": "https://l1aoxingyu.github.io/blogpages/book%20review/programming/2021/06/02/hackers-and-painters.html",
            "relUrl": "/book%20review/programming/2021/06/02/hackers-and-painters.html",
            "date": " • Jun 2, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "深度学习中的分布式并行介绍",
            "content": "Introduction . 随着深度学习中的数据规模和网络规模越来越大，训练神经网络会耗费越来越多的时间，势必需要从单 GPU 训练向多 GPU 训练甚至多机训练进行扩展。比如在大规模人脸识别中，训练上千万人脸 ID 需要对最后的全连接层做模型并行，而 GPT-3 为代表的大模型更是有 1750 亿参数，需要在多台机器上做流水并行才能训起来。 . 近年来除了算力增长非常迅速外，深度学习框架近也在飞速发展，分布式并行的实现变得越来越成熟，不同的细节实现对最后的性能也有着很大的影响，下面简单介绍一下其中的一些并行方式作为扫盲，如有问题，欢迎拍砖。 . Data Parallel . 第一种并行方式叫做数据并行，也是现在最流行的一种并行方式。当一块 GPU 可以存储下整个模型时，可以采用数据并行的方式获得更准确的梯度，同时还可以加速训练。主要的方式为每个 GPU 复制一份模型，将一个 batch 的样本平均分为多份，分别输入到不同的卡上做并行计算。 . 因为求导以及加和都是线性的，所以数据并行在数学上是等价的。假设一个 batch 有 n 个样本，一共有 k 个 GPU，第 j 个 GPU 分到 $m_j$ 个样本，考虑等分情况，则 $m_j = frac{n}{k}$ ，如果考虑总损失函数 loss 对参数 w 求导，则有 . ∂Loss∂w=1n∑i=1n∂l(xi,yi)∂w=m1n∂[1m1∑i=1m1l(xi,yi)]∂w+m2n∂[1m2∑i=m1+1m2l(xi,yi)]∂w+⋯=m1n∂l1∂w+m2n∂l2∂w+⋯+mkn∂lk∂w=1k[∂l1∂w+∂l2∂w+⋯+∂lk∂w] frac{ partial{Loss}}{ partial w} = frac{1}{n} sum_{i=1}^n frac{ partial{l(x_i, y_i)}}{ partial w} = frac{m_1}{n} frac{ partial [ frac{1}{m_1} sum_{i=1}^{m_1} l(x_i, y_i)]}{ partial w} + frac{m_2}{n} frac{ partial [ frac{1}{m_2} sum_{i=m_1+1}^{m_2} l(x_i, y_i)]}{ partial w} + cdots = frac{m_1}{n} frac{ partial l_1}{ partial w} + frac{m_2}{n} frac{ partial l_2}{ partial w} + cdots + frac{m_k}{n} frac{ partial l_k}{ partial w} = frac{1}{k} [ frac{ partial l_1}{ partial w} + frac{ partial l_2}{ partial w} + cdots + frac{ partial l_k}{ partial w}]∂w∂Loss​=n1​i=1∑n​∂w∂l(xi​,yi​)​=nm1​​∂w∂[m1​1​∑i=1m1​​l(xi​,yi​)]​+nm2​​∂w∂[m2​1​∑i=m1​+1m2​​l(xi​,yi​)]​+⋯=nm1​​∂w∂l1​​+nm2​​∂w∂l2​​+⋯+nmk​​∂w∂lk​​=k1​[∂w∂l1​​+∂w∂l2​​+⋯+∂w∂lk​​] . 从上面的计算公式中可以看出，所有卡上总 batch 的平均梯度，和单卡上 mini-batch 的平均梯度汇总之后在平均的结果是一样的。 . 在 PyTorch 中，数据并行主要有两种实现方式：DataParallel 和 DistributedDataParallel。 . DataParallel . 在 PyTorch 中，DataParallel 的使用非常方便，只需要下面一行代码，就可以将原本单卡的 module 改成多卡的数据并行 . model = nn.DataParallel(model, device_ids=[0,1,2,3]) . DataParallel 的原理可以参考下面的图片 . Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU &amp; Distributed setups . 在前向的计算过程中，将数据平分到不同的卡上，同时将模型也复制到不同的卡上，然后在每张卡上并行做计算，最后在 device[0] 上获取所有卡上的计算结果。 . 在反向的计算过程中，在 device[0] 上用 outputs 和 label 计算相应的 loss ，然后计算 outputs 的梯度，接着将梯度发回到每张卡上，然后在每张卡上并行做反向传播得到对应的梯度，最后再一次将不同卡的梯度收集到 device[0] 上，然后在 device[0] 上做梯度下降更新参数。 . 通过上面的流程，可以发现 device[0] 会比其他 device 使用更多次，而且因为所有的 loss 以及 loss 的梯度都是在 device[0] 上进行的计算的，所以也会出现负载不均衡的问题。 . 有一种简单的方法可以缓解负载均衡问题，就是将 loss 计算放到网络前向中，这样在前向计算结束之后，device[0] 上获取的就是每张卡上 loss 的计算结果，然后再并行的在每张卡上进行反向传播计算梯度，整体计算流和下面的 Parameter Server 类似 . https://d2l.ai/chapter_computational-performance/parameterserver.html . 过程一(红色部分): 各卡分别计算损失和梯度； . 过程二(蓝色部分): 所有梯度整合到 device[0]； . 过程三(绿色部分): device[0] 进行参数更新，分发参数到其他卡上； . Parameter Servers 的核心概念在 [Smola &amp; Narayanamurthy, 2010] 中引入，实现非常简洁，不过整体上还是有一些缺点 . device[0] 会被更多的使用，从而导致 bottleneck 出现； | 负载均衡问题，不同的卡所占的显存不一致； | 通信开销很大，同步策略非常慢，假设有 k 个 GPU，完成一次通信需要时间 t ，如果使用 PS 算法，总共耗时 $T = 2(k-1) t$ | 在 PyTorch 的实现中，使用 Python 单进程，会有 GIL 锁，并不是真正的并发执行 | . PyTorch 在很早的版本引入了上述实现方式的 DataParallel，不过他们也意识到了这个版本的效率问题，所以后续版本中提出了一个效率更高的数据并行方法 DistributedDataParallel，同时在目前 PyTorch 1.8 版本中官方也更推荐使用 DistributedDataParallel 这种方式。 . DistributedDataParallel . DDP 是 DP 的升级版本，调用方式如下 . model = nn.DistirbutedDataParallel(model, device_ids=[rank]) . 他们大致原理是类似的，不过有很多细节上的区别，使得 DDP 效率更高，主要的区别如下： . 多进程 . 使用多进程支持真正的高并发，官方推荐做法是每张卡一个进程，从而避免单进程多线程的 GIL 问题，当然也支持多张卡在一个进程上，这样就和 DP 一样使用的多线程； . | 通信效率 . DP 的通信成本随 GPU 数量线性增加，而 DDP 使用 Ring AllReduce，保证通讯成本与 GPU 数量无关，能够扩展到大规模分布式训练中； . | 同步参数方式 . DP 通过收集梯度到 device[0]，在 device[0] 进行梯度更新，然后再将参数分发到其他所有设备上；DDP 则通过保证初始状态相同而且改变量也相同（同步梯度）的方式，保证模型同步和更新； . | . 下面我们重点讲一下 Ring Allreduce，这是效率提升的关键。 . Ring Allreduce . Ring Allreduce 原本是 HPC 领域一种比较成熟的通信算法，后被 Baidu SVAIL 引入到深度学习的训练当中，并与 2017年2月公开。 . 下面两张图直观的看到 allreduce 和 ring allreduce 之间的差别，allreduce 有一个中心参数服务器，而 ring allreduce 则像他的名字一样，构成了一个环。 . . . 下面我们具体来讲讲 ring allreduce 是如何进行梯度同步，从而保证总体同步和 GPU 数目无关。 . . 上面的动图展示了第一个成环的过程，每个 GPU 都接受来自另外上一个 GPU 的信息，同时发送给下一个 GPU，且每次发送的数据和 GPU 数量 k 成反比，即每张卡不会将这张卡上所有的数据都发给下一张卡，只会发 $ frac{1}{k}$ 的数据量。 . 在上面的例子中，一共有 5 个 GPU 参与通信，所以每次传递 $ frac{1}{5}$ 的数据量，第一次传递是从对角线开始，以第一份参数为例，在第一次传递中， GPU-0 将 $a_0$ 传递给 GPU-1，完成传递后， GPU-1 的第一份参数就变成了 $a_0 + a_1$ ，这时 GPU-1 在进行下一次传递，将 $a_0 + a_1$ 传递给 GPU-2，这样 GPU-2 的第一份参数就变成了 $a_0 + a_1 + a_2$ ，以此类推，通过 k-1 次传递之后，会获得下图的情况 . . 这时可以发现在每张 GPU 上都有一份参数是完整的，比如 GPU-0 上，第二份参数 $b_2 + b_1 + b_3 + b_4 + b_0$ 已经完整地收集到了所有卡上的数据，接着将上图橙色框的数据分别再做 k-1 次传递，最后就可以在每张卡上获得完整的数据信息。 . . 可以分析一下通信开销，假设有 k 个 GPU，传输总量是 p，b 为每次的通信上限，首先将梯度分为 k 份，每张卡每次传输 $ frac{p}{k}$ 的通信量，传递 k-1 次就可以分别在每张卡上收集到 $ frac{1}{k}$ 完整的数据，之后再传 k-1 次可以使得每张卡上获得完整的数据，所以总的通信开销是 . 2(k−1)pkb=2pbkk−12 (k-1) frac{ frac{p}{k}}{b} = frac{2 p}{b} frac{k}{k-1}2(k−1)bkp​​=b2p​k−1k​ . 所以整个式子在 k 很大的时候，和 k 是无关的，也证明了 ring allreduce 在通信上是和 GPU 数量无关的。 . Model Parallel . 上面讲的数据并行需要一张卡能够装下模型，当模型非常大，一张 GPU 无法放下模型的所有 tensor 时，就需要用到 model parallel，也叫做 tensor parallel。随着 GPT-3 等超级大模型的流行，未来模型越来越大也会是一个趋势，所以不要觉得一个模型需要用多张 GPU 来存放是一件离我们很遥远的事情。 . 说到模型并行，下面有一个简单的例子，这是从 pytorch forum 里面截取的，把模型的第一层 Linear 放到了 device[0] 上，第二层 Linear 放到了 device[1] 上，那么这个能被成为模型并行吗？ . class ToyModel(nn.Module): def __init__(self): super(ToyModel, self).__init__() self.net1 = torch.nn.Linear(10, 10).to(&#39;cuda:0&#39;) self.relu = torch.nn.ReLU() self.net2 = torch.nn.Linear(10, 5).to(&#39;cuda:1&#39;) def forward(self, x): x = self.relu(self.net1(x.to(&#39;cuda:0&#39;))) return self.net2(x.to(&#39;cuda:1&#39;)) . 其实从严格意义上来讲，这个并不能称为模型并行，只是把同一个模型的不同层 split 到不同的 device 上，真正的模型并行还需要是的他们能够同步执行 (concurrently)，但是上面的例子中，两个 Linear 并不能同时计算，第二个 Linear 需要获取第一个 Linear 的输出才能进行计算。 . 那么如何能够写一个简单的模型并行例子呢？在 PyTorch model parallel tutorial 中，给出了一个简单的例子。 . 得益于 PyTorch 使用的 CUDA operations 是异步的，所以可以用下面的方式来轻松构建一个模型并行的操作，而不需要使用到多线程或是多进程。 . class PipelineParallelResNet50(ModelParallelResNet50): def __init__(self, split_size=20, *args, **kwargs): super(PipelineParallelResNet50, self).__init__(*args, **kwargs) self.split_size = split_size def forward(self, x): splits = iter(x.split(self.split_size, dim=0)) s_next = next(splits) s_prev = self.seq1(s_next).to(&#39;cuda:1&#39;) ret = [] for s_next in splits: # A. s_prev runs on cuda:1 s_prev = self.seq2(s_prev) ret.append(self.fc(s_prev.view(s_prev.size(0), -1))) # B. s_next runs on cuda:0, which can run concurrently with A s_prev = self.seq1(s_next).to(&#39;cuda:1&#39;) s_prev = self.seq2(s_prev) ret.append(self.fc(s_prev.view(s_prev.size(0), -1))) return torch.cat(ret) . 在上面的例子中，首先提前在 device[0] 做一次计算，然后将结果 copy 到 device[1] 上，接着在进行后续的计算。后续是一个 for loop，代码顺序是先执行 device[1] 上运算 A，不过因为 CUDA 的异步特性，这个计算 A 并不会马上执行，随后代码上再执行 device[0] 上的计算 B，这时两个操作 A 和 B 会一起进行计算。等待 B 计算完毕后，会再次实现和之前一样的操作，将 tensor 从 device[0] 上 copy 到 device[1] 上，因为在 cuda 上 device-to-device 的操作会在当前 streams 上进行同步，而上面的实现在 device[0] 和 device[1] 上都使用的是默认的 streams，所以不需要额外进行同步。 . 其实在上面的实现中，使用了流水并行的技巧，后面我们会更详细的讲解。 . Partial-FC . 最后我们以人脸识别为模型并行的一个经典例子，介绍其中应用非常广泛的 FC 并行以及他的一种变种 Partial-FC。 . Partial FC: Training 10 Million Identities on a Single Machine . 上面是人脸识别中模型并行的经典图示，在 Backbone 部分做数据并行，在 FC 部分做模型并行，比如一共有 C 个 ID，k 张 GPU，那么每个 GPU 上会放 $ frac{C}{k}$ 类别中心。 . 整体的计算过程如下： . 将数据分到不同的卡上，在 backbone 上并行做前向计算得到 features X； | 每张卡上都同步其他所有卡的 features，然后在每张卡上并行计算对应类别中心的 logits； | 每张卡上同步其他卡上的 logits 结果，并行计算 loss 以及 logits 对应的梯度； | 在每张卡上并行计算各自类别中心权重 $w_i$ 对应的梯度 $ nabla w_i$ 和 feature X 对应的梯度 $ nabla X$； | 同步其他所有卡的 $ nabla X$ ，获得平均梯度，然后将其 scatter 到各自对应的卡上，并行做自动求导，获得 backbone 的梯度； | 以上过程就是人脸识别中标准的 FC 并行框架，不过这种方式的并行会出现一些显存问题，我们可以看看下面的公式 . Memw=d×C↑k↑×4 bytesMemlogits=Nk×Ck×4 bytesMem_w = d times frac{C uparrow}{k uparrow} times 4 bytes Mem_{logits} = N k times frac{C}{k} times 4 bytesMemw​=d×k↑C↑​×4 bytesMemlogits​=Nk×kC​×4 bytes . 上面分别表示每个 GPU 上权重 w 的显存和计算 logits 的显存，其中 d 表示 feature 维度，C 是类别数目，k 是 GPU 数量，N 是 mini-batch size，4 bytes 表示用 float32 进行计算。 . 通过上面的计算公式可以看出，如果增加一倍 ID 规模，那么可以通过增加一倍的 GPU 数量 k 来保证每张卡上的显存占用量一致，不过通过观察 logits 的显存占用量就会发现，如果不断地增加 GPU 数量 k，会导致 logits 的显存占用量线性增长，所以随着 ID 规模的增加，不断增加 GPU 数目最终会导致显存爆炸。 . Partial-FC 提供了一个非常简单的思路来解决这个问题，既然 logits 显存会随着 GPU 数量一直增加，那么减少 logits 的显存就可以了。接着通过实验发现采样部分负样本和全部负样本最后的收敛效果几乎一致，所以在 Partial-FC 中，每次计算 logits 并不会使用 w 中的全部负样本，只会采样固定比例的负样本，最终可以使得计算的 logits 显存以固定比例降低。 . Pipeline Parallelism . 最后讲一下流水并行，当模型非常巨大，需要用多张 GPU 进行存储的时候，就需要用到流水并行。流水线并行算是广义模型并行的一种特例，通过多个设备来共同分担显存消耗，同时只在相邻的设备之间进行通讯，因此通信张量较小。 . GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism . 上图中 (a) 展示了流水并行的前向反向计算流，(b) 表示一种 naive pipeline parallelism，同步更新和串行计算，后一个设备依赖上一个设备的结果，所以每次都只有一个设备在计算，其他设备在等待，没有发挥分布式的优势。 . (c) 是 GPipe 这篇论文提出了一个解决方案，将一个 mini-batch 切分成多个更小的 micro-batch，实现不同的 GPU 并行同步计算，每个 micro-batch 反向计算获得的梯度进行累加，在最后一个 micro-batch 累加结束之后，再统一更新模型。有兴趣的同学可以直接去看 GPipe 这篇论文。 . 混合并行 . https://www.deepspeed.ai/tutorials/pipeline/ . 上面讲了多种并行方式一般会混合使用，当多种并行同时使用的时候，也叫做混合并行。上面是从微软发布的 DeepSpeed 的 tutorial 贴出一个例子，主要使用了数据并行 + 流水并行，GPU-0 和 GPU-2 作为 Group-1，GPU-1 和 GPU-3 作为 Group-2，Group-1 和 Group-2 进行数据并行，而在每个 Group 内部进行流水并行，将 Group 内的 batch 数据切分成 4 个 micro-batch。另外 Deep Speed 还提供了一些其他的 features，比如 ZeRO 可以降低内存开销，训练更大的模型，有兴趣的同学可以自行查看。 . Further Reading . 分布式并行是深度学习中的一个重要的问题，随着数据，算力和模型的规模都越来越大，如何高效、稳定地训练模型也变得越来越重要，上面介绍的并行只是一个入门的内容，有兴趣的同学可以看看这篇 oneflow 的文章 OneFlow —— 让每一位算法工程师都有能力训练 GPT，用 GPT-3 作为例子介绍了分布式训练模型的一些最新的技术。 . Reference . PyTorch 源码解读之 DP &amp; DDP：模型并行和分布式训练解析 | https://d2l.ai/chapter_computational-performance/parameterserver.html | Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU &amp; Distributed setups | Visual intuition on ring-Allreduce for distributed Deep Learning | Bringing HPC Techniques to Deep Learning | 单机多卡的正确打开方式（一）：理论基础 | Partial FC: Training 10 Million Identities on a Single Machine | PyTorch tutorial | GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism | Deep Speed | OneFlow —— 让每一位算法工程师都有能力训练 GPT | .",
            "url": "https://l1aoxingyu.github.io/blogpages/summary/self-supervised%20learning/2021/05/16/dl-dist-train.html",
            "relUrl": "/summary/self-supervised%20learning/2021/05/16/dl-dist-train.html",
            "date": " • May 16, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Study Less, Study Smart",
            "content": "学习变得越来越重要，甚至在很多领域需要终生学习，能否高效学习决定了人与人之间的差距。在这篇文章中，我将分享给大家一些高效学习的技巧和建议，帮助大家快速而高效地学习新知识。 . . 为什么需要终生学习 . 一提到学习，可能大家的脑子里面会回想起高中或者大学时的情景，每天日以继夜，为了考试努力学习。好不容易终于熬完了大学，以为之后的人生中再也不会有”学习“了，如果你这样想，那么就大错特错了。 . 如果你是一个对自我要求很高的人，期望不断学习来提升自己，同时也希望能够成为同龄人中的佼佼者，那么终生学习对你而言是毫无疑问的事情，你需用一直努力才能保持竞争力。 | 如果你说我只希望躺平，做一个打工人，满足于普通的人生，那么是不是就不需要学习了呢？现在社会发展的速度越来越快，不需要动脑子的工作慢慢会被人工智能替代，而需要动脑子的工作会出现很多新的概念和内容，如果不持续的学习，根本无法完成正常的工作。所以仅仅是持有不被社会淘汰的标准，也需要不断地学习。 | 所以不管是对于自我要求很高的人，还是希望仅仅跟上社会发展脚步的人来说，终身学习都变得越来越必要。工欲善其事必先利其器，学习”如何学习“可以帮助我们在遇到新的概念和问题的时候更加游刃有余，使用更少的时间掌握更多的内容。 . 如何高效地学习 . 无意间看到了Marty Lobdell 在 Youtube 的视频 Study Less Study Smart 后，非常遗憾自己没能在更早的时候就看到，在这里强烈推荐给大家。 . 在视频中，Marty 列出了影响学习效率的几个因素，同时配合相关的案例讲解，下面列举一些我认为比较重要的观点，更详细的内容建议大家直接食用原始视频，效果更佳。 . 番茄工作法 . 番茄工作法是一种时间管理技巧，相信很多人都有一定的了解，简单来说就是使用一个定时器来分割出一个一般为25分钟的工作时间和5分钟的休息时间，这个时间段被称为一个番茄钟。 . 番茄工作法的发明者使用这种方法来对抗拖延症，因为每个番茄钟只有一小段时间，比较容易开始。所以可以逼迫自己先开始一个番茄钟，而人往往在开始进行执行任务之后，更容易坚持下去，这样就能够开始做任务从而避免拖延症。 . 现在大家对番茄工作法的评价褒贬不一，有的人认为将一段工作划分成小的时间段会破坏工作的完整性，比如对于创作或者编程这样的任务，需要长时间的专注，如果切割成了很多小块的时间，那么很可能刚刚进入状态，一个番茄工作时就结束了，被迫退出这种状态。 . Marty 在视频中表示，人在长时间学习时，精力高度集中，如果长时间保持这种状态，效率会随着时间越来越低。这个时候如果强迫自己继续坚持学习或工作，反而效果会越来越差，但是如果休息一小段时间，比如5分钟，这是精力会重新充电，这时再继续学习可以获得更好的效果。 . 我个人认为在学习或者工作一段时间之后休息是很有必要的，一小段时间的休息之后再重新开始工作可以获得更好的效果。但是考虑到有一些工作可能需要较长的启动时间，所以可以根据个人需求和任务类型进行番茄钟时长的自定义，比如进行编程或者写作等任务时，可以将工作时长设定为50分钟而非25分钟，或是在番茄钟响后并不进入休息，继续工作直到感觉累了，再进行休息。 . 建立自己的学习区 . 大多数人都没有自己专属的学习区，要不在客厅学习，要不在床上看书，这样其实不利于构建一个良好的学习环境。每个房间和区域都有自己的意义，比如客厅是大家娱乐和社交的场所，卧室和床是休息的地方，如果在这些地方学习，潜意识里并不会认为你是真正在学习，所以很难进入学习状态。 . 如果建立一个专属的学习区，每次来到这个区域就开始学习，久而久之会在大脑深处构建一种暗示效果，当下次开始学习的时候，直接来到学习区，这样可以更快地进入状态。书房是一个非常好的学习区，不过并不是每个人家里都有书房这个条件，所以很多人去学校，图书馆或者咖啡厅进行学习和阅读。 . 如果你不想去外面，就想在家里学习，但是家里又没有书房，Marty 还提供了一个低成本的方式来构建自己的学习区，那就是使用一盏台灯。每次要要学习的时候，就打开特定的台灯，当学习结束之后再关闭台灯，通过这样简单的步骤构建一个学习空间，虽然并没有在物理意义上开辟新的空间，但是可以给大脑一种心理暗示，可以让大脑将这种灯光和学习工作关联起来，这样有利于快速进入学习状态。 . 专注力 . 很多人喜欢在学习的时候听音乐或者是看电影，认为这种事情不会耗费注意力。其实人是一个单线程生物，我们的注意力用于只能聚焦在一件事情上，当你在学习是听音乐，有一部分的注意力会在你不经意间分散到音乐和歌词上，从而降低学习效率。 . 所以记得每次专注于一件事儿上，如果实在想听音乐，可以听一些白噪音，这样有助于提升学习效率，但是一定不要听歌或者是看电影。 . 发现事实背后的概念 . 很多时候教材为了方便概念的讲解，会使用隐喻和类比的方式进行举例，或者是讲一些概念的实际用途。这个时候不要去记住这些事实，而是要理解和记住事实背后的概念，概念就是指它的原理，功能以及它和其他的概念如何进行联系。一旦记住这个概念，那么你将一辈子都不会忘记，而相关的事实可以通过 Google 和网络进行搜索。 . 首先需要具备区分事实和概念的能力，同时寻找事实背后的概念往往需要进行总结和归纳，找到事实背后的本质，这些能力都需要不断训练才能获得。 . 费曼技巧 . 理查德·费曼是理论物理学家，不过它广为人知的便是“费曼学习法”。 . 简要来说，“费曼学习法”分为4步： . 选定一个学习的内容，这个内容可以是任何你当下想学习的知识，然后通过各种资料进行学习； | 想另外一个人教授你所学习的内容，或者是在一张白纸上想象你正在向别人教授这个内容，简单来说就是知识的输出； | 在教授过程中，发现自己“卡壳”以及解释不清的内容，这就是自己的薄弱点，返工学习和纠错，然后再次进行教授和输出，直到可以清晰地表达相关的知识； | 回顾和精简，努力简化表达，将所学的知识进行内化； | 学习金字塔是美国国家训练实验室研究的成果，从下面的金字塔中，也能看出“费曼学习法”位于主动学习中的最高层，可以最大程度地留存学习的知识。 . . 在 Marty 的视频中，通过几点不同的描述来联合构成了费曼技巧：1. 使用自己的话对概念进行解释；2. 关上书回忆学习的内容而非仅仅打开书辨认知识点；3. 教授给另外一个人所学的内容，如果没有其他人可以试着交给一张椅子或者在一张白纸上写下来。 . 找到一个学习小组 . 除了自己独立学习之外，也可以尝试组建一个学习小组共同学习，不仅可以互相帮助解答疑问，同时也可以互相激励。同时也可以在学习小组的成员中进行费曼技巧，以及互相分享学习方法。 . 充足的睡眠 . 充足的睡眠在 Marty 看来是一个“命令”而非“建议”。睡眠的影响也是一直以来我所忽略的内容，我们往往到了该睡觉的时候想着再多看几页书，或者是多看几分钟视频，从而导致晚睡，这样往往是舍本逐末。 . 在视频中，Marty 强调好的睡眠不仅仅对身体健康有益处，同时也有利于帮助大脑巩固长期记忆的知识点成为永久记忆的知识点。看到这里我突然想起了之前高中为了刷题总是熬夜到晚上2点，如果能够更早看到这个视频，可能我会早早地“放过”自己，让自己11点半就进入睡眠，或许能够获得一个更好的成绩。 . SQ3R 方法 . SQ3R 是 Survey, Question, Read, Recite and Review 5 个单词的缩写，这是一种学习教科书的方法。首先明确教科书并不是小说，跳着读并不会影响学习效果，可以直接跳到需要学习的页面。 . SQ3R 可以简要描述为下面的步骤： . 简要浏览一下需要学习的主题，看看相关的插图等等 (Review)； | 寻找这一章的主要希望解决的主要问题是什么，记住并写下他们，时刻在心里提醒自己 (Question)； | 阅读教材中的粗体字，比如题目，副标题，被标粗的部分等等，因为这些内容是作者想强调的。接着如果有很长的段落，读这些段落的第一个句子和最后一个句子。然后再读教材中的所有内容，同时尝试回答之前提出的主要问题，如果不能回答，那么重新读这些内容，不过这一次用记号笔标记一些你认为重要的内容，同时在每一页的边缘做笔记，直到完成问题的回答 (Read)； | 列出下一个希望回答的问题，然后重复上面的3步； | 使用费曼技巧将你希望解释的内容和问题重新用自己的话来描述，确保使用一个6岁孩子也能听懂的语言；除此之外，也可以假装是这个领域的专家，写出相关的文章，如果你发现你不能很好的解释和概括其中的内容，回到材料中重新学习 (Recite)； | 如果你完成了上面的步骤，那么你会获得3个材料：a）一本有标注和笔记的教材；b）这个话题主要的问题以及答案；c）一篇相关的文章或者是思维导图。接着需要记得定期复习相关的内容，形成长期记忆 (Review)； | 结语 . 上面的内容就是我在学习 Marty 视频后总结的几个我认为比较重要的学习技巧，同时我正在看 Coursera 相关的课程 Learning How to Learn: Powerful mental tools to help you master tough subjects，如果后续学习完这个课程，会在写一篇文章进行介绍。 . 最后希望大家能够从这篇文章中得到一些收获，也强烈推荐大家去看 Marty 的原版视频，最后欢迎大家在评论中提供自己比较高效的学习技巧。 .",
            "url": "https://l1aoxingyu.github.io/blogpages/utility/2021/05/09/Study-Less-Study-Smart.html",
            "relUrl": "/utility/2021/05/09/Study-Less-Study-Smart.html",
            "date": " • May 9, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "FastReID V1.0: Beyond reID",
            "content": "引言 . FastReID 自20年6月发布以来，我们收到了很多用户的反馈，当初的 v0.1 版本存在大量需要优化的部分，经过了最近半年的持续优化，终于在 21年1月14日，我们低调地发布了 FastReID V1.0，这次更新包括非常多的方面，但是最大的贡献在于我们将 FastReID 扩展到了更多的任务上，同时在这些任务上都达到了 SOTA 结果。 . tldr: 我们更新了 FastReID V1.0 版本，不仅实现了更快的分布式训练和测试，模型一键导出 caffe/onnx/tensorRT，还实现了蒸馏，自动超参搜索以及更多任务的扩展，比如人脸识别，细粒度检索等等，最后基于 FastReID 我们拿到了深圳 NAIC20 ReID track 的第一名。 . 下面简单介绍一下 FastReID V1.0 的各项改进。 . . Embedding 知识蒸馏 . 深度神经网络一般有较多的冗余，同时模型太大会导致 inference 时间变长，所以在部署的时候需要考虑对模型进行压缩，减小其参数量，其中有较多的压缩方式，比如剪枝，量化和蒸馏等。 其中蒸馏是一种比较流行的范式，可以保证模型不需要进行结构修改的情况下，得到较大的性能提升，所以我们选择在 FastReID 中加入蒸馏的支持，可以让我们用小模型部署的时候获得更大的精度提升。 . 虽然蒸馏发展了这么多年，但是通过尝试我们发现 Hinton 的 Distilling the Knowledge in a Neural Network 还是最 solid 的选择。 同时我们将原本的蒸馏 loss 优化为具有对称性的 JS Div loss，最后修改蒸馏的 soft label 生成方式。 . 不同于 softmax 分类 loss，在 embedding 任务中通常会使用效果更好的 margin-based softmax，比如 arcface 等等， 这时直接使用基于 margin 的 logits 生成 soft label 效果很不好，所以我们将 soft label 修改为去掉 margin 的 logits 输出。 . 除了可以对 label 进行蒸馏之外，也可以对 feature 进行蒸馏，通过实验了一大堆不 work 的特征蒸馏方法之后，我们发现 overhaul-distillation 可以在 loss 蒸馏的基础上进一步对网络进行提升，所以也将这个方法加入到了 FastReID 当中，但是由于 overhaul 需要对 backbone 进行一些修改，获得 relu 之前的 feature，所以我们选择构建了一个新的 project 而不是直接去 FastReID 里面修改 backbone。 . 最后我们在 dukeMTMC 上进行实验，使用 r101_ibn 作为 teacher model, r34 作为 student model，可以获得如下的效果提升。 . DukeMTMC-reid . Model Rank@1 mAP . R101_ibn (teacher) | 90.66 | 81.14 | . R34 (student) | 86.31 | 73.28 | . JS Div | 88.60 | 77.80 | . JS Div + Overhaul | 88.60 | 78.25 | . 蒸馏的使用也非常简单，只需要首先按照正常的方式训练一个 teacher model，如果只想使用 loss 蒸馏，可以使用 Distiller 作为 meta_arch，如果希望加上 overhaul，只需要使用 DistillerOverhaul 作为 meta_arch 就可以了。 最后再指定 teacher model 的配置文件和训好的 weights 就可以了。 . 下面用 R101_ibn 作为 teacher model，R34 作为 student model 举一个例子 . # teacher model training python3 projects/FastDistill/train_net.py --config-file projects/FastDistill/configs/sbs_r101ibn.yml --num-gpus 4 # loss distillation python3 projects/FastDistill/train_net.py --config-file projects/FastDistill/configs/kd-sbs_r101ibn-sbs_r34.yaml --num-gpus 4 MODEL.META_ARCHITECTURE Distiller KD.MODEL_CONFIG projects/FastDistill/logs/dukemtmc/r101_ibn/config.yaml KD.MODEL_WEIGHTS projects/FastDistill/logs/dukemtmc/r101_ibn/model_best.pth # loss+overhaul distillation python3 projects/FastDistill/train_net.py --config-file projects/FastDistill/configs/kd-sbs_r101ibn-sbs_r34.yaml --num-gpus 4 MODEL.META_ARCHITECTURE DistillerOverhaul KD.MODEL_CONFIG projects/FastDistill/logs/dukemtmc/r101_ibn/config.yaml KD.MODEL_WEIGHTS projects/FastDistill/logs/dukemtmc/r101_ibn/model_best.pth . 自动超参搜索 . 炼丹一直困扰着各位调参侠，特别是每次到了一个新的场景，就需要重新调参来适应新的数据分布，非常浪费时间。 所以我们决定在 FastReID 中加入了自动超参搜索的功能来解放各位调参侠的双手，让大家可以更好的划水。 . 通过一系列调研，最后决定使用 ray[tune] 这个超参搜索的库，在集成到 FastReID 中间也遇到了非常多的坑，最后我们成功地在 FastReID 中实现了超参搜索的功能。 . 使用方式非常简单，如果你想用 Bayesian 超参搜索跑 12 组试验，可以使用下面的代码就可以开始自动分布式训练，如果有4张卡，那么可以4个试验同步一起跑 . python3 projects/FastTune/tune_net.py --config-file projects/FastTune/configs/search_trial.yml --num-trials 12 --srch-alog &quot;bohb&quot; . 另外需要搜索的超参空间需要在 projects/FastTune/tune_net.py 中进行配置，更具体的使用方式可以参考 tutorial。 . 唯一不足的是还不能用pytorch的分布式数据并行，后续有时间会进一步优化，希望这能够成为大家打比赛刷分，做业务的利器。 . 最多最全的任务支持 . 我们刚刚发布 FastReID v0.1 时，他只是作为一个重识别的 toolbox，支持重识别的业务模型和 research。 . 后面考虑到各种识别任务的模型结构都长得差不多，所以我们希望 FastReID 只需要稍微 customize 就能够支持各种不同的任务。 . 但是每种任务都有自己的一些特殊性，把这些特殊性全部往 FastReID 里面塞肯定是不现实的，为了不引入冗余性，我们通过对每种 task 单独构建 project 的方式对 FastReID 进行扩展，同时也相当于提供了一些扩展任务的参考写法和 example，毕竟我们的文档一直没有时间写(逃~)。 . 最后呈现在 FastReID 的 projects 中一共可以支持 image classification (FastClas), attribute recognition (FastAttr), face recognition (FastFace) 和 fine-grained image retrieval (FastRetri) 4 种比较常见的识别任务，同时我们也分别跑了几个 benchmark 以保证代码的实现是正确的。 . . . 同时大家在 customize 自己的 project 时，也可以将这些 projects 中的东西进行排列组合来实现新的功能，比如将 FastDistill 和 FastFace 组合在一起，就可以实现人脸识别中的模型蒸馏。 . NAIC20 reID 比赛 . 借助 FastReID 高效的分布式训练模式和超参搜索等功能，我们拿到了 naic20 比赛的第一名，比赛方案也开源在 FastReID 的 projects/NAIC20 中。 一些比赛中的 tricks 已经直接集成到了 FastReID 中，有空再专门写一下比赛的方案吧，总结起来就是大模型+大 input size + ensemble。 . 总结 . 一套好的 codebase 对于大家做实验和做业务都起着事半功倍的效果，大家也越来越发现代码的工程质量不仅影响业务模型的研发效率和性能，同时还对研究工作有着影响。 . FastReID 不仅仅希望能给 ReID 社区提供稳定高效的代码实现，同时也希望大家能够基于 FastReID 去做算法研究，同时扩展到更多其他任务上。 . 也希望大家能够踊跃地在 GitHub 上提 issue 和 PR，让我们一起把 FastReID 越做越好。 . 在此感谢 JD AI 组的同事和老师的支持，正是因为大家的努力让 FastReID 变得更好，并且科研项目也都在 FastReID 上取得了更好的性能。 . Reference . FastReID: A Pytorch Toolbox for General Instance Re-identification, He, Lingxiao and Liao, Xingyu and Liu, Wu and Liu, Xinchen and Cheng, Peng and Mei, Tao, arXiv preprint arXiv:2006.02631, 2020 . | Deep spatial feature reconstruction for partial person re-identification: Alignment-free approach, He, Lingxiao and Liang, Jian and Li, Haiqing and Sun, Zhenan, CVPR2018 . | Foreground-aware Pyramid Reconstruction for Alignment-free Occluded Person Re-identification, He, Lingxiao and Wang, Yinggang and Liu, Wu and Zhao, He and Sun, Zhenan and Feng, Jiashi, ICCV2019 . | Black Re-ID: A Head-shoulder Descriptor for the Challenging Problem of Person Re-Identification, Boqiang, Xu and Lingxiao, He and Xingyu, Liao and Wu,Liu and Zhenan, Sun and Tao, Mei, ACM MM2020 . | A Comprehensive Overhaul of Feature Distillation, Heo, Byeongho and Kim, Jeesoo and Yun, Sangdoo and Park, Hyojin and Kwak, Nojun and Choi, Jin Young . | Distilling the Knowledge in a Neural Network, Geoffrey Hinton, Oriol Vinyals, Jeff Dean . | Tune: A Research Platform for Distributed Model Selection and Training, Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion . | ArcFace: Additive Angular Margin Loss for Deep Face Recognition, Jiankang Deng, Jia Guo, Niannan Xue, Stefanos Zafeiriou . | PaddleClas: https://github.com/PaddlePaddle/PaddleClas . | .",
            "url": "https://l1aoxingyu.github.io/blogpages/reid/fastreid/2021/04/28/fastreid-v1.html",
            "relUrl": "/reid/fastreid/2021/04/28/fastreid-v1.html",
            "date": " • Apr 28, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Introducing fastlinkcheck",
            "content": ". Motivation . Recently, fastai has been hard at work improving and overhauling nbdev, a literate programming environment for python. A key feature of nbdev is automated generation of documentation from Jupyter notebooks. This documentation system adds many niceties, such as the following types of hyperlinks automatically: . Links to source code on GitHub. | Links to both internal and external documentation by introspecting variable names in backticks. | . Because documentation is so easy to create and maintain in nbdev, we find ourselves and others creating much more of it! In addition to automatic hyperlinks, we often include our own links to relevant websites, blogs and videos when documenting code. For example, one of the largest nbdev generated sites, docs.fast.ai, has more than 300 external and internal links at the time of this writing. . The Solution . Due to the continued popularity of fastai and the growth of new nbdev projects, grooming these links manually became quite tedious. We investigated solutions that could verify links for us automatically, but were not satisfied with any existing solutions. These are the features we desired: . A platform independent solution that is not tied to a specific static site generator like Jekyll or Hugo. | Intelligent introspection of external links that are actually internal links. For example, if we are building the site docs.fast.ai, a link to https://docs.fast.ai/tutorial should not result in a web request, but rather introspection of the local file system for the presence of tutorial.html in the right location. | Verification of any links to assets like CSS, data, javascript or other files. | Logs that are well organized that allow us to see each broken link or reference to a non-existent path, and the pages these are found in. | Parallelism to verify links as fast as possible. | Lightweight, easy to install with minimal dependencies. | . We tried tools such as linkchecker and pylinkvalidator, but these required your site to be first be hosted. Since we wanted to check links on a static site, hosting is overhead we wanted to avoid. . This is what led us to create fastlinkcheck, which we discuss below. . Note: For Ruby users, htmlproofer apperas to provide overlapping functionality. We have not tried this library. . A tour of fastlinkcheck . For this tour we will be referring to the files in the fastlinkcheck repo. You should clone this repo in the current directory in order to follow along: . git clone https://github.com/fastai/fastlinkcheck.git cd fastlinkcheck . Cloning into &#39;fastlinkcheck&#39;... remote: Enumerating objects: 135, done. remote: Counting objects: 100% (135/135), done. remote: Compressing objects: 100% (98/98), done. remote: Total 608 (delta 69), reused 76 (delta 34), pack-reused 473 Receiving objects: 100% (608/608), 1.12 MiB | 10.47 MiB/s, done. Resolving deltas: 100% (302/302), done. . Installation . You can install fastlinkcheck with pip: . pip install fastlinkcheck . Usage . After installing fastlinkcheck, the cli command link_check is available from the command line. We can see various options with the --help flag. . link_check --help . usage: link_check [-h] [--host HOST] [--config_file CONFIG_FILE] [--pdb] [--xtra XTRA] path Check for broken links recursively in `path`. positional arguments: path Root directory searched recursively for HTML files optional arguments: -h, --help show this help message and exit --host HOST Host and path (without protocol) of web server --config_file CONFIG_FILE Location of file with urls to ignore --pdb Run in pdb debugger (default: False) --xtra XTRA Parse for additional args (default: &#39;&#39;) . From the root of fastlinkcheck repo, We can search the directory _example/broken_links recursively for broken links like this: . link_check _example/broken_links . ERROR: The Following Broken Links or Paths were found: - &#39;http://fastlinkcheck.com/test.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - Path(&#39;/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.js&#39;) was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . Specifying the --host parameter allows you detect links that are internal by identifying links with that host name. External links are verified by making a request to the appropriate website. On the other hand, internal links are verified by inspecting the presence and content of local files. . We must be careful when using the --host argument to only pass the host (and path, if necessary) without the protocol. For example, this is how we specify the hostname if your site&#39;s url is http://fastlinkcheck.com/test.html: . link_check _example/broken_links --host fastlinkcheck.com . ERROR: The Following Broken Links or Paths were found: - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - Path(&#39;/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.js&#39;) was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . We now have one less broken link as there is indeed a file named test.html in the root of the path we are searching. However, if we add a path to the end of --host , such as fastlinkcheck.com/mysite the link would again be listed as broken because _example/broken_links/mysite/test.html does not exist: . link_check _example/broken_links --host fastlinkcheck.com/mysite . ERROR: The Following Broken Links or Paths were found: - &#39;http://fastlinkcheck.com/test.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - Path(&#39;/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.js&#39;) was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . You can ignore links by creating a text file that contains a list of urls and paths to ignore. For example, the file _example/broken_links/linkcheck.rc contains: . cat _example/broken_links/linkcheck.rc . test.js https://www.google.com . We can use this file to ignore urls and paths with the --config_file argument. This will filter out references to the broken link /test.js from our earlier results: . link_check _example/broken_links --host fastlinkcheck.com --config_file _example/broken_links/linkcheck.rc . ERROR: The Following Broken Links or Paths were found: - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . Finally, if there are no broken links, link_check will not return anything. The directory _example/no_broken_links/ does not contain any HTML files with broken links: . link_check _example/no_broken_links . No broken links found! . Python . You can also use these utilities from python instead of the terminal. Please see these docs for more information. . Using link_check in GitHub Actions . The link_check CLI utility that is installed with fastlinkcheck can be very useful in continuous integration systems like GitHub Actions. Here is an example GitHub Actions workflow that uses link_check: . name: Check Links on: [workflow_dispatch, push] jobs: check-links: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 - name: check for broken links run: | pip install fastlinkcheck link_check _example . We can a few more lines of code to open an issue instead when a broken link is found, using the gh cli: . ... - name: check for broken links run: | pip install fastlinkcheck link_check _example 2&gt; err || true export GITHUB_TOKEN=&quot;YOUR_TOKEN&quot; [[ -s err ]] &amp;&amp; gh issue create -t &quot;Broken links found&quot; -b &quot;$(&lt; err)&quot; -R &quot;yourusername/yourrepo&quot; . We can extend this even further to only open an issue when another issue with a specific label isn&#39;t already open: . ... - name: check for broken links run: | pip install fastlinkcheck link_check &quot;docs/_site&quot; --host &quot;docs.fast.ai&quot; 2&gt; err || true export GITHUB_TOKEN=&quot;YOUR_TOKEN&quot; if [[ -z $(gh issue list -l &quot;broken-link&quot;)) &amp;&amp; (-s err) ]]; then gh issue create -t &quot;Broken links found&quot; -b &quot;$(&lt; err)&quot; -l &quot;broken-link&quot; -R &quot;yourusername/yourrepo&quot; fi . See the GitHub Actions docs for more information. . Resources . The following resources are relevant for those interested in learning more about fastlinkcheck: . The fastlinkcheck GitHub repo | The fastlinkcheck docs | .",
            "url": "https://l1aoxingyu.github.io/blogpages/fastlinkcheck/",
            "relUrl": "/fastlinkcheck/",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "fastcore: An Underrated Python Library",
            "content": ". Background . I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn&#39;t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That&#39;s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions. . And that&#39;s exactly what I did over the past month! I&#39;m pleased to report that it has been the most efficient learning experience I&#39;ve ever experienced. I&#39;ve discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique. . For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I&#39;m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more! . Why fastcore is interesting . Get exposed to ideas from other languages without leaving python: I’ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I’ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages. | You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems. | Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work. | A whirlwind tour through fastcore . Here are some things you can do with fastcore that immediately caught my attention. . . Making **kwargs transparent . Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example: . def baz(a, b=2, c=3, d=4): return a + b + c def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, **kwargs)&gt; . Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates: . def baz(a, b=2, c=3, d=4): return a + b + c @delegates(baz) # this decorator will pass down keyword arguments from baz def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4)&gt; . You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs: . @delegates(baz, keep=True) def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4, **kwargs)&gt; . You can also exclude arguments. For example, we exclude argument d from delegation: . def basefoo(a, b=2, c=3, d=4): pass @delegates(basefoo, but=[&#39;d&#39;]) # exclude `d` def foo(c, a, **kwargs): pass inspect.signature(foo) . &lt;Signature (c, a, b=2)&gt; . You can also delegate between classes: . class BaseFoo: def __init__(self, e, c=2): pass @delegates()# since no argument was passsed here we delegate to the superclass class Foo(BaseFoo): def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs) inspect.signature(Foo) . &lt;Signature (a, b=1, c=2)&gt; . For more information, read the docs on delegates. . . Avoid boilerplate when setting instance attributes . Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__? . class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c . Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr: . class Test: def __init__(self, a, b, c): store_attr() t = Test(5,4,3) assert t.b == 4 . You can also exclude certain attributes: . class Test: def __init__(self, a, b, c): store_attr(but=[&#39;c&#39;]) t = Test(5,4,3) assert t.b == 4 assert not hasattr(t, &#39;c&#39;) . There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail. . P.S. you might be thinking that Python dataclasses also allow you to avoid this boilerplate. While true in some cases, store_attr is more flexible.1 . 1. For example, store_attr does not rely on inheritance, which means you won&#39;t get stuck using multiple inheritance when using this with your own classes. Also, unlike dataclasses, store_attr does not require python 3.7 or higher. Furthermore, you can use store_attr anytime in the object lifecycle, and in any location in your class to customize the behavior of how and when variables are stored.↩ . . Avoiding subclassing boilerplate . One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example: . class ParentClass: def __init__(self): self.some_attr = &#39;hello&#39; class ChildClass(ParentClass): def __init__(self): super().__init__() cc = ChildClass() assert cc.some_attr == &#39;hello&#39; # only accessible b/c you used super . We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass: . class NewParent(ParentClass, metaclass=PrePostInitMeta): def __pre_init__(self, *args, **kwargs): super().__init__() class ChildClass(NewParent): def __init__(self):pass sc = ChildClass() assert sc.some_attr == &#39;hello&#39; . . Type Dispatch . Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y: . collide_with(x::Asteroid, y::Asteroid) = ... # deal with asteroid hitting asteroid collide_with(x::Asteroid, y::Spaceship) = ... # deal with asteroid hitting spaceship collide_with(x::Spaceship, y::Asteroid) = ... # deal with spaceship hitting asteroid collide_with(x::Spaceship, y::Spaceship) = ... # deal with spaceship hitting spaceship . Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks. . Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function: . @typedispatch def f(x:str, y:str): return f&#39;{x}{y}&#39; @typedispatch def f(x:np.ndarray): return x.sum() @typedispatch def f(x:int, y:int): return x+y . Below is a demonstration of type dispatch at work for the function f: . f(&#39;Hello &#39;, &#39;World!&#39;) . &#39;Hello World!&#39; . f(2,3) . 5 . f(np.array([5,5,5,5])) . 20 . There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask). . After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing. . . A better version of functools.partial . functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values &gt;= val: . test_input = [1,2,3,4,5,6] def f(arr, val): &quot;Filter a list to remove any values that are less than val.&quot; return [x for x in arr if x &gt;= val] f(test_input, 3) . [3, 4, 5, 6] . You can create a new function out of this function using partial that sets the default value to 5: . filter5 = partial(f, val=5) filter5(test_input) . [5, 6] . One problem with partial is that it removes the original docstring and replaces it with a generic docstring: . filter5.__doc__ . &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; . fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent: . filter5 = partialler(f, val=5) filter5.__doc__ . &#39;Filter a list to remove any values that are less than val.&#39; . . Composition of functions . A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers: . def add(arr, val): return [x + val for x in arr] def arrsum(arr): return sum(arr) # See the previous section on partialler add2 = partialler(add, val=2) transform = compose(filter5, add2, arrsum) transform([1,2,3,4,5,6]) . 15 . But why is this useful? You might me thinking, I can accomplish the same thing with: . arrsum(add2(filter5([1,2,3,4,5,6]))) . You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following: . def fit(x, transforms:list): &quot;fit a model after performing transformations&quot; x = compose(*transforms)(x) y = [np.mean(x)] * len(x) # its a dumb model. Don&#39;t judge me return y # filters out elements &lt; 5, adds 2, then predicts the mean fit(x=[1,2,3,4,5,6], transforms=[filter5, add2]) . [7.5, 7.5] . For more information about compose, read the docs. . . A more useful __repr__ . In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier). . class Test: def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously Test(1) . &lt;__main__.Test at 0x7ffcd766cee0&gt; . We can use basic_repr to quickly give us a more sensible default: . class Test: def __init__(self, a, b=2, c=3): store_attr() __repr__ = basic_repr(&#39;a,b,c&#39;) Test(2) . Test(a=2, b=2, c=3) . . Monkey Patching With A Decorator . It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so: . class MyClass(int): pass @patch def func(self:MyClass, a): return self+a mc = MyClass(3) . Now, MyClass has an additional method named func: . mc.func(10) . 13 . Still not convinced? I&#39;ll show you another example of this kind of patching in the next section. . . A better pathlib.Path . When you see these extensions to pathlib.path you won&#39;t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as: . Path.readlines: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.readlines() | Path.read: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.read() | Path.save: saves file as pickle | Path.load: loads pickle file | Path.ls: shows the contents of the path as a list. | etc. | . Read more about this here. Here is a demonstration of ls: . from fastcore.utils import * from pathlib import Path p = Path(&#39;.&#39;) p.ls() # you don&#39;t get this with vanilla Pathlib.Path!! . (#7) [Path(&#39;2020-09-01-fastcore.ipynb&#39;),Path(&#39;README.md&#39;),Path(&#39;fastcore_imgs&#39;),Path(&#39;2020-02-20-test.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;2020-02-21-introducing-fastpages.ipynb&#39;),Path(&#39;my_icons&#39;)] . Wait! What&#39;s going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.utils module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I&#39;ll go ahead and add another method to Path right now: . @patch def fun(self:Path): return &quot;This is fun!&quot; p.fun() . &#39;This is fun!&#39; . That is magical, right? I know! That&#39;s why I&#39;m writing about it! . . An Even More Concise Way To Create Lambdas . Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let&#39;s create a lambda for taking the sum of a Numpy array: . arr=np.array([5,4,3,2,1]) f = lambda a: a.sum() assert f(arr) == 15 . You can use Self in the same way: . f = Self.sum() assert f(arr) == 15 . Let&#39;s create a lambda that does a groupby and max of a Pandas dataframe: . import pandas as pd df=pd.DataFrame({&#39;Some Column&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, ], &#39;Another Column&#39;: [5, 7, 50, 70]}) f = Self.groupby(&#39;Some Column&#39;).mean() f(df) . Another Column . Some Column . a 6 | . b 60 | . Read more about Self in the docs). . . Notebook Functions . These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell: . from fastcore.imports import in_notebook, in_colab, in_ipython in_notebook(), in_colab(), in_ipython() . (True, False, True) . This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment. . . A Drop-In Replacement For List . You might be pretty happy with Python&#39;s list. This is one of those situations that you don&#39;t know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies. . The best way I can describe L is to pretend that list and numpy had a pretty baby: . define a list (check out the nice __repr__ that shows the length of the list!) . L(1,2,3) . (#3) [1,2,3] . Shuffle a list: . p = L.range(20).shuffle() p . (#20) [8,7,5,12,14,16,2,15,19,6...] . Index into a list: . p[2,4,6] . (#3) [5,14,2] . L has sensible defaults, for example appending an element to a list: . 1 + L(2,3,4) . (#4) [1,2,3,4] . There is much more L has to offer. Read the docs to learn more. . But Wait ... There&#39;s More! . There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn&#39;t demo in this blog post: . Utilities . The Basics section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides. . mk_class: quickly add a bunch of attributes to a class | wrap_class: add new methods to a class with a simple decorator | groupby: similar to Scala&#39;s groupby | merge: merge dicts | fasttuple: a tuple on steroids | Infinite Lists: useful for padding and testing | chunked: for batching and organizing stuff | . Multiprocessing . The Multiprocessing section extends python&#39;s multiprocessing library by offering features like: . progress bars | ability to pause to mitigate race conditions with external services | processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks | . Functional Programming . The functional programming section is my favorite part of this library. . maps: a map that also composes functions | mapped: A more robust map | using_attr: compose a function that operates on an attribute | . Transforms . Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post. . Further Reading . It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation. . The fastcore documentation site. | The fastcore GitHub repo. | Blog post on delegation. | . Shameless plug: fastpages . This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages. .",
            "url": "https://l1aoxingyu.github.io/blogpages/fastcore/",
            "relUrl": "/fastcore/",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "FastReID: 一个面向学术界和工业界的 ReID Toolbox",
            "content": "引言 . FastReID 平台已经成为京东人工智能研究（JD AI Research）的开源项目之一，它是面向学术界和工业界的研究/开源项目，主要用于研究和应用落地。 先放上 Github 链接： . fast-reid . 我们的 FastReID 也有一篇 paper 进行更加详细地介绍，如果想要了解更多关于 FastReID 的信息，可以直接去看原始 paper。 . FastReID: A Pytorch Toolbox for Real-world Person Re-identification . 接下来会分享开发 FastReID 初衷以及 FastReID 的特点。 . 动机 . 最早的时候和罗博(@罗浩)搞了一个 reid strong baseline，不过那个项目在 pytorch 的基础上，又用 ignite 包了一下，开源之后很多人都说 ignite 比较影响使用体验，所以后面在我自己维护的 baseline 版本里面就去掉了 ignite。 . 我们自己做项目，以及实习生做研究都是基于 strong baseline 去魔改的，后面发现各自搞的 project 和原始的 baseline 差别越来越大，导致我们想要在实际场景中运用研究工作时效果不好，遇到了很多代码不对齐的现象。出现这个问题原因在于其中一个同学修改了某一个训练逻辑或者预处理的地方，他自己忘记了，最终发现把模型合并在一起的效果总是不好，需要花很多时间去解决这些琐碎的问题。 . 正是由于这个原因，我们决定把 baseline 这套框架封成一个库，大家基于这套库去做工作就更利于找到各自定制化的地方。开源社区中也有几个比较流行的 reid 库，比如 torchreid，Person_reID_baseline_pytorch 等等，都是很好的库，值得我们去学习。最近 Facebook AI Researck 开源了 Detectron2 项目，它里面的整体概念和设计哲学都非常棒，所以我们决定参考 detectron2 的设计来整个 FastReID 架构。 基于 FastReID，我们的产品模型和 research 的模型有了比较好的兼容性，同时也比较容易去 customize 一些功能，模块化的设计允许研究人员能自定义地插入自己想要的模块。 . 我们构建 FastReID 的目的在于满足 JD AI Research 的研究需求，能够快速地准备地实现一些 ideas，并且能够将研究员的研究成果快速地部署到实践中。 无论在学术界还是工业界，开源项目都有助于整个社区的快速发展，使我们的想法快速付诸于实际落地项目中。我们也希望 FastReID 的发布能够继续加速行人重识别领域的发展。 . 一些新特性 . FastReID 采用高度模块化设计，它具有更高的灵活性和可扩展性，能够在支持多 GPU 训练，它的扩展性设计使其在重构代码的情况快速实现很多研究项目。 . 下面我们介绍一下其中的一些新特性。 . 1.基于 FastReID，我们在多个 ReID 任务都获得非常不错的性能，并且用于业务线中，包括行人 ReID、Occluded/Partial 行人 ReID、跨域行人 ReID 和车辆 ReID。 . 虽然在 ReID 发展的这几年里面，有了很多 ReID 的 paper，大家的刷的点也越来越高了，但是性能好且稳定的方法其实还是基于最简单的 global feature 和分块的 local feature，其他使用额外信息如 pose，mask，parsing 之类的方法在实际使用中都不够稳定，同时也比较笨重。 . 所以我们在 toolbox 中内置了这两种方法，一种是基于 global feature 的 strong baseline，一种是基于分块的 MGN。 然后在 BagofTricks 的基础上，将其他可能有用的 tricks 都实现了一下，包括有效的，比如 circle loss，gem pooling 之类的，也有没有效果的，比如 SWA, AugMix 等等。 最终基于 ResNet50-ibn backbone，在三个数据库上实现了下面的性能 . Method Market1501 DukeMTMC MSMT17 .   | Rank@1 (mAP) | Rank@1 (mAP) | Rank@1 (mAP) | . BagTricks | 94.4% (86.1%) | 87.1% (76.9%) | 72.3% (48.3%) | . FastReID-baseline | 95.7% (89.3%) | 91.3% (81.6%) | 84.0% (61.2%) | . FastReID-MGN | 95.8% (89.7%) | 91.6% (82.1%) | 85.1% (65.4%) | . 在 Marekt1501 上面提升空间已经不大了，因为后面有一些错误标签，但是在 DukeMTMC 和 MSMT17 上还是有比较显著的提升，详情可以去 model zoo 里面查看完整的配置文件。 . 在 partial re-id 上，我们也基于之前 DSR 的工作，在三个 partial 库上有了持续的提升 . Method PartialReID OccludedReID PartialiLIDS .   | Rank@1 (mAP) | Rank@1 (mAP) | Rank@1 (mAP) | . FPR | 81.0% (76.6%) | 78.3% (68.0%) | 68.1% (61.8%) | . FastReID-DSR | 82.7% (76.8%) | 81.6% (70.9%) | 73.1% (79.8%) | . 具体可以去 projects/PartialReID 中查看代码和训练配置。 . 在 cross-domain reid 上面，我们也做了一些工作，正在投稿中，之后会在开源在projects/Cross-domain-reid 中，从效果上看，在跨域上已经大大缩小了和有监督 reid 的差距。 . Method Market1501 to DukeMTMC DukeMTMC to Market1501 .   | Rank@1 (mAP) | Rank@1 (mAP) | . DirectTransfer | 54.4% (34.0%) | 62.6% (32.1%) | . Our method | 82.7% (69.2%) | 92.7% (80.5%) | . 在实际场景中我们发现穿黑衣服的人是一个比较难的问题，所以我们也基于 FastReID 构建了头肩模块去解决黑衣人的问题，也实现了比较不错的性能提升，paper 正在投稿，后面会开源在 projects/HAA 中。 . Method Black-ReID .   | Rank@1 (mAP) | . Baseline(R50) | 80.9% (70.8%) | . HAA(R50) | 86.7% (79.0%) | . 在 vehicle re-id 上，我们也在 VeRI 数据集上跑了一下 baseline，得到了一个比较不错的结果，另外两个数据集 VehicleID 和 VERI-Wild 上也跑了一下，具体可以去 model zoo 里面查看。 . Method VeRi .   | Rank@1 (mAP) | . FastReID-baseline | 97.0% (81.9%) | . 另外还有一些基于 FastReID 做的工作都在投稿中，就不详细介绍了，后续都会开源在 fast-reid/projects 里面。 . 2.在模型评估上我们实现了更多的功能，比如我们支持比较灵活的测试方式，通过下面的命令可以实现在 Market1501 和 MSMT17 上联合训练，然后在 Market1501 和 DukeMTMC 上进行测试。 . DATASETS: NAMES: (&quot;Market1501&quot;, &quot;MSMT17&quot;,) TESTS: (&quot;Market1501&quot;, &quot;DukeMTMC&quot;,) . 另外也提供了更加丰富的指标评估，除了 reid 中最为常见的 CMC 和 mAP，以及在 reid-survey 中提出的 mINP之外，我们还提供了 ROC 曲线和分布图 . 因为我们发现在实际业务场景中往往是开集测试，甚至 gallery 都是在动态变化的，在这种情况下通过单一的 rank1 或者是 mAP 来评估模型就不那么准确了，在实际应用时往往需要卡阈值再出 topK，所以通过分布和 ROC 曲线可以更好地帮我们找到阈值。 . 除了评估指标，可视化其实非常重要，通过可视化 rank list 可以快速定位模型的问题，同时也会发现一些错误标注，比如通过可视化我们发现 Market1501 里面有一些错误标注，最高的 rank@1 就只能做到 96 左右，而一些公司的 PR 文可以做到 99，我也不知道他们是怎么做到把标注错误都搞定的 😂。 . 我们发现很多库都只是实现了最基本的可视化功能，比如可视化 rank list，但是这种单一的可视化其实并不能帮助我们从多个维度了解问题，所以我们实现了更好的可视化功能。首先可以根据每个 query 的 AP 进行排序展示，比如 AP 从小到大进行展示，那么可视化出来的第一张图片就是 AP 最低的 query，通过这个方式我们可以了解到模型处理能力最差的 bad case。 . 另外我们在看预测结果的时候，其实也会想知道到底这个 query 的标注是怎么样的图片，比如我们再看 duke 数据集中下面的 rank list 时，发现他的 AP 是0，下面的蓝色框表示都是错误的匹配。 . 这时我们就会疑惑，到底这张 query 的标注长什么样，这时如果我们像下面这样将 label 同时可视化出来，我们就可以快速地知道，原来 query 其实是黄衣服后面那个黑衣服的人，因为是用 tracking 算法标注的，他大部分都被前面穿黄衣服的人挡住了，所以模型无法找对，而且这种情况下搞模型结构很难解决的，在实际业务中直接从源头上选择质量好的 query 是一个更好的解决方案。 . 3.大多数的库都只关注学术界做 research，我们更希望能够产学研结合，research 中 work 的东西能够快速到实际场景中去验证效果，发现实际中真正需要解决的问题。 当然在实际研究中可以天马行空去写代码，但是这份代码无法快速地在实际场景中去验证，如果基于 FastReID 去重构和开发，那么我们就能够找到新方法所需要的最小代码实现，就能够很轻易地移植到实际业务中，也不用把大量的时间花在对齐训练逻辑以及预处理上了。 . 另外就是如何将 pytorch 训练的模型更容易地部署到生产环境上，这也是工业界比较关心的事情，python 写的模型如果没有优化和加速的话，在实际中是很慢的。 为了更好地在工业界中应用，我们会在 FastReID 中加上一些脚本能够容易地将 pytorch 训练的模型转到 caffe 和 TensorRT 上，最后做一下模型的量化。目前 pytorch 升级到 1.3 之后慢慢开始支持量化了，我们也会尝试在 pytorch 端直接做量化，和蒸馏小模型。不过这些部分的内容还在整理和开发中，目前还没有 ready。 . 未来的一些改进方向 . 上面说了 FastReID 中的一些新特性，同时还有一些地方需要继续改进。 . 目前的多卡训练还是基于 DataParallel 来实现的，会存在负载不均衡，速度损失以及无法实现多机的缺点，我们正在用 DistributedDataParallel 来替换 DataParallel。 | 模型转换，量化和蒸馏小模型等部分的代码还没有搞定，后续会慢慢开源一部分。 | 可能会考虑将 FastReID 推广到通用的 image retrieval 上。 | 结语 . 科技的进步是整个社区的努力，包括学术界和工业界。 个人的努力永远赶不上整个社区的努力，这也是开源 FastReID 的初衷。 我们一直主张共享代码，快速试验新的想法，通过 FastReID 的发布加速整个 ReID 的产业化落地。 我们也会继续发展和完善FastReID。希望大家能够 star/fork/watch/pr，大家互相学习，共同推动计算机视觉的发展。 . 在此感谢 JD AI 组的同事和老师的支持，正是因为大家的努力让 FastReID 变得更好，并且科研项目也都在 FastReID 上取得了很好的性能。 . . [1] Luo, Hao and Gu, Youzhi and Liao, Xingyu and Lai, Shenqi and Jiang, Wei. Bag of Tricks and a Strong Baseline for Deep Person Re-Identification. . [2] Wang, G. and Yuan, Y. and Chen, X. and Li, J. and Zhou, X. Learning Discriminative Features with Multiple Granularities for Person Re-Identification. . [3] Ye, Mang and Shen, Jianbing and Lin, Gaojie and Xiang, Tao and Shao, Ling and Hoi, Steven C. H.Deep Learning for Person Re-identification: A Survey and Outlook. . [4] Y. Sun, C. Cheng, Y. Zhang, C. Zhang, L. Zheng, Z. Wang, Y. Wei. Circle Loss: A Unified Perspective of Pair Similarity Optimization. .",
            "url": "https://l1aoxingyu.github.io/blogpages/reid/fastreid/2020/05/29/fastreid.html",
            "relUrl": "/reid/fastreid/2020/05/29/fastreid.html",
            "date": " • May 29, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Introducing fastpages",
            "content": ". We are very pleased to announce the immediate availability of fastpages. fastpages is a platform which allows you to create and host a blog for free, with no ads and many useful features, such as: . Create posts containing code, outputs of code (which can be interactive), formatted text, etc directly from Jupyter Notebooks; for instance see this great example post from Scott Hawley. Notebook posts support features such as: Interactive visualizations made with Altair remain interactive. | Hide or show cell input and output. | Collapsable code cells that are either open or closed by default. | Define the Title, Summary and other metadata via a special markdown cells | Ability to add links to Colab and GitHub automatically. | . | Create posts, including formatting and images, directly from Microsoft Word documents. | Create and edit Markdown posts entirely online using GitHub&#39;s built-in markdown editor. | Embed Twitter cards and YouTube videos. | Categorization of blog posts by user-supplied tags for discoverability. | ... and much more | . fastpages relies on Github pages for hosting, and Github Actions to automate the creation of your blog. The setup takes around three minutes, and does not require any technical knowledge or expertise. Due to built-in automation of fastpages, you don&#39;t have to fuss with conversion scripts. All you have to do is save your Jupyter notebook, Word document or markdown file into a specified directory and the rest happens automatically. Infact, this blog post is written in a Jupyter notebook, which you can see with the &quot;View on GitHub&quot; link above. . fast.ai have previously released a similar project called fast_template, which is even easier to set up, but does not support automatic creation of posts from Microsoft Word or Jupyter notebooks, including many of the features outlined above. . Because fastpages is more flexible and extensible, we recommend using it where possible. fast_template may be a better option for getting folks blogging who have no technical expertise at all, and will only be creating posts using Github&#39;s integrated online editor. . Setting Up Fastpages . The setup process of fastpages is automated with GitHub Actions, too! Upon creating a repo from the fastpages template, a pull request will automatically be opened (after ~ 30 seconds) configuring your blog so it can start working. The automated pull request will greet you with instructions like this: . . All you have to do is follow these instructions (in the PR you receive) and your new blogging site will be up and running! . Jupyter Notebooks &amp; Fastpages . In this post, we will cover special features that fastpages provides for Jupyter notebooks. You can also write your blog posts with Word documents or markdown in fastpages, which contain many, but not all the same features. . Options via FrontMatter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . All of the above settings are enabled in this post, so you can see what they look like! . the summary field (preceeded by &gt;) will be displayed under your title, and will also be used by social media to display as the description of your page. | toc: setting this to true will automatically generate a table of contents | badges: setting this to true will display Google Colab and GitHub links on your blog post. | comments: setting this to true will enable comments. See these instructions for more details. | author this will display the authors names. | categories will allow your post to be categorized on a &quot;Tags&quot; page, where readers can browse your post by categories. | . Markdown front matter is formatted similarly to notebooks. The differences between the two can be viewed on the fastpages README. . Code Folding . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . If you want to completely hide cells (not just collapse them), read these instructions. . Interactive Charts With Altair . Interactive visualizations made with Altair remain interactive! . We leave this below cell unhidden so you can enjoy a preview of syntax highlighting in fastpages, which uses the Dracula theme. . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Other Features . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Images w/Captions . You can include markdown images with captions like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Of course, the caption is optional. . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . More Examples . This tutorial contains more examples of what you can do with notebooks. . How fastpages Converts Notebooks to Blog Posts . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . We will discuss how GitHub Actions work in a follow up blog post. . Resources &amp; Next Steps . We highly encourage you to start blogging with fastpages! Some resources that may be helpful: . fastpages repo - this is where you can go to create your own fastpages blog! | Fastai forums - nbdev &amp; blogging category. You can ask questions about fastpages here, as well as suggest new features. | nbdev: this project powers the conversion of Jupyter notebooks to blog posts. | . If you end up writing a blog post using fastpages, please let us know on Twitter: @jeremyphoward, @HamelHusain. .",
            "url": "https://l1aoxingyu.github.io/blogpages/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "relUrl": "/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "date": " • Feb 21, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://l1aoxingyu.github.io/blogpages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Self-Supervised Learning 入门介绍",
            "content": "引子 . 最近 self-supervised learning 变得非常火，首先是 kaiming 的 MoCo 引发一波热议，然后最近 Yann 在 AAAI 上讲 self-supervised learning 是未来。 所以觉得有必要了解一下 SSL，也看了一些 paper 和 blog，最后决定写这篇文章作为一个总结。 . . 什么是 Self-Supervised Learning . 首先介绍一下到底什么是 SSL，我们知道一般机器学习分为监督学习，非监督学习和强化学习。 而 self-supervised learning 是无监督学习里面的一种，主要是希望能够学习到一种通用的特征表达用于下游任务。 其主要的方式就是通过自己监督自己，比如把一段话里面的几个单词去掉，用他的上下文去预测缺失的单词，或者将图片的一些部分去掉，依赖其周围的信息去预测缺失的 patch。 . 根据我看的文章，现在 self-supervised learning 主要分为两大类：1. Generative Methods；2. Contrastive Methods。 下面我们分别简要介绍一下这这两种方法。 . Generative Methods . 首先我们介绍一下 generative methods。 这类方法主要关注 pixel space 的重建误差，大多以 pixel label 的 loss 为主。 主要是以 AutoEncoder 为代表，以及后面的变形，比如 VAE 等等。 对编码器的基本要求就是尽可能保留原始数据的重要信息，所以如果能通过 decoder 解码回原始图片，则说明 latent code 重建的足够好了。 . . 这种直接在 pixel level 上计算 loss 是一种很直观的做法，除了这种直接的做法外，还有生成对抗网络的方法，通过判别网络来算 loss。 . 对于 generative methods，有一些问题，比如： . 基于 pixel 进行重建计算开销非常大； | 要求模型逐像素重建过于苛刻，而用 GAN 的方式构建一个判别器又会让任务复杂和难以优化。 | 从这个 blog 中我看到一个很好的例子来形容这种 generative methods。 对于一张人民币，我们能够很轻易地分辨其真假，说明我们对其已经提取了一个很好的特征表达，这个特征表达足够去刻画人民币的信息， 但是如果你要我画一张一模一样的人民币的图片，我肯定没法画出来。 通过这个例子可以明显看出，要提取一个好的特征表达的充分条件是能够重建，但是并不是必要条件，所以有了下面这一类方法。 . . Contrasive self-supervised learning . 除了上面这类方法外，还有一类方法是基于 contrastive 的方法。 这类方法并不要求模型能够重建原始输入，而是希望模型能够在特征空间上对不同的输入进行分辨，就像上面美元的例子。 . 这类方法有如下的特点：1. 在 feature space 上构建距离度量；2. 通过特征不变性，可以得到多种预测结果；3. 使用 Siamese Network；4. 不需要 pixel-level 重建。 正因为这类方法不用在 pixel-level 上进行重建，所以优化变得更加容易。当然这类方法也不是没有缺点，因为数据中并没有标签，所以主要的问题就是怎么取构造正样本和负样本。 . 目前基于 contrastive 的方法已经取得了很好的紧张，在分类任上已经接近监督学习的效果，同时在一些检测、分割的下游任务上甚至超越了监督学习作为 pre-train的方法。 . 下面是这两类方法的总结图片。 . . 为什么需要 self-supervised learning . 上面我们讲了什么是 self-supervised learning，那么为什么我们需要自监督学习呢，以及它能够给我们带来哪些帮助？ . 在目前深度学习发展的情况下，对于监督学习，我们希望使用更少的标注样本就能够训练一个泛化能力很好的模型，因为数据很容易获取，但是标注成本却是非常昂贵的。 而在强化学习中，需要大量的经验对 agent 进行训练，如果能搞减少 agent 的尝试次数，也能够加速训练。 除此之外，如果拿到一个好的特征表达，那么也有利于做下游任务的 finetune 和 multi-task 的训练。 . 最后我们总结一下监督学习和自监督学习的特点，其中 supervised learning 的特点如下： . 对于每一张图片，机器预测一个 category 或者是 bounding box | 训练数据都是人所标注的 | 每个样本只能提供非常少的信息(比如 1024 个 categories 只有 10 bits 的信息) | 于此对比的是，self-supervised learning 的特点如下： . 对于一张图片，机器可以预任何的部分 | 对于视频，可以预测未来的帧 | 每个样本可以提供很多的信息 | 所以通过自监督学习，我们可以做的事情可以远超过监督学习，也难怪 Yann 未来看好 self-supervised learning。 目前出现的性能很好的文章主要是基于 contrastive 的方法，所以下面我们介绍几篇基于 contrastive 方法的文章。 . Contrastive Predictive Coding . 第一篇文章是 Representation Learning with Contrastive Predictive Coding。 这篇文章主要是通过 contrastive 的方式在 speech, images, text 和 reinforcement learning 中都取得了很好的效果。 . 从前面我们知道，由一个原始的 input 去建模一个 high-level representation 是很难的，这也是自监督学习想做的事情。 其中常用的策略是: future，missing 和 contextual，即预测未来的信息，比如 video 中当前帧预测后面的帧；丢失的信息或者是上下文的信息，比如 NLP 里面的 word2vec 和 BERT。 . 对于一个目标 x 和他的上下文 c 来说，直接去建模输出 $p(x|c)$ 会损失很多信息，将 target x 和 context c 更合适的建模方式是最大化他们之间的 mutual information，即下面的公式 . I(x;c)=∑x,cp(x,c)log⁡p(x∣c)p(x)I(x; c)= sum_{x, c} p(x, c) log frac{p(x | c)}{p(x)}I(x;c)=x,c∑​p(x,c)logp(x)p(x∣c)​ . 优化了他们之间的互信息，即最大化 $ frac{p(x | c)}{p(x)}$，说明 $p(x|c)$ 要远大于 $p(x)$，即在给定 context c 的情况下， 要找到专属于 c 的那个 x，而不是随机采样的 x。 . 基于这个观察，论文对 density ratio 进行建模，这样可以保留他们之间的互信息 . fk(xt+k,ct)∝p(xt+k∣ct)p(xt+k)f_{k} left(x_{t+k}, c_{t} right) propto frac{p left(x_{t+k} | c_{t} right)}{p left(x_{t+k} right)}fk​(xt+k​,ct​)∝p(xt+k​)p(xt+k​∣ct​)​ . 对于这个 density ratio，可以构建左边的函数 f 去表示它，只要基于函数 f 构造下面的损失函数，优化这个损失函数就等价于优化这个 density ratio，下面论文会证明这一点。 . LN=−EX[log⁡fk(xt+k,ct)∑xj∈Xfk(xj,ct)] mathcal{L}_{ mathrm{N}}=- underset{X}{ mathbb{E}} left[ log frac{f_{k} left(x_{t+k}, c_{t} right)}{ sum_{x_{j} in X} f_{k} left(x_{j}, c_{t} right)} right]LN​=−XE​[log∑xj​∈X​fk​(xj​,ct​)fk​(xt+k​,ct​)​] . 而这个损失函数，其实就是一个类似交叉熵的函数，分子是正样本的概率，分母是正负样本的概率求和。 . 下面我们证明如果能够最优化这个损失函数，则等价于优化了 density ratio，也就优化了互信息。 . 首先将这个 loss 函数变成概率的形式，最大化这个正样本的概率分布，然后通过 bayesian 公式进行推导，其中 X 是负样本，和 $x_i$ 以及 c 都无关。 . p(xi∣X,ct)=p(X∣xi,ct)p(xi∣ct)∑j=1Np(X∣xj,ct)p(xj∣ct)=p(xi∣ct)∏l≠ip(xl)∑j=1Np(xj∣ct)∏l≠jp(xl)=p(xi∣ct)p(xi)∑j=1Np(xj∣ct)p(xj) begin{aligned} p left(x_i | X, c_{t} right) &amp;= frac{p(X | x_i, c_t) p(x_i | c_t)}{ sum_{j=1}^N p(X | x_j, c_t) p(x_j | c_t)} . &amp;= frac{p left(x_{i} | c_{t} right) prod_{l neq i} p left(x_{l} right)}{ sum_{j=1}^{N} p left(x_{j} | c_{t} right) prod_{l neq j} p left(x_{l} right)} &amp;= frac{ frac{p left(x_{i} | c_{t} right)}{p left(x_{i} right)}}{ sum_{j=1}^{N} frac{p left(x_{j} | c_{t} right)}{p left(x_{j} right)}} end{aligned}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;p(xi​∣X,ct​)​=∑j=1N​p(X∣xj​,ct​)p(xj​∣ct​)p(X∣xi​,ct​)p(xi​∣ct​)​=∑j=1N​p(xj​∣ct​)∏l​=j​p(xl​)p(xi​∣ct​)∏l​=i​p(xl​)​=∑j=1N​p(xj​)p(xj​∣ct​)​p(xi​)p(xi​∣ct​)​​​&lt;/span&gt;&lt;/span&gt; . 通过上面的推导，可以看出优化这个损失函数其实就是在优化 density ratio。论文中把 f 定义成一个 log 双线性函数，后面的论文更加简单，直接定义为了 cosine similarity。 . fk(xt+k,ct)=exp⁡(zt+kTWkct)f_{k} left(x_{t+k}, c_{t} right)= exp left(z_{t+k}^{T} W_{k} c_{t} right)fk​(xt+k​,ct​)=exp(zt+kT​Wk​ct​) . 有了这个 loss，我们只需要采集正负样本就可以了。 对于语音和文本，可以充分利用了不同的 k 时间步长，来采集正样本，而负样本可以从序列随机取样来得到。 对于图像任务，可以使用 pixelCNN 的方式将其转化成一个序列类型，用前几个 patch 作为输入，预测下一个 patch。 . . . Deep InfoMax . 通过上面的分析和推导，我们有了这样一个通用的框架，那么 deep infomax 这篇文章就非常好理解了，其中正样本就是第 i 张图片的 global feature 和中间 feature map 上个的 local feature，而负样本就是另外一张图片作为输入，非常好理解。 . . Contrastive MultiView Coding . 除了像上面这样去构建正负样本，还可以通过多模态的信息去构造，比如同一张图片的 RGB图 和 深度图。 CMC 这篇 paper 就是从这一点出发去选择正样本，而且通过这个方式，每个 anchor 不仅仅只有一个正样本，可以通过多模态得到多个正样本，如下图右边所示。 . . 现在我们能够拿到很多正样本，问题是怎么获得大量的负样本，对于 contrastive loss 而言，如何 sample 到很多负样本是关键，mini-batch 里面的负样本太少了，而每次对图片重新提取特征又非常的慢。虽然可以通过 memory bank 将负样本都存下来，但是效果并不好，所以如何节省内存和空间获得大量的负样本仍然没有很好地解决。 . MoCo . 有了上面这么多工作的铺垫，其实 contrastive SSL 的大框架已经形成了，MoCo 这篇文章也变得很好理解，可以把 target x 看成第 i 张图片的随机 crop，他的正样本通过一个 model ema 来得到，可以理解为过去 epochs 对这张图片的 smooth aggregation。 而负样本则从 memory bank 里面拿，同时 memory bank 的 feature 也是通过 model ema 得到，并且通过队列的形式丢掉老的 feature。 . . MoCo 通过工程的方式，和一些 trick，比如 model ema 和 shuffleBN 来解决之前没法很好 sample 负样本的问题。 . SimCLR . 最近，hinton 组也放了一篇做 ssl 的 paper，其实都是用的同一套框架，也没有太多的 novelty。 虽然摘要里面说可以抛弃 memory bank，不过细看论文，训练的 batchsize 需要到几千，要用32-128 cores 的 TPU，普通人根本用不起。 . 不过这篇文章系统地做了很多实验，比如探究了一下数据增强的影响，以及的 projection head 的影响等，不过也没有从理论上去解释这些问题，只是做了实验之后获得了一些结论。 . Results . . 最后展示了不同方法的结果，可以看到在性能其实已经逼近监督学习的效果，但是需要 train 4x 的时间，同时网络参数也比较大。 . 虽然性能没有超过监督学习，不过我认为这仍然给了我们很好的启发，比如训练一个通用的 encoder 来接下游任务，或者是在 cross domain 的时候只需要少量样本去 finetune，这都会给实际落地带来收益。 . Reference . contrastive self-supervised learning . deep infomax 和 深度学习中的互信息 .",
            "url": "https://l1aoxingyu.github.io/blogpages/summary/self-supervised%20learning/2020/02/20/ssl-survey.html",
            "relUrl": "/summary/self-supervised%20learning/2020/02/20/ssl-survey.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "A Simple Framework for Contrastive Learning of Visual Representations" 阅读笔记",
            "content": "介绍 . 这篇文章是 Hinton 团队出品的，主要做的是目前炙手可热的领域，self supervised learning， 提出了一个简单的框架来解决 visual representation 中的 contrastive learning。 其实前两个月 kaiming 团队也提出了一个叫 MoCo 的方法来解决这个问题，这篇文章总体思路和 MoCo 几乎一样，最大的 contribution 我认为是去探索了框架中的每个部分分别对最终结果的影响。 最后根据论文的发现，作者调出了目前最强的结果如下，点数非常高。 . . 主要贡献 . SimCLR 整体框架如下，和目前其他的方法是一致的 . . 主要由四个部分组成： . 随机数据增强 | 神经网络 encoder | project head $g( centerdot)$ 进行非线性映射和降维 | contrastive loss 函数 | li,j=−log⁡exp⁡(sim(zi,zj)/τ)∑k≠iexp⁡(sim(zi,zk)/τ)l_{i,j} = - log frac{ exp(sim(z_i, z_j)/ tau)}{ sum_{k neq i} exp(sim(z_i, z_k)/ tau)}li,j​=−log∑k​=i​exp(sim(zi​,zk​)/τ)exp(sim(zi​,zj​)/τ)​ . Memory bank . 这篇文章提出了可以去掉 memory bank 进行训练，实际上并不可行。 因为作者使用了 8192 的 batch size，这样每个 batch 可以产生 16382 个负样本。 当然当前 batch 提取的 feature 对比 memory bank 更好，但是这需要 128 cores 的 TPU 进行训练，对于财大气粗的 google 当然用得起，对于普通的研究人员来讲，还是老老实实用 memory bank 吧。 . Global BN . 使用 contrastive loss 进行训练的时候，正样本是一张相同的图片通过不同的数据增强方式得到的，这两张图片都在相同的 batch 中，这样非常因为 bn 统计量的问题出现信息泄露。 这篇文章使用了 global bn 的方式来就解决，即大 batch 下面，使用所有图片统计 bn 的均值和方差。 当然使用 MoCo 中的 suffle bn 也是可以的。 . 数据增强 . 本文系统的探索了数据增强对于表示学习的影响，其中 random cropping 和 random color distortion 是非常有用的。 random cropping 可以产生很多小 patch，但是这些小 patch 有着非常相似的颜色分布，所以可以用 color distortion 去弥补这个问题。 . Projection Head . 不同的 head 也有着不同的影响 . . 可以看出，直接使用 global average feature 效果是最差的，而一个 non-linear head 有着最好的效果。 . 其他的因素 . 除了上面这些因素之外，还用 contrastive loss 中的 temperatual factor $ tau$ 的影响，以及是否对 feature 做归一化。 当然这些在别的 paper 中都有了结论，这里就不再赘述。 . 另外还有 batch size 的影响，因为其没有用 memory bank，当然 batch size 越大，包含越多的负样本，效果越好。 . 总结 . 总体来说，这篇文章通过了很多实验来验证到底是哪些因素影响了 SSL 的效果。 很多结论也非常 solid，效果也非常好，可以指导很多调参的工作， 但是 novelty 上并没有给人太大的启发。 .",
            "url": "https://l1aoxingyu.github.io/blogpages/self-supervised%20learning/2020/02/15/simclr.html",
            "relUrl": "/self-supervised%20learning/2020/02/15/simclr.html",
            "date": " • Feb 15, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://l1aoxingyu.github.io/blogpages/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Currently, Xingyu is working as the Researcher Engineer in JD AI Lab. . Before that, Xingyu received his M.Sc.’s degree in USTC (University of Science and Technology of China). . Research Interests . Xingyu’s current research interests mainly include machine learning, computer vision, especially on deep learning, visual recognition and person re-identification. .",
          "url": "https://l1aoxingyu.github.io/blogpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://l1aoxingyu.github.io/blogpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}