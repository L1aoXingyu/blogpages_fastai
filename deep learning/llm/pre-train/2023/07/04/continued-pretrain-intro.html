<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>如何做 continued pre-train | Distill</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="如何做 continued pre-train" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="介绍一下 continued pre-train" />
<meta property="og:description" content="介绍一下 continued pre-train" />
<link rel="canonical" href="https://l1aoxingyu.github.io/blogpages/deep%20learning/llm/pre-train/2023/07/04/continued-pretrain-intro.html" />
<meta property="og:url" content="https://l1aoxingyu.github.io/blogpages/deep%20learning/llm/pre-train/2023/07/04/continued-pretrain-intro.html" />
<meta property="og:site_name" content="Distill" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-07-04T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://l1aoxingyu.github.io/blogpages/deep%20learning/llm/pre-train/2023/07/04/continued-pretrain-intro.html","@type":"BlogPosting","headline":"如何做 continued pre-train","dateModified":"2023-07-04T00:00:00-05:00","datePublished":"2023-07-04T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://l1aoxingyu.github.io/blogpages/deep%20learning/llm/pre-train/2023/07/04/continued-pretrain-intro.html"},"description":"介绍一下 continued pre-train","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blogpages/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://l1aoxingyu.github.io/blogpages/feed.xml" title="Distill" /><link rel="shortcut icon" type="image/x-icon" href="/blogpages/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blogpages/">Distill</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blogpages/about/">About Me</a><a class="page-link" href="/blogpages/search/">Search</a><a class="page-link" href="/blogpages/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">如何做 continued pre-train</h1><p class="page-description">介绍一下 continued pre-train</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2023-07-04T00:00:00-05:00" itemprop="datePublished">
        Jul 4, 2023
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      1 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blogpages/categories/#deep learning">deep learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blogpages/categories/#LLM">LLM</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blogpages/categories/#pre-train">pre-train</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#0x0-什么是-continued-pre-train">0x0 什么是 continued pre-train</a></li>
<li class="toc-entry toc-h2"><a href="#0x1-为什么要做-continued-pre-train">0x1 为什么要做 continued pre-train</a></li>
<li class="toc-entry toc-h2"><a href="#0x2-continued-pre-train-有什么挑战">0x2 continued pre-train 有什么挑战</a>
<ul>
<li class="toc-entry toc-h3"><a href="#0x21-词表扩充问题">0x2.1 词表扩充问题</a></li>
<li class="toc-entry toc-h3"><a href="#0x22-防止遗忘的问题">0x2.2 防止遗忘的问题</a></li>
<li class="toc-entry toc-h3"><a href="#0x23-如何高效-tuning">0x2.3 如何高效 tuning</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#0x3-总结">0x3 总结</a></li>
</ul><p>在 LLM 中，为了获得一个好的 basemodel，往往都会使用 pre-train 的方式。但是通常 pre-train 都需要消耗很多资源，不管是算力还是数据。其实除了 pre-train 之外，还有一种方案叫做 continued pre-train，这篇文章会简要介绍一下这种方案。</p>

<h2 id="0x0-什么是-continued-pre-train">
<a class="anchor" href="#0x0-%E4%BB%80%E4%B9%88%E6%98%AF-continued-pre-train" aria-hidden="true"><span class="octicon octicon-link"></span></a>0x0 什么是 continued pre-train</h2>
<p>continued pre-train 从名字就可以看出，他是在 pre-train 的基础上继续训练，是一个 secondary stage 任务。他的 loss function 和 pre-train 完全一致，在 decoder-only 的架构下是以 AR<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> 做为目标进行训练。</p>

<p>我们知道 SIFT<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> 也是一个 pre-train 的 secondary stage 任务，那么他们之间有什么区别呢？
他们之间的区别主要有下面这几个方面：1）SIFT 的数据规模和 continued pre-train 不同，远远少于 pre-train 阶段，而且 SIFT 如果使用过多的数据容易出现 overfitting 的问题；2）SIFT 的数据对质量和多样性的要求比较高；3）SIFT 和 pre-train 的 loss objective 有轻微区别，SIFT 是部分的 AR，会 mask 掉 instruction 部分，只对 response 部分算 loss。</p>

<h2 id="0x1-为什么要做-continued-pre-train">
<a class="anchor" href="#0x1-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%81%9A-continued-pre-train" aria-hidden="true"><span class="octicon octicon-link"></span></a>0x1 为什么要做 continued pre-train</h2>
<p>continued pre-train 介于 pre-train 和 SIFT 之间，他的存在有着他的独特价值。</p>

<p>目前存在的基本共识是 pre-train 阶段将知识注入模型，在 SIFT 阶段学习 style 和 instruction follow 的能力。所以当我们需要注入新知识的时候，continued pre-train 就派上用场了。</p>

<p>当注入的新知识只是很小的一部分，pre-train from scratch 并不是一个高效的选择，另外有的时候我们只能拿到模型训练好的权重，并不能拿到 pre-train data，所以想要重新 pre-train 也是不可能的。</p>

<p>有的时候，针对特定 domain 进行 continued pre-train 能让模型的能力更 focus 在对应的 domain 上获得更好的能力。</p>

<h2 id="0x2-continued-pre-train-有什么挑战">
<a class="anchor" href="#0x2-continued-pre-train-%E6%9C%89%E4%BB%80%E4%B9%88%E6%8C%91%E6%88%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>0x2 continued pre-train 有什么挑战</h2>
<p>虽然说 continued pre-train 和 pre-train 非常类似，就是在 pre-train 模型的基础上继续训练，但是还是会有一些新的问题。</p>

<h3 id="0x21-词表扩充问题">
<a class="anchor" href="#0x21-%E8%AF%8D%E8%A1%A8%E6%89%A9%E5%85%85%E9%97%AE%E9%A2%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>0x2.1 词表扩充问题</h3>
<p>我们知道模型在进行训练之后，需要对 corpus 做 tokenize，如果要做 continued pre-train 的 corpus 和之前 pre-train 的不一致，就会遇到 tokenize 效率的问题。</p>

<p>比如之前 Meta 开源的 LLaMa 模型，它有着非常强的性能，但是其在英文语料上进行训练的，如果希望他支持中文，就需要用中文进行 continued pre-train。但是 LLaMa 的 tokenizer 是在英文语料上训练的<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>，在遇到没有见过的中文时候，会怎么样呢？</p>

<p>实际上，现在的 tokenizer 基本都是基于 BPE<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup> 设计的，所以原理上能够支持所有的字符，当他遇到没有见过的字符时，它会 fall back to bytes，比如 “我” 这个字符就可以用 <code class="language-plaintext highlighter-rouge">E6 88 91</code> 来表示，最终也可以被 tokenizer 编码。</p>

<p>但是这种策略会引入效率的问题，不仅会影响 tokenizer 的 encode/decode 效率，还会显著增加编码之后的序列长度，这也直接影响了模型的训练效率，相当相同的 token 数包含的信息更少了。</p>

<p>这篇文章<a href="https://arxiv.org/pdf/2304.08177.pdf">EFFICIENT AND EFFECTIVE TEXT ENCODING FOR CHINESE LLAMA AND ALPACA</a> 提出可以通过扩充词表来解决这个问题。方法非常简单，具体来说就是对新 domain corpus 用 SentencePiece 重新训练一个 tokenizer，然后将新的词表和之前旧的词表重新合并在一起，在合并的时候取他们的并集，同时扩充 embedding，保留之前训好的 embedding，在后面初始化新词表的 embedding，这样可以保证之前训练好的 embedding 不受影响。</p>

<p>因为新加的词表是随机初始化的，和之前训好的词表在分布式上是不一致的，所以可以考虑固定网络的其它层，先少规模 tuning embedding 的部分。</p>

<h3 id="0x22-防止遗忘的问题">
<a class="anchor" href="#0x22-%E9%98%B2%E6%AD%A2%E9%81%97%E5%BF%98%E7%9A%84%E9%97%AE%E9%A2%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>0x2.2 防止遗忘的问题</h3>
<p>当我们对模型注入新知识的时候，可能导致模型遗忘之前学习到的知识，比如用 code 对模型进行 continued pre-train 时，一些 text-based tasks 性能会降低。为了缓解这个问题，可以在 data mixture 的时候，采样之前 domain 的数据，和当前 domain 的数据按照某种比例进行混合。除此之外，还可以考虑使用 EMA 或者是 mean-teacher 的方案来更新模型，防止模型过快地遗忘之前的知识。</p>

<h3 id="0x23-如何高效-tuning">
<a class="anchor" href="#0x23-%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88-tuning" aria-hidden="true"><span class="octicon octicon-link"></span></a>0x2.3 如何高效 tuning</h3>
<p>continued pre-train 的一个优势是不需要从头训练之前见过的 token，只需要训练新增的 token。为了更进一步降低成本，还可以在 continued pre-train 阶段使用 LoRA<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup> tuning，这种方案可以不用训练模型的所有参数，只需要一部分很小的参数，实现更小的显存和更快的训练速度。</p>

<h2 id="0x3-总结">
<a class="anchor" href="#0x3-%E6%80%BB%E7%BB%93" aria-hidden="true"><span class="octicon octicon-link"></span></a>0x3 总结</h2>
<p>这篇文章主要介绍一下 continued pre-train 的好处以及其应用场景，分析了 continued pre-train 面临的一些问题。笔者水平有限，如有遗漏或者问题，欢迎指出。</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Auto Regressive. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Supervised Instruction Fine-Tuning. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Less than one thousand Chinese character. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Byte-Pair Encoding. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>https://github.com/microsoft/LoRA <a href="#fnref:5" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="L1aoXingyu/blogpages"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blogpages/deep%20learning/llm/pre-train/2023/07/04/continued-pretrain-intro.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blogpages/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blogpages/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blogpages/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>I want to create some new things! Happy blogging &amp; coding... ❤ | Scenius</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/L1aoXingyu" target="_blank" title="L1aoXingyu"><svg class="svg-icon grey"><use xlink:href="/blogpages/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/xingyu_liao" target="_blank" title="xingyu_liao"><svg class="svg-icon grey"><use xlink:href="/blogpages/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
