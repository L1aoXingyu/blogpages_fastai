<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">如何做 continued pre-train</h1><p class="page-description">介绍一下 continued pre-train</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2023-07-04T00:00:00-05:00" itemprop="datePublished">
        Jul 4, 2023
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      1 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blogpages/categories/#deep learning">deep learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blogpages/categories/#LLM">LLM</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blogpages/categories/#pre-train">pre-train</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#0x0-什么是-continued-pre-train">0x0 什么是 continued pre-train</a></li>
<li class="toc-entry toc-h2"><a href="#0x1-为什么要做-continued-pre-train">0x1 为什么要做 continued pre-train</a></li>
<li class="toc-entry toc-h2"><a href="#0x2-continued-pre-train-有什么挑战">0x2 continued pre-train 有什么挑战</a>
<ul>
<li class="toc-entry toc-h3"><a href="#0x21-词表扩充问题">0x2.1 词表扩充问题</a></li>
<li class="toc-entry toc-h3"><a href="#0x22-防止遗忘的问题">0x2.2 防止遗忘的问题</a></li>
<li class="toc-entry toc-h3"><a href="#0x23-如何高效-tuning">0x2.3 如何高效 tuning</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#0x3-总结">0x3 总结</a></li>
</ul><p>在 LLM 中，为了获得一个好的 basemodel，往往都会使用 pre-train 的方式。但是通常 pre-train 都需要消耗很多资源，不管是算力还是数据。其实除了 pre-train 之外，还有一种方案叫做 continued pre-train，这篇文章会简要介绍一下这种方案。</p>

<h2 id="0x0-什么是-continued-pre-train">
<a class="anchor" href="#0x0-%E4%BB%80%E4%B9%88%E6%98%AF-continued-pre-train" aria-hidden="true"><span class="octicon octicon-link"></span></a>0x0 什么是 continued pre-train</h2>
<p>continued pre-train 从名字就可以看出，他是在 pre-train 的基础上继续训练，是一个 secondary stage 任务。他的 loss function 和 pre-train 完全一致，在 decoder-only 的架构下是以 AR<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> 做为目标进行训练。</p>

<p>我们知道 SIFT<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> 也是一个 pre-train 的 secondary stage 任务，那么他们之间有什么区别呢？
他们之间的区别主要有下面这几个方面：1）SIFT 的数据规模和 continued pre-train 不同，远远少于 pre-train 阶段，而且 SIFT 如果使用过多的数据容易出现 overfitting 的问题；2）SIFT 的数据对质量和多样性的要求比较高；3）SIFT 和 pre-train 的 loss objective 有轻微区别，SIFT 是部分的 AR，会 mask 掉 instruction 部分，只对 response 部分算 loss。</p>

<h2 id="0x1-为什么要做-continued-pre-train">
<a class="anchor" href="#0x1-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%81%9A-continued-pre-train" aria-hidden="true"><span class="octicon octicon-link"></span></a>0x1 为什么要做 continued pre-train</h2>
<p>continued pre-train 介于 pre-train 和 SIFT 之间，他的存在有着他的独特价值。</p>

<p>目前存在的基本共识是 pre-train 阶段将知识注入模型，在 SIFT 阶段学习 style 和 instruction follow 的能力。所以当我们需要注入新知识的时候，continued pre-train 就派上用场了。</p>

<p>当注入的新知识只是很小的一部分，pre-train from scratch 并不是一个高效的选择，另外有的时候我们只能拿到模型训练好的权重，并不能拿到 pre-train data，所以想要重新 pre-train 也是不可能的。</p>

<p>有的时候，针对特定 domain 进行 continued pre-train 能让模型的能力更 focus 在对应的 domain 上获得更好的能力。</p>

<h2 id="0x2-continued-pre-train-有什么挑战">
<a class="anchor" href="#0x2-continued-pre-train-%E6%9C%89%E4%BB%80%E4%B9%88%E6%8C%91%E6%88%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>0x2 continued pre-train 有什么挑战</h2>
<p>虽然说 continued pre-train 和 pre-train 非常类似，就是在 pre-train 模型的基础上继续训练，但是还是会有一些新的问题。</p>

<h3 id="0x21-词表扩充问题">
<a class="anchor" href="#0x21-%E8%AF%8D%E8%A1%A8%E6%89%A9%E5%85%85%E9%97%AE%E9%A2%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>0x2.1 词表扩充问题</h3>
<p>我们知道模型在进行训练之后，需要对 corpus 做 tokenize，如果要做 continued pre-train 的 corpus 和之前 pre-train 的不一致，就会遇到 tokenize 效率的问题。</p>

<p>比如之前 Meta 开源的 LLaMa 模型，它有着非常强的性能，但是其在英文语料上进行训练的，如果希望他支持中文，就需要用中文进行 continued pre-train。但是 LLaMa 的 tokenizer 是在英文语料上训练的<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>，在遇到没有见过的中文时候，会怎么样呢？</p>

<p>实际上，现在的 tokenizer 基本都是基于 BPE<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup> 设计的，所以原理上能够支持所有的字符，当他遇到没有见过的字符时，它会 fall back to bytes，比如 “我” 这个字符就可以用 <code class="language-plaintext highlighter-rouge">E6 88 91</code> 来表示，最终也可以被 tokenizer 编码。</p>

<p>但是这种策略会引入效率的问题，不仅会影响 tokenizer 的 encode/decode 效率，还会显著增加编码之后的序列长度，这也直接影响了模型的训练效率，相当相同的 token 数包含的信息更少了。</p>

<p>这篇文章<a href="https://arxiv.org/pdf/2304.08177.pdf">EFFICIENT AND EFFECTIVE TEXT ENCODING FOR CHINESE LLAMA AND ALPACA</a> 提出可以通过扩充词表来解决这个问题。方法非常简单，具体来说就是对新 domain corpus 用 SentencePiece 重新训练一个 tokenizer，然后将新的词表和之前旧的词表重新合并在一起，在合并的时候取他们的并集，同时扩充 embedding，保留之前训好的 embedding，在后面初始化新词表的 embedding，这样可以保证之前训练好的 embedding 不受影响。</p>

<p>因为新加的词表是随机初始化的，和之前训好的词表在分布式上是不一致的，所以可以考虑固定网络的其它层，先少规模 tuning embedding 的部分。</p>

<h3 id="0x22-防止遗忘的问题">
<a class="anchor" href="#0x22-%E9%98%B2%E6%AD%A2%E9%81%97%E5%BF%98%E7%9A%84%E9%97%AE%E9%A2%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>0x2.2 防止遗忘的问题</h3>
<p>当我们对模型注入新知识的时候，可能导致模型遗忘之前学习到的知识，比如用 code 对模型进行 continued pre-train 时，一些 text-based tasks 性能会降低。为了缓解这个问题，可以在 data mixture 的时候，采样之前 domain 的数据，和当前 domain 的数据按照某种比例进行混合。除此之外，还可以考虑使用 EMA 或者是 mean-teacher 的方案来更新模型，防止模型过快地遗忘之前的知识。</p>

<h3 id="0x23-如何高效-tuning">
<a class="anchor" href="#0x23-%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88-tuning" aria-hidden="true"><span class="octicon octicon-link"></span></a>0x2.3 如何高效 tuning</h3>
<p>continued pre-train 的一个优势是不需要从头训练之前见过的 token，只需要训练新增的 token。为了更进一步降低成本，还可以在 continued pre-train 阶段使用 LoRA<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup> tuning，这种方案可以不用训练模型的所有参数，只需要一部分很小的参数，实现更小的显存和更快的训练速度。</p>

<h2 id="0x3-总结">
<a class="anchor" href="#0x3-%E6%80%BB%E7%BB%93" aria-hidden="true"><span class="octicon octicon-link"></span></a>0x3 总结</h2>
<p>这篇文章主要介绍一下 continued pre-train 的好处以及其应用场景，分析了 continued pre-train 面临的一些问题。笔者水平有限，如有遗漏或者问题，欢迎指出。</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Auto Regressive. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Supervised Instruction Fine-Tuning. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Less than one thousand Chinese character. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Byte-Pair Encoding. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>https://github.com/microsoft/LoRA <a href="#fnref:5" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="L1aoXingyu/blogpages"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blogpages/deep%20learning/llm/pre-train/2023/07/04/continued-pretrain-intro.html" hidden></a>
</article>
