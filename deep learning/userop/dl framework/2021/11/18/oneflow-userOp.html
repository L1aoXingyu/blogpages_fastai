<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>如何在 OneFlow 中开发一个新的 UserOp | Distill</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="如何在 OneFlow 中开发一个新的 UserOp" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="记录在 oneflow 中开发 userOp 的流程以及中间遇到的一些问题" />
<meta property="og:description" content="记录在 oneflow 中开发 userOp 的流程以及中间遇到的一些问题" />
<link rel="canonical" href="https://l1aoxingyu.github.io/blogpages/deep%20learning/userop/dl%20framework/2021/11/18/oneflow-userOp.html" />
<meta property="og:url" content="https://l1aoxingyu.github.io/blogpages/deep%20learning/userop/dl%20framework/2021/11/18/oneflow-userOp.html" />
<meta property="og:site_name" content="Distill" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-11-18T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://l1aoxingyu.github.io/blogpages/deep%20learning/userop/dl%20framework/2021/11/18/oneflow-userOp.html","@type":"BlogPosting","headline":"如何在 OneFlow 中开发一个新的 UserOp","dateModified":"2021-11-18T00:00:00-06:00","datePublished":"2021-11-18T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://l1aoxingyu.github.io/blogpages/deep%20learning/userop/dl%20framework/2021/11/18/oneflow-userOp.html"},"description":"记录在 oneflow 中开发 userOp 的流程以及中间遇到的一些问题","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blogpages/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://l1aoxingyu.github.io/blogpages/feed.xml" title="Distill" /><link rel="shortcut icon" type="image/x-icon" href="/blogpages/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blogpages/">Distill</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blogpages/about/">About Me</a><a class="page-link" href="/blogpages/search/">Search</a><a class="page-link" href="/blogpages/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">如何在 OneFlow 中开发一个新的 UserOp</h1><p class="page-description">记录在 oneflow 中开发 userOp 的流程以及中间遇到的一些问题</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-11-18T00:00:00-06:00" itemprop="datePublished">
        Nov 18, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blogpages/categories/#deep learning">deep learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blogpages/categories/#userOp">userOp</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blogpages/categories/#dl framework">dl framework</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#整体开发流程">整体开发流程</a>
<ul>
<li class="toc-entry toc-h3"><a href="#op-注册">Op 注册</a>
<ul>
<li class="toc-entry toc-h4"><a href="#注册-op-的具体实现">注册 Op 的具体实现</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#实现-kernel-计算">实现 Kernel 计算</a>
<ul>
<li class="toc-entry toc-h4"><a href="#kernel-计算的具体实现">Kernel 计算的具体实现</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#完成-functional-接口">完成 functional 接口</a>
<ul>
<li class="toc-entry toc-h4"><a href="#注册-functional-接口的具体操作">注册 functional 接口的具体操作</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#注册-eager-求导逻辑">注册 eager 求导逻辑</a>
<ul>
<li class="toc-entry toc-h4"><a href="#eager-求导逻辑的具体实现">eager 求导逻辑的具体实现</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#ending">Ending</a></li>
<li class="toc-entry toc-h2"><a href="#reference">Reference</a></li>
</ul><h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>

<p>这篇文章主要记录了笔者学习使用 oneflow 开发 userOp 中的整个过程，将这个流程写成文章是为了进一步加深自己的学习和理解，毕竟输出才是最好的学习过程。</p>

<p>因为这篇文章是自己学习过程中的梳理，所以文章中可能会缺少一些背景知识的介绍，笔者会尽量弱化这些背景知识跟本文核心内容之间的联系，尽量用深度学习框架中都有的概念来讲解这些内容。</p>

<p>通过阅读这篇文章，你可以了解到 OneFlow 开发 UserOp 的整体流程以及流程中一些关键步骤的作用。</p>

<h2 id="整体开发流程">
<a class="anchor" href="#%E6%95%B4%E4%BD%93%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B" aria-hidden="true"><span class="octicon octicon-link"></span></a>整体开发流程</h2>

<p>要实现一个 Op 分为两个大的部分，分别是 Op 的注册和 kernel 的实现，Op 是用来描述逻辑上的概念，kernel 实现了具体在物理设备上的运算。所以 Op 更多会关注输入输出的参数名和 shape，计算中需要的属性等等，而 kernel 更关注在不同的设备上的具体计算流程，比如在 cpu 和 gpu 上进行计算就需要不同的实现方式。</p>

<p>所以整体上需要实现一个 Op 只需要注册号 Op，同时完成这个 Op 在不同设备下的 kernel 实现即可。不过因为在模型搭建中还需要考虑更多的问题，比如 Op 需要计算梯度，因为在深度学习中需要通过反向传播算法计算整个计算图中参数的梯度，用户只会显示构建前向图，反向图如何根据前向图进行自动构建等等，所以完成一个 Op 还需要一些额外的步骤，下面我们具体来讲一下每个流程以及其目的。</p>

<h3 id="op-注册">
<a class="anchor" href="#op-%E6%B3%A8%E5%86%8C" aria-hidden="true"><span class="octicon octicon-link"></span></a>Op 注册</h3>

<p>第一步需要完成 Op 的注册，即为每个 Op 选择一个唯一的名字，这样当你使用这个名字的时候系统就知道你要调用这个 Op。同时在注册 Op 的时候还需要指定 Op 的输入、输出的参数名，属性的类型和参数名，数据 shape 的推断，参数类型的推断等，最后一个非常重要的作用是设置当前 Op 的 SBP 签名。</p>

<p>SBP 是 oneflow 区别于其他框架的一个重要特性，在 oneflow 中会使用一致性视角来看待所有的张量，用于简化分布式训练，在这个视角下，整个集群会被抽象成一台机器，用户不用关系具体集群的通信细节，只需要关注逻辑上的计算即可，而在逻辑上整个集群和单卡并没有什么区别。在一致性视角下就有了 sbp 等重要概念，要了解这些内容可以查看 <a href="https://docs.oneflow.org/master/parallelism/02_sbp.html">集群的一致性视角 - OneFlow</a>。</p>

<p>因为一个 Op 不仅需要前向计算，还需要有梯度计算以进行反向传播，所以需要注册 Op 和 Op_grad 分别用于前向和反向。除此之外，还需要额外注册一个 backward Op conf，这个作用是将 Op 和 Op_grad 进行绑定，在静态图中构图时能够自动基于前向的 Op 生成反向的计算图。</p>

<h4 id="注册-op-的具体实现">
<a class="anchor" href="#%E6%B3%A8%E5%86%8C-op-%E7%9A%84%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0" aria-hidden="true"><span class="octicon octicon-link"></span></a>注册 Op 的具体实现</h4>

<p>前面大致介绍了 Op 的注册流程以及作用，下面以 <code class="language-plaintext highlighter-rouge">leaky_relu</code> 为例进行简要说明。首先通过宏 <code class="language-plaintext highlighter-rouge">REGISTER_USER_OP</code> 对 Op 进行注册，在注册过程中会返回一个 <code class="language-plaintext highlighter-rouge">OpRegistry</code> 对象，可以通过对个对象的方法进行调用来设置 Op 的属性。</p>

<p><strong>设定输入，输出和属性</strong></p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">REGISTER_USER_OP</span><span class="p">(</span><span class="s">"leaky_relu"</span><span class="p">)</span>
    <span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="s">"x"</span><span class="p">)</span>
    <span class="p">.</span><span class="n">Output</span><span class="p">(</span><span class="s">"y"</span><span class="p">)</span>
    <span class="p">.</span><span class="n">Attr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="s">"alpha"</span><span class="p">)</span>
</code></pre></div></div>

<p>通过 <code class="language-plaintext highlighter-rouge">Input("x")</code> 和 <code class="language-plaintext highlighter-rouge">Output("y")</code> 设置了输入和输出的参数名，<code class="language-plaintext highlighter-rouge">Attr&lt;float&gt;("alpha")</code> 则设置了数据类型是 float 的参数 alpha。</p>

<p><strong>检查 TensorDesc 的合法性</strong></p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">.</span><span class="n">SetTensorDescInferFn</span><span class="p">([](</span><span class="n">user_op</span><span class="o">::</span><span class="n">InferContext</span><span class="o">*</span> <span class="n">ctx</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">Shape</span><span class="o">&amp;</span> <span class="n">x_shape</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">InputShape</span><span class="p">(</span><span class="s">"x"</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
  <span class="n">Shape</span><span class="o">*</span> <span class="n">y_shape</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">OutputShape</span><span class="p">(</span><span class="s">"y"</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
  <span class="o">*</span><span class="n">y_shape</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">;</span>
  <span class="k">return</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;::</span><span class="n">Ok</span><span class="p">();</span>
<span class="p">})</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">SetTensorDescInferFn</code> 通过注册回调函数对数据描述进行检查，这里通过输入的 shape 给输出指定 shape 以分配对应的内存。常规的 Op 只需要写一个回调函数，内部会调用 logical 和 physical 的推导设置为同一套，而有一些复杂的 Op 则需要分别写 logical 和 physical 的相同推导。</p>

<p><strong>设置和推理输出数据类型</strong></p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">.</span><span class="n">SetDataTypeInferFn</span><span class="p">([](</span><span class="n">user_op</span><span class="o">::</span><span class="n">InferContext</span><span class="o">*</span> <span class="n">ctx</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;</span> <span class="p">{</span>
  <span class="o">*</span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">OutputDType</span><span class="p">(</span><span class="s">"y"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">InputDType</span><span class="p">(</span><span class="s">"x"</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
  <span class="k">return</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;::</span><span class="n">Ok</span><span class="p">();</span>
<span class="p">});</span>
</code></pre></div></div>

<p>因为是激活函数，所以设置输出的数据类型和输入一致即可。</p>

<p><strong>设置 SBP Signature</strong></p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">.</span><span class="n">SetGetSbpFn</span><span class="p">([](</span><span class="n">user_op</span><span class="o">::</span><span class="n">SbpContext</span><span class="o">*</span> <span class="n">ctx</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">user_op</span><span class="o">::</span><span class="n">TensorDesc</span><span class="o">&amp;</span> <span class="n">x_tensor</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">LogicalTensorDesc4InputArgNameAndIndex</span><span class="p">(</span><span class="s">"x"</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
  <span class="n">FOR_RANGE</span><span class="p">(</span><span class="kt">int64_t</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x_tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">().</span><span class="n">NumAxes</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">NewBuilder</span><span class="p">().</span><span class="n">Split</span><span class="p">(</span><span class="n">user_op</span><span class="o">::</span><span class="n">OpArg</span><span class="p">(</span><span class="s">"x"</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">i</span><span class="p">).</span><span class="n">Split</span><span class="p">(</span><span class="n">user_op</span><span class="o">::</span><span class="n">OpArg</span><span class="p">(</span><span class="s">"y"</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">i</span><span class="p">).</span><span class="n">Build</span><span class="p">();</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;::</span><span class="n">Ok</span><span class="p">();</span>
<span class="p">})</span>
</code></pre></div></div>

<p>在 sbp signature 的设置中，默认支持 broadcast，如果一下支持其他类型的输入和输出，则需要手工进行设置，比如对于上面的 <code class="language-plaintext highlighter-rouge">leaky_relu</code> 激活函数，支持在输入和输出的任意维度进行 split，所以可以通过一个在 Axes 上的循环建立不同的 sbp signature。</p>

<p>通过上面的方式可以对前向 Op 和反向 Op 进行注册，最后还需要通过宏 <code class="language-plaintext highlighter-rouge">REGISTER_USER_OP_GRAD</code>  注册 Op_grad，通过回调函数 <code class="language-plaintext highlighter-rouge">SetGenBackwardOpConfFn</code> 将前向 Op 和反向 Op 绑定起来，这样在静态图构建前向图时能够自动构建反向图。</p>

<h3 id="实现-kernel-计算">
<a class="anchor" href="#%E5%AE%9E%E7%8E%B0-kernel-%E8%AE%A1%E7%AE%97" aria-hidden="true"><span class="octicon octicon-link"></span></a>实现 Kernel 计算</h3>

<p>Kernel 是实际计算的控制单元，决定了用什么物理设备进行计算，针对什么样的数据类型以及用哪一种计算方式进行计算，所以我们需要实现在 cpu 和 gpu 下的计算流程，最终实现同一个 Op 可以根据情况使用不同的设备进行计算。</p>

<h4 id="kernel-计算的具体实现">
<a class="anchor" href="#kernel-%E8%AE%A1%E7%AE%97%E7%9A%84%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0" aria-hidden="true"><span class="octicon octicon-link"></span></a>Kernel 计算的具体实现</h4>

<p>Kernel 的注册和 Op 类似，通过 <code class="language-plaintext highlighter-rouge">REGISTER_USER_KERNEL</code> 进行注册，在注册之前，需要完成实际的计算过程。</p>

<p>Kernel 都需要继承 <code class="language-plaintext highlighter-rouge">user_op::OpKernel</code> 这个类，通过 override <code class="language-plaintext highlighter-rouge">Compute</code> 方法实现具体的计算过程，以 <code class="language-plaintext highlighter-rouge">leaky_relu</code> 为例。</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">Compute</span><span class="p">(</span><span class="n">user_op</span><span class="o">::</span><span class="n">KernelComputeContext</span><span class="o">*</span> <span class="n">ctx</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">user_op</span><span class="o">::</span><span class="n">Tensor</span><span class="o">*</span> <span class="n">x</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">Tensor4ArgNameAndIndex</span><span class="p">(</span><span class="s">"x"</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
  <span class="n">user_op</span><span class="o">::</span><span class="n">Tensor</span><span class="o">*</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">Tensor4ArgNameAndIndex</span><span class="p">(</span><span class="s">"y"</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
  <span class="k">const</span> <span class="kt">int32_t</span> <span class="n">elem_cnt</span> <span class="o">=</span> <span class="n">x</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">().</span><span class="n">elem_cnt</span><span class="p">();</span>
  <span class="k">const</span> <span class="kt">float</span> <span class="n">alpha</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">Attr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="s">"alpha"</span><span class="p">);</span>
  <span class="k">const</span> <span class="n">T</span><span class="o">*</span> <span class="n">x_ptr</span> <span class="o">=</span> <span class="n">x</span><span class="o">-&gt;</span><span class="n">dptr</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">();</span>
  <span class="n">T</span><span class="o">*</span> <span class="n">y_ptr</span> <span class="o">=</span> <span class="n">y</span><span class="o">-&gt;</span><span class="n">mut_dptr</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">();</span>
  <span class="n">FOR_RANGE</span><span class="p">(</span><span class="kt">int32_t</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">elem_cnt</span><span class="p">)</span> <span class="p">{</span> <span class="n">y_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">?</span> <span class="n">x_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">:</span> <span class="n">x_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">;</span> <span class="p">}</span>
<span class="err">}</span>
</code></pre></div></div>

<p>整个计算过程如下：</p>

<ol>
  <li>
    <p>获得输入的 tensor “x” 和 输出 tensor “y”，这里的参数名在注册 Op 的时候已经确定；</p>
  </li>
  <li>
    <p>计算 x 的元素个数；</p>
  </li>
  <li>
    <p>获得属性 alpha 的值，为一个 float 类型；</p>
  </li>
  <li>
    <p>获得 tensor x 和 tensor y 的指针用于后续的计算；</p>
  </li>
  <li>
    <p>遍历所有的元素，根据 leaky_relu 的公式，如果 x[i] &gt; 0 则直接返回 x[i] 的结果，否则返回 alpha * x[i]。</p>
  </li>
</ol>

<p>完成 Kernel 的具体计算流程之后，可以通过下面的方式完成 Kernel 的注册，<code class="language-plaintext highlighter-rouge">SetCreateFn</code> 可以将这个 Kernel 具体的计算进行绑定，<code class="language-plaintext highlighter-rouge">SetIsMatchedHob</code> 则接受一些表达式用于对设备和数据类型的判断，比如通过 <code class="language-plaintext highlighter-rouge">REGISTER_CPU_LEAKY_RELU_KERNEL(float)</code> 则表示在 cpu 设备上，输出 y 的类型是 float 时，使用 <code class="language-plaintext highlighter-rouge">CpuLeakyReluKernel&lt;float&gt;</code> 进行计算。</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#define REGISTER_CPU_LEAKY_RELU_KERNEL(dtype)                         \
  REGISTER_USER_KERNEL("leaky_relu")                                  \
      .SetCreateFn&lt;CpuLeakyReluKernel&lt;dtype&gt;&gt;()                       \
      .SetIsMatchedHob((user_op::HobDeviceType() == DeviceType::kCPU) \
                       &amp;&amp; (user_op::HobDataType("y", 0) == GetDataType&lt;dtype&gt;::value));
</span>
<span class="n">REGISTER_CPU_LEAKY_RELU_KERNEL</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span>
<span class="n">REGISTER_CPU_LEAKY_RELU_KERNEL</span><span class="p">(</span><span class="kt">double</span><span class="p">)</span>
</code></pre></div></div>

<p>除了需要完成 cpu 的 Kernel 之外，还需要完成 gpu 的 Kernel，整体的注册流程是类似的，不过在 gpu 实现的时候可以充分利用 cuda 编程，这里就不再展开，cuda 相关的内容后续再写成一篇或者几篇文章进行介绍。</p>

<h3 id="完成-functional-接口">
<a class="anchor" href="#%E5%AE%8C%E6%88%90-functional-%E6%8E%A5%E5%8F%A3" aria-hidden="true"><span class="octicon octicon-link"></span></a>完成 functional 接口</h3>

<p>在 c++ 端完成了 Op 的注册和 Kernel 的实现之后，需要导出到 python 端以及 eager 模式下 autograd engine 进行使用，这是需要利用 functional 进行接口的导出，这样可以通过 <code class="language-plaintext highlighter-rouge">oneflow._C.xxx</code> 在 python 中对注册接口进行调用。</p>

<h4 id="注册-functional-接口的具体操作">
<a class="anchor" href="#%E6%B3%A8%E5%86%8C-functional-%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%85%B7%E4%BD%93%E6%93%8D%E4%BD%9C" aria-hidden="true"><span class="octicon octicon-link"></span></a>注册 functional 接口的具体操作</h4>

<p>functional 接口的注册主要分为三个步骤：</p>

<ol>
  <li>
    <p>为接口增加前向和反向的 Functor 实现；</p>
  </li>
  <li>
    <p>通过 <code class="language-plaintext highlighter-rouge">m.add_functor&lt;impl::MyOpFunctor&gt;("MyOp")</code> 将 Functor 注册到 Functional Library 中；</p>
  </li>
  <li>
    <p>在 <code class="language-plaintext highlighter-rouge">functional_api.yaml</code> 中增加接口的配置文件自动生成接口。</p>
  </li>
</ol>

<p>所有的 functor 函数都在 <code class="language-plaintext highlighter-rouge">oneflow/core/functional/impl</code> 中，被设计成 class 或者是 struct，可以持有一个或是多个 Op。</p>

<p>在 constructor 中将 Op 都构造好，通常只需要声明好 Op 的输入和输出，属性则可以省略。</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LeakyReluFunctor</span> <span class="p">{</span>
 <span class="nl">public:</span>
  <span class="n">LeakyReluFunctor</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">op_</span> <span class="o">=</span> <span class="n">CHECK_JUST</span><span class="p">(</span><span class="n">one</span><span class="o">::</span><span class="n">OpBuilder</span><span class="p">(</span><span class="s">"leaky_relu"</span><span class="p">).</span><span class="n">Input</span><span class="p">(</span><span class="s">"x"</span><span class="p">).</span><span class="n">Output</span><span class="p">(</span><span class="s">"y"</span><span class="p">).</span><span class="n">Build</span><span class="p">());</span>
  <span class="p">}</span>

 <span class="nl">private:</span>
  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">OpExpr</span><span class="o">&gt;</span> <span class="n">op_</span><span class="p">;</span>
<span class="p">};</span>
</code></pre></div></div>

<p>然后实现 <code class="language-plaintext highlighter-rouge">operator()</code> 接口，在这个接口中完成 Op 的所有计算流程，可以是多个 Op 的组合，oneflow 通过 dispatch op 的机制来调用 Op 下面具体执行计算的 kernel 完成计算流程。</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Maybe</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="k">operator</span><span class="p">()(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">one</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;&amp;</span> <span class="n">x</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">&amp;</span> <span class="n">alpha</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
  <span class="n">MutableAttrMap</span> <span class="n">attrs</span><span class="p">;</span>
  <span class="n">JUST</span><span class="p">(</span><span class="n">attrs</span><span class="p">.</span><span class="n">SetAttr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="s">"alpha"</span><span class="p">,</span> <span class="n">alpha</span><span class="p">));</span>
  <span class="k">return</span> <span class="n">OpInterpUtil</span><span class="o">::</span><span class="n">Dispatch</span><span class="o">&lt;</span><span class="n">one</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="o">*</span><span class="n">op_</span><span class="p">,</span> <span class="p">{</span><span class="n">x</span><span class="p">},</span> <span class="n">attrs</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>在 <code class="language-plaintext highlighter-rouge">functional_api.yaml</code> 中增加接口配置文件时，每个接口信息由三个字段组成，示例如下</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">xxx"</span>
  <span class="na">signature</span><span class="pi">:</span> <span class="s2">"</span><span class="s">R(Args...)</span><span class="nv"> </span><span class="s">=&gt;</span><span class="nv"> </span><span class="s">Func"</span>
  <span class="na">bind_python</span><span class="pi">:</span> <span class="s">True or False</span>
</code></pre></div></div>

<p>其中 name 表示导出到 python 接口的名字，signature 指定了接口函数的签名，签名需要和之前定义的 functor 一致，同时 <code class="language-plaintext highlighter-rouge">Func</code> 作为 signature 的函数名，需要和签名注册到 function library 中的函数名一致，因为需要在 c++ 中使用的是这个函数名。</p>

<p><code class="language-plaintext highlighter-rouge">bind_python</code> 表示是否需要为当前的接口生成 python 接口，所有的前向接口都会在 python 搭建模型中用到，所以需要导出到 python，而有的函数不会在 python 中被显示调用，比如求梯度的函数，只会在 c++ 中 autograd 用到，这时就可以不为这种函数增加 python 的接口。</p>

<h3 id="注册-eager-求导逻辑">
<a class="anchor" href="#%E6%B3%A8%E5%86%8C-eager-%E6%B1%82%E5%AF%BC%E9%80%BB%E8%BE%91" aria-hidden="true"><span class="octicon octicon-link"></span></a>注册 eager 求导逻辑</h3>

<p>是为了实现 eager 下的 backward op conf，将 eager 过程中的前向 Op 自动绑定反向 Op，所以需要再次注册 eager 下的求导逻辑，主要的代码在 <code class="language-plaintext highlighter-rouge">oneflow/core/autograd/gradient_funcs</code> 中。</p>

<h4 id="eager-求导逻辑的具体实现">
<a class="anchor" href="#eager-%E6%B1%82%E5%AF%BC%E9%80%BB%E8%BE%91%E7%9A%84%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0" aria-hidden="true"><span class="octicon octicon-link"></span></a>eager 求导逻辑的具体实现</h4>

<p>首先初始化一个结构体来保存一些属性，这个结构体继承 <code class="language-plaintext highlighter-rouge">AutoGradCaptureState</code>，比如<code class="language-plaintext highlighter-rouge">requires_grad</code>, <code class="language-plaintext highlighter-rouge">alpha</code> 等参数。</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">struct</span> <span class="nc">LeakyReluCaptureState</span> <span class="o">:</span> <span class="k">public</span> <span class="n">AutoGradCaptureState</span> <span class="p">{</span>
  <span class="kt">bool</span> <span class="n">requires_grad</span><span class="p">;</span>
  <span class="kt">float</span> <span class="n">alpha</span><span class="p">;</span>
<span class="p">};</span>
</code></pre></div></div>

<p>接着需要实现一个 class 继承 <code class="language-plaintext highlighter-rouge">OpExprGradFunction</code>，同时需要特例化他的状态结构体 <code class="language-plaintext highlighter-rouge">class LeakyRelu : public OpExprGradFunction&lt;LeakyReluCaptureState&gt;</code></p>

<p>接着需要实现下面是个成员函数：</p>

<p>在 <code class="language-plaintext highlighter-rouge">Init</code> 中完成一些初始化工作，可以根据前向 Op 的 proto 来初始化一个 attrs</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;</span> <span class="n">Init</span><span class="p">(</span><span class="k">const</span> <span class="n">OpExpr</span><span class="o">&amp;</span> <span class="n">op</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
  <span class="k">const</span> <span class="k">auto</span><span class="o">*</span> <span class="n">fw_op_expr</span> <span class="o">=</span> <span class="k">dynamic_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">UserOpExpr</span><span class="o">*&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">op</span><span class="p">);</span>
  <span class="n">CHECK_NOTNULL_OR_RETURN</span><span class="p">(</span><span class="n">fw_op_expr</span><span class="p">);</span>
  <span class="n">base_attrs_</span> <span class="o">=</span> <span class="n">MakeAttrMapFromUserOpConf</span><span class="p">(</span><span class="n">fw_op_expr</span><span class="o">-&gt;</span><span class="n">proto</span><span class="p">());</span>
  <span class="k">return</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;::</span><span class="n">Ok</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div>

<p>接着在 <code class="language-plaintext highlighter-rouge">Capture</code> 中进行输入的检查，查看是否需要对它求梯度，如果需要的话，就保存一些需要在求梯度的时候使用的内容</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;</span> <span class="n">Capture</span><span class="p">(</span><span class="n">LeakyReluCaptureState</span><span class="o">*</span> <span class="n">ctx</span><span class="p">,</span> <span class="k">const</span> <span class="n">TensorTuple</span><span class="o">&amp;</span> <span class="n">inputs</span><span class="p">,</span>
                    <span class="k">const</span> <span class="n">TensorTuple</span><span class="o">&amp;</span> <span class="n">outputs</span><span class="p">,</span> <span class="k">const</span> <span class="n">AttrMap</span><span class="o">&amp;</span> <span class="n">attrs</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
  <span class="n">CHECK_EQ_OR_RETURN</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="mi">1</span><span class="p">);</span>
  <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">requires_grad</span><span class="p">();</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">requires_grad</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;::</span><span class="n">Ok</span><span class="p">();</span> <span class="p">}</span>

  <span class="n">ComposedAttrMap</span> <span class="n">composed_attrs</span><span class="p">(</span><span class="n">attrs</span><span class="p">,</span> <span class="n">base_attrs_</span><span class="p">);</span>
  <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">JUST</span><span class="p">(</span><span class="n">composed_attrs</span><span class="p">.</span><span class="n">GetAttr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="s">"alpha"</span><span class="p">));</span>
  <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">SaveTensorForBackward</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="mi">0</span><span class="p">));</span>
  <span class="k">return</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;::</span><span class="n">Ok</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Apply</code> 是具体求导的过程，首先可以对后面传回来的梯度 <code class="language-plaintext highlighter-rouge">out_grads</code> 做一些检查，最后调用在 <code class="language-plaintext highlighter-rouge">functional</code> 中注册的接口进行梯度的具体计算，然后将结果写回 <code class="language-plaintext highlighter-rouge">in_grads</code></p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;</span> <span class="n">Apply</span><span class="p">(</span><span class="k">const</span> <span class="n">LeakyReluCaptureState</span><span class="o">*</span> <span class="n">ctx</span><span class="p">,</span> <span class="k">const</span> <span class="n">TensorTuple</span><span class="o">&amp;</span> <span class="n">out_grads</span><span class="p">,</span>
                  <span class="n">TensorTuple</span><span class="o">*</span> <span class="n">in_grads</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
  <span class="n">CHECK_EQ_OR_RETURN</span><span class="p">(</span><span class="n">out_grads</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="mi">1</span><span class="p">);</span>
  <span class="n">in_grads</span><span class="o">-&gt;</span><span class="n">resize</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">requires_grad</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">SavedTensors</span><span class="p">().</span><span class="n">at</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
    <span class="n">in_grads</span><span class="o">-&gt;</span><span class="n">at</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">=</span> <span class="n">JUST</span><span class="p">(</span><span class="n">functional</span><span class="o">::</span><span class="n">LeakyReluGrad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out_grads</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">alpha</span><span class="p">));</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;::</span><span class="n">Ok</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div>

<p>最后通过 <code class="language-plaintext highlighter-rouge">REGISTER_OP_EXPR_GRAD_FUNCTION</code> 注册这个 op 的梯度计算逻辑，第一个参数是之前在 user_op 注册中的名字，
第二个参数是刚才定义的类名，比如 <code class="language-plaintext highlighter-rouge">REGISTER_OP_EXPR_GRAD_FUNCTION("leaky_relu", LeakyRelu)</code>。</p>

<h2 id="ending">
<a class="anchor" href="#ending" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ending</h2>

<p>非常感谢你能看到最后，这篇文章主要是从自己学习的角度出发进行编写，所以整篇文章的目的也是为了梳理自己学习过程中的一些总结，如果这篇文章有任何地方能够帮到你，那就更好了。</p>

<p>最后如果你发现文章有任何纰漏欢迎斧正，如果你有任何建议和意见，也欢迎去评论区留言。</p>

<h2 id="reference">
<a class="anchor" href="#reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference</h2>

<ul>
  <li>
    <p><a href="https://docs.oneflow.org/master/parallelism/02_sbp.html">集群的一致性视角 - OneFlow</a></p>
  </li>
  <li>
    <p>https://github.com/Oneflow-Inc/oneflow/pull/5854/files</p>
  </li>
  <li>
    <p>https://github.com/Oneflow-Inc/oneflow/pull/4130</p>
  </li>
  <li>
    <p>https://github.com/Oneflow-Inc/oneflow/pull/5797</p>
  </li>
  <li>
    <p>https://github.com/Oneflow-Inc/oneflow/wiki/Functional-Interface</p>
  </li>
  <li>
    <p>https://github.com/Oneflow-Inc/oneflow/pull/5329</p>
  </li>
  <li>
    <p>https://github.com/Oneflow-Inc/oneflow/pull/6544</p>
  </li>
  <li>
    <p>https://cloud.tencent.com/developer/article/1891442</p>
  </li>
</ul>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="L1aoXingyu/blogpages"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blogpages/deep%20learning/userop/dl%20framework/2021/11/18/oneflow-userOp.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blogpages/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blogpages/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blogpages/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>I want to create some new things! Happy blogging &amp; coding... ❤ | Scenius</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/L1aoXingyu" title="L1aoXingyu"><svg class="svg-icon grey"><use xlink:href="/blogpages/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/xingyu_liao" title="xingyu_liao"><svg class="svg-icon grey"><use xlink:href="/blogpages/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
